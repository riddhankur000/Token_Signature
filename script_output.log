nohup: ignoring input
outputs/MultiArith/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
outputs/MultiArith/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
outputs/MultiArith/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
outputs/MultiArith/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
outputs/MultiArith/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
outputs/MultiArith/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 58.12it/s]
  0%|          | 0/49 [00:00<?, ?it/s]  0%|          | 0/49 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 116, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids =  8%|▊         | 4/49 [01:30<13:14, 17.66s/it, acc=nan] 10%|█         | 5/49 [01:30<13:40, 18.65s/it, acc=nan] 10%|█         | 5/49 [01:46<13:40, 18.65s/it, acc=nan] 12%|█▏        | 6/49 [01:46<12:35, 17.58s/it, acc=nan] 12%|█▏        | 6/49 [02:00<12:35, 17.58s/it, acc=nan] 14%|█▍        | 7/49 [02:00<11:40, 16.67s/it, acc=nan] 14%|█▍        | 7/49 [02:16<11:40, 16.67s/it, acc=nan] 16%|█▋        | 8/49 [02:16<11:15, 16.48s/it, acc=nan] 16%|█▋        | 8/49 [02:39<11:15, 16.48s/it, acc=nan] 18%|█▊        | 9/49 [02:39<12:19, 18.48s/it, acc=nan] 18%|█▊        | 9/49 [03:11<12:19, 18.48s/it, acc=nan] 20%|██        | 10/49 [03:11<14:45, 22.71s/it, acc=nan] 20%|██        | 10/49 [03:42<14:45, 22.71s/it, acc=nan] 22%|██▏       | 11/49 [03:42<15:49, 25.00s/it, acc=nan] 22%|██▏       | 11/49 [04:07<15:49, 25.00s/it, acc=nan] 24%|██▍       | 12/49 [04:07<15:31, 25.18s/it, acc=nan] 24%|██▍       | 12/49 [04:29<15:31, 25.18s/it, acc=nan] 27%|██▋       | 13/49 [04:29<14:33, 24.25s/it, acc=nan] 27%|██▋       | 13/49 [04:57<14:33, 24.25s/it, acc=nan] 29%|██▊       | 14/49 [04:57<14:44, 25.27s/it, acc=nan] 29%|██▊       | 14/49 [05:24<14:44, 25.27s/it, acc=nan] 31%|███       | 15/49 [05:24<14:35, 25.74s/it, acc=nan] 31%|███       | 15/49 [05:53<14:35, 25.74s/it, acc=nan] 33%|███▎      | 16/49 [05:53<14:44, 26.80s/it, acc=nan] 33%|███▎      | 16/49 [06:25<14:44, 26.80s/it, acc=nan] 35%|███▍      | 17/49 [06:25<15:06, 28.33s/it, acc=nan] 35%|███▍      | 17/49 [06:55<15:06, 28.33s/it, acc=nan] 37%|███▋      | 18/49 [06:55<14:53, 28.83s/it, acc=nan] 37%|███▋      | 18/49 [07:18<14:53, 28.83s/it, acc=nan] 39%|███▉      | 19/49 [07:18<13:33, 27.10s/it, acc=nan] 39%|███▉      | 19/49 [07:46<13:33, 27.10s/it, acc=nan] 41%|████      | 20/49 [07:46<13:17, 27.49s/it, acc=nan] 41%|████      | 20/49 [08:15<13:17, 27.49s/it, acc=nan] 43%|████▎     | 21/49 [08:15<12:55, 27.68s/it, acc=nan] 43%|████▎     | 21/49 [08:38<12:55, 27.68s/it, acc=nan] 45%|████▍     | 22/49 [08:38<11:54, 26.47s/it, acc=nan] 45%|████▍     | 22/49 [09:06<11:54, 26.47s/it, acc=nan] 47%|████▋     | 23/49 [09:06<11:36, 26.79s/it, acc=nan] 47%|████▋     | 23/49 [09:32<11:36, 26.79s/it, acc=nan] 49%|████▉     | 24/49 [09:32<11:05, 26.62s/it, acc=nan] 49%|████▉     | 24/49 [10:11<11:05, 26.62s/it, acc=nan] 51%|█████     | 25/49 [10:11<12:10, 30.45s/it, acc=nan] 51%|█████     | 25/49 [10:41<12:10, 30.45s/it, acc=nan] 53%|█████▎    | 26/49 [10:41<11:36, 30.28s/it, acc=nan] 53%|█████▎    | 26/49 [10:58<11:36, 30.28s/it, acc=nan] 55%|█████▌    | 27/49 [10:58<09:39, 26.35s/it, acc=nan] 55%|█████▌    | 27/49 [11:19<09:39, 26.35s/it, acc=nan] 57%|█████▋    | 28/49 [11:19<08:34, 24.51s/it, acc=nan] 57%|█████▋    | 28/49 [11:35<08:34, 24.51s/it, acc=nan] 59%|█████▉    | 29/49 [11:35<07:24, 22.20s/it, acc=nan] 59%|█████▉    | 29/49 [11:52<07:24, 22.20s/it, acc=nan] 61%|██████    | 30/49 [11:52<06:27, 20.41s/it, acc=nan] 61%|██████    | 30/49 [12:07<06:27, 20.41s/it, acc=nan] 63%|██████▎   | 31/49 [12:07<05:41, 18.96s/it, acc=nan] 63%|██████▎   | 31/49 [12:26<05:41, 18.96s/it, acc=nan] 65%|██████▌   | 32/49 [12:26<05:23, 19.03s/it, acc=nan] 65%|██████▌   | 32/49 [12:45<05:23, 19.03s/it, acc=nan] 67%|██████▋   | 33/49 [12:45<05:02, 18.91s/it, acc=nan] 67%|██████▋   | 33/49 [13:04<05:02, 18.91s/it, acc=nan] 69%|██████▉   | 34/49 [13:04<04:45, 19.03s/it, acc=nan] 69%|██████▉   | 34/49 [13:22<04:45, 19.03s/it, acc=nan] 71%|███████▏  | 35/49 [13:22<04:19, 18.54s/it, acc=nan] 71%|███████▏  | 35/49 [13:38<04:19, 18.54s/it, acc=nan] 73%|███████▎  | 36/49 [13:38<03:53, 17.94s/it, acc=nan] 73%|███████▎  | 36/49 [13:54<03:53, 17.94s/it, acc=nan] 76%|███████▌  | 37/49 [13:54<03:27, 17.26s/it, acc=nan] 76%|███████▌  | 37/49 [14:09<03:27, 17.26s/it, acc=nan] 78%|███████▊  | 38/49 [14:09<03:03, 16.67s/it, acc=nan] 78%|███████▊  | 38/49 [14:27<03:03, 16.67s/it, acc=nan] 80%|███████▉  | 39/49 [14:27<02:49, 16.94s/it, acc=nan] 80%|███████▉  | 39/49 [14:43<02:49, 16.94s/it, acc=nan] 82%|████████▏ | 40/49 [14:43<02:30, 16.70s/it, acc=nan] 82%|████████▏ | 40/49 [15:01<02:30, 16.70s/it, acc=nan] 84%|████████▎ | 41/49 [15:01<02:15, 16.99s/it, acc=nan] 84%|████████▎ | 41/49 [15:17<02:15, 16.99s/it, acc=nan] 86%|████████▌ | 42/49 [15:17<01:57, 16.76s/it, acc=nan] 86%|████████▌ | 42/49 [15:33<01:57, 16.76s/it, acc=nan] 88%|████████▊ | 43/49 [15:33<01:39, 16.59s/it, acc=nan] 88%|████████▊ | 43/49 [15:52<01:39, 16.59s/it, acc=nan] 90%|████████▉ | 44/49 [15:52<01:25, 17.17s/it, acc=nan] 90%|████████▉ | 44/49 [16:08<01:25, 17.17s/it, acc=nan] 92%|█████████▏| 45/49 [16:08<01:07, 16.81s/it, acc=nan] 92%|█████████▏| 45/49 [16:25<01:07, 16.81s/it, acc=nan] 94%|█████████▍| 46/49 [16:25<00:50, 16.84s/it, acc=nan] 94%|█████████▍| 46/49 [16:41<00:50, 16.84s/it, acc=nan] 96%|█████████▌| 47/49 [16:41<00:33, 16.78s/it, acc=nan] 96%|█████████▌| 47/49 [16:57<00:33, 16.78s/it, acc=nan] 98%|█████████▊| 48/49 [16:57<00:16, 16.44s/it, acc=nan] 98%|█████████▊| 48/49 [17:12<00:16, 16.44s/it, acc=nan]100%|██████████| 49/49 [17:12<00:00, 16.18s/it, acc=nan]100%|██████████| 49/49 [17:12<00:00, 21.08s/it, acc=nan]
outputs/commensenseqa/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
outputs/commensenseqa/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/commensenseqa/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/lsat/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
outputs/lsat/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
outputs/lsat/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/lsat/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/crows/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/crows/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/crows/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/sst-2/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/sst-2/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/sst-2/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_challenge/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_challenge/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_challenge/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_easy/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_easy/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/arc_easy/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/FOLIO/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/FOLIO/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/FOLIO/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/gpqa/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/gpqa/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/gpqa/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/MuSR/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
outputs/MuSR/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 132, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 65, in main
    device = torch.device(f"cuda:0,1" if torch.cuda.is_available() else "cpu")
RuntimeError: Invalid device string: 'cuda:0,1'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 58.56it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 756 examples [00:00, 41047.70 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:00<00:21, 35.22 examples/s]Tokenizing data (num_proc=16):   8%|▊         | 61/756 [00:00<00:03, 179.09 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 136/756 [00:00<00:01, 341.85 examples/s]Tokenizing data (num_proc=16):  46%|████▌     | 348/756 [00:00<00:00, 829.65 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 525/756 [00:00<00:00, 1057.05 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 695/756 [00:00<00:00, 1194.44 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:01<00:00, 728.26 examples/s] 
  0%|          | 0/16 [00:00<?, ?it/s]  0%|          | 0/16 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 61.74it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 1838 examples [00:00, 252154.87 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 71/1838 [00:00<00:08, 208.57 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 345/1838 [00:00<00:01, 908.61 examples/s]Tokenizing data (num_proc=16):  39%|███▉      | 722/1838 [00:00<00:00, 1582.30 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1109/1838 [00:00<00:00, 2176.03 examples/s]Tokenizing data (num_proc=16):  84%|████████▍ | 1540/1838 [00:00<00:00, 2728.49 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:00<00:00, 1872.23 examples/s]
  0%|          | 0/74 [00:00<?, ?it/s]  0%|          | 0/74 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 54.90it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 79/1838 [00:00<00:06, 265.31 examples/s]Tokenizing data (num_proc=16):  10%|▉         | 175/1838 [00:00<00:03, 472.06 examples/s]Tokenizing data (num_proc=16):  29%|██▊       | 526/1838 [00:00<00:00, 1368.17 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 817/1838 [00:00<00:00, 1816.83 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 1274/1838 [00:00<00:00, 2529.56 examples/s]Tokenizing data (num_proc=16):  95%|█████████▌| 1752/1838 [00:00<00:00, 3014.50 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:00<00:00, 1861.07 examples/s]
  0%|          | 0/23 [00:00<?, ?it/s]  0%|          | 0/23 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 61.52it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 70/1838 [00:00<00:08, 203.57 examples/s]Tokenizing data (num_proc=16):  14%|█▍        | 263/1838 [00:00<00:02, 644.34 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 651/1838 [00:00<00:00, 1502.01 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 1150/1838 [00:00<00:00, 2461.26 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 1495/1838 [00:00<00:00, 2428.45 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:01<00:00, 1787.95 examples/s]
  0%|          | 0/37 [00:00<?, ?it/s]  0%|          | 0/37 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/microsoft/cot/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 62.11it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 102/1954 [00:00<00:05, 340.40 examples/s]Tokenizing data (num_proc=16):  11%|█         | 208/1954 [00:00<00:03, 554.85 examples/s]Tokenizing data (num_proc=16):  34%|███▎      | 659/1954 [00:00<00:00, 1661.00 examples/s]Tokenizing data (num_proc=16):  52%|█████▏    | 1021/1954 [00:00<00:00, 2170.53 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 1344/1954 [00:00<00:00, 2457.94 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 1908/1954 [00:00<00:00, 3305.96 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:00<00:00, 1996.80 examples/s]
  0%|          | 0/79 [00:00<?, ?it/s]  0%|          | 0/79 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/microsoft/direct_answer/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 54.53it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▎         | 72/1954 [00:00<00:07, 239.82 examples/s]Tokenizing data (num_proc=16):  14%|█▎        | 268/1954 [00:00<00:02, 696.46 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 432/1954 [00:00<00:01, 927.04 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 856/1954 [00:00<00:00, 1775.29 examples/s]Tokenizing data (num_proc=16):  76%|███████▌  | 1488/1954 [00:00<00:00, 2832.61 examples/s]Tokenizing data (num_proc=16):  95%|█████████▌| 1858/1954 [00:00<00:00, 2926.30 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:01<00:00, 1833.83 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]  0%|          | 0/25 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/microsoft/standard/Phi-3.5-Mini-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 60.90it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▌         | 109/1954 [00:00<00:05, 361.36 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 246/1954 [00:00<00:02, 646.36 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 612/1954 [00:00<00:00, 1543.87 examples/s]Tokenizing data (num_proc=16):  50%|████▉     | 970/1954 [00:00<00:00, 2139.18 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 1581/1954 [00:00<00:00, 3012.93 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:00<00:00, 2817.73 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:01<00:00, 1915.54 examples/s]
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.39it/s]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:01<00:15, 36.88 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 76/600 [00:01<00:09, 52.78 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 114/600 [00:01<00:06, 75.02 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:02<00:04, 93.04 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 190/600 [00:02<00:03, 107.17 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 228/600 [00:02<00:03, 116.93 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:02<00:02, 125.90 examples/s]Tokenizing data (num_proc=16):  57%|█████▋    | 341/600 [00:03<00:01, 166.40 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 378/600 [00:03<00:01, 163.82 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 415/600 [00:03<00:01, 155.13 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 452/600 [00:03<00:00, 152.62 examples/s]Tokenizing data (num_proc=16):  82%|████████▏ | 489/600 [00:04<00:00, 135.31 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 526/600 [00:04<00:00, 155.17 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 563/600 [00:04<00:00, 136.84 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 140.89 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:05<00:00, 118.15 examples/s]
  0%|          | 0/24 [00:00<?, ?it/s]  0%|          | 0/24 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
outputs/MultiArith/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
outputs/MultiArith/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.87it/s]
  0%|          | 0/12 [00:00<?, ?it/s]  0%|          | 0/12 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 40.05it/s]
  0%|          | 0/49 [00:00<?, ?it/s]  0%|          | 0/49 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
outputs/commensenseqa/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
outputs/commensenseqa/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
outputs/commensenseqa/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
outputs/commensenseqa/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl exist!!!!!!!!!!!!!!!!!!!!!!
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.13it/s]
  0%|          | 0/41 [00:00<?, ?it/s]  0%|          | 0/41 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.80it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1009 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 48/1009 [00:01<00:24, 38.83 examples/s]Tokenizing data (num_proc=16):  11%|█         | 110/1009 [00:01<00:10, 85.45 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 168/1009 [00:01<00:06, 120.50 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 235/1009 [00:01<00:04, 157.26 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 295/1009 [00:02<00:04, 177.38 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 355/1009 [00:02<00:03, 199.17 examples/s]Tokenizing data (num_proc=16):  41%|████      | 416/1009 [00:02<00:03, 179.86 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 482/1009 [00:03<00:02, 209.42 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 543/1009 [00:03<00:01, 233.26 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 608/1009 [00:03<00:01, 253.26 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 736/1009 [00:03<00:00, 309.98 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 775/1009 [00:04<00:00, 287.05 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 811/1009 [00:04<00:00, 296.99 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 860/1009 [00:04<00:00, 318.89 examples/s]Tokenizing data (num_proc=16):  89%|████████▉ | 902/1009 [00:04<00:00, 285.47 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 938/1009 [00:04<00:00, 298.78 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 985/1009 [00:04<00:00, 305.54 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:04<00:00, 207.81 examples/s]
  0%|          | 0/13 [00:00<?, ?it/s]  0%|          | 0/13 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.27it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1009 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 51/1009 [00:01<00:23, 41.20 examples/s]Tokenizing data (num_proc=16):  11%|█▏        | 116/1009 [00:01<00:09, 90.04 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 174/1009 [00:01<00:06, 124.41 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 233/1009 [00:01<00:05, 152.00 examples/s]Tokenizing data (num_proc=16):  30%|██▉       | 300/1009 [00:02<00:03, 182.08 examples/s]Tokenizing data (num_proc=16):  36%|███▌      | 363/1009 [00:02<00:03, 198.98 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 424/1009 [00:02<00:02, 209.37 examples/s]Tokenizing data (num_proc=16):  49%|████▊     | 491/1009 [00:03<00:02, 224.72 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 568/1009 [00:03<00:01, 244.75 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 614/1009 [00:03<00:01, 231.83 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 674/1009 [00:03<00:01, 243.37 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 737/1009 [00:03<00:01, 249.24 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 778/1009 [00:04<00:00, 255.80 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 820/1009 [00:04<00:00, 281.42 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 861/1009 [00:04<00:00, 286.53 examples/s]Tokenizing data (num_proc=16):  89%|████████▉ | 903/1009 [00:04<00:00, 278.17 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 946/1009 [00:04<00:00, 308.27 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 991/1009 [00:04<00:00, 288.59 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:04<00:00, 202.51 examples/s]
  0%|          | 0/41 [00:00<?, ?it/s]  0%|          | 0/41 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DataParallel' object has no attribute 'generate'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.94it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.93it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.91it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.35it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.41it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.43it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.96it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 1172 examples [00:00, 156541.76 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/1172 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 74/1172 [00:01<00:16, 66.89 examples/s]Tokenizing data (num_proc=16):  11%|█▏        | 134/1172 [00:01<00:09, 108.65 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 220/1172 [00:01<00:05, 173.78 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 272/1172 [00:01<00:04, 189.81 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 369/1172 [00:02<00:03, 253.15 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 442/1172 [00:02<00:02, 266.43 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 515/1172 [00:02<00:02, 290.78 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 588/1172 [00:02<00:01, 305.45 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 661/1172 [00:02<00:01, 316.71 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 734/1172 [00:03<00:01, 325.21 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 807/1172 [00:03<00:01, 327.94 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 880/1172 [00:03<00:00, 331.30 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 953/1172 [00:03<00:00, 310.65 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1026/1172 [00:03<00:00, 334.84 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1099/1172 [00:04<00:00, 364.99 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 379.19 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 264.41 examples/s]
  0%|          | 0/47 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/47 [00:33<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacity of 79.26 GiB of which 3.83 GiB is free. Including non-PyTorch memory, this process has 29.62 GiB memory in use. Process 2461394 has 45.79 GiB memory in use. Of the allocated memory 22.87 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.36it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 81, in main
    model = model.to(device)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4343, in to
    return super().to(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 15.19 MiB is free. Process 2461394 has 74.54 GiB memory in use. Including non-PyTorch memory, this process has 4.69 GiB memory in use. Of the allocated memory 4.25 GiB is allocated by PyTorch, and 33.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 40.01it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1172 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 74/1172 [00:01<00:18, 59.02 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 148/1172 [00:01<00:09, 111.15 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 222/1172 [00:01<00:06, 154.71 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 296/1172 [00:02<00:04, 189.94 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 369/1172 [00:02<00:03, 216.24 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 442/1172 [00:02<00:03, 237.15 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 515/1172 [00:02<00:02, 248.37 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 588/1172 [00:03<00:02, 265.08 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 661/1172 [00:03<00:01, 272.75 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 734/1172 [00:03<00:01, 270.86 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 807/1172 [00:03<00:01, 287.25 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 880/1172 [00:03<00:00, 296.67 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 953/1172 [00:04<00:00, 296.13 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1026/1172 [00:04<00:00, 319.91 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1099/1172 [00:04<00:00, 384.57 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 371.96 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 242.23 examples/s]
  0%|          | 0/24 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/24 [00:44<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 11.40 GiB is free. Process 2461394 has 34.64 GiB memory in use. Including non-PyTorch memory, this process has 33.20 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 10.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.96it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 2376 examples [00:00, 222428.05 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 90/2376 [00:01<00:30, 74.95 examples/s]Tokenizing data (num_proc=16):  10%|▉         | 237/2376 [00:01<00:10, 200.67 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 298/2376 [00:01<00:09, 224.33 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 447/2376 [00:01<00:05, 337.31 examples/s]Tokenizing data (num_proc=16):  25%|██▍       | 589/2376 [00:02<00:04, 420.31 examples/s]Tokenizing data (num_proc=16):  29%|██▊       | 680/2376 [00:02<00:03, 424.23 examples/s]Tokenizing data (num_proc=16):  35%|███▍      | 824/2376 [00:02<00:03, 499.68 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 1043/2376 [00:02<00:02, 654.86 examples/s]Tokenizing data (num_proc=16):  47%|████▋     | 1125/2376 [00:02<00:02, 581.84 examples/s]Tokenizing data (num_proc=16):  53%|█████▎    | 1271/2376 [00:03<00:01, 622.11 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 1488/2376 [00:03<00:01, 746.74 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 1571/2376 [00:03<00:01, 637.33 examples/s]Tokenizing data (num_proc=16):  72%|███████▏  | 1716/2376 [00:03<00:00, 759.48 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 1932/2376 [00:03<00:00, 977.19 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 2080/2376 [00:04<00:00, 762.51 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 2228/2376 [00:04<00:00, 681.14 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 775.39 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 522.94 examples/s]
  0%|          | 0/96 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/96 [00:20<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 61.19 MiB is free. Process 2461394 has 62.96 GiB memory in use. Including non-PyTorch memory, this process has 16.22 GiB memory in use. Of the allocated memory 11.91 GiB is allocated by PyTorch, and 3.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.43it/s]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 77/2376 [00:01<00:38, 60.06 examples/s]Tokenizing data (num_proc=16):   9%|▉         | 224/2376 [00:01<00:12, 174.16 examples/s]Tokenizing data (num_proc=16):  16%|█▌        | 376/2376 [00:01<00:07, 273.00 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 520/2376 [00:02<00:05, 345.53 examples/s]Tokenizing data (num_proc=16):  28%|██▊       | 671/2376 [00:02<00:04, 409.86 examples/s]Tokenizing data (num_proc=16):  34%|███▍      | 813/2376 [00:02<00:03, 449.36 examples/s]Tokenizing data (num_proc=16):  41%|████      | 965/2376 [00:02<00:02, 492.13 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 1192/2376 [00:03<00:01, 615.40 examples/s]Tokenizing data (num_proc=16):  53%|█████▎    | 1269/2376 [00:03<00:02, 520.91 examples/s]Tokenizing data (num_proc=16):  59%|█████▉    | 1413/2376 [00:03<00:01, 531.43 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 1565/2376 [00:03<00:01, 550.55 examples/s]Tokenizing data (num_proc=16):  72%|███████▏  | 1709/2376 [00:04<00:01, 547.99 examples/s]Tokenizing data (num_proc=16):  78%|███████▊  | 1858/2376 [00:04<00:00, 650.94 examples/s]Tokenizing data (num_proc=16):  84%|████████▍ | 2006/2376 [00:04<00:00, 633.87 examples/s]Tokenizing data (num_proc=16):  90%|█████████ | 2150/2376 [00:04<00:00, 578.39 examples/s]Tokenizing data (num_proc=16):  97%|█████████▋| 2306/2376 [00:04<00:00, 719.12 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:05<00:00, 470.95 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/30 [00:02<?, ?it/s, acc=nan]  3%|▎         | 1/30 [00:02<01:08,  2.35s/it, acc=nan]  3%|▎         | 1/30 [00:04<01:08,  2.35s/it, acc=nan]  7%|▋         | 2/30 [00:04<01:02,  2.22s/it, acc=nan]  7%|▋         | 2/30 [00:06<01:02,  2.22s/it, acc=nan] 10%|█         | 3/30 [00:06<00:53,  1.97s/it, acc=nan] 10%|█         | 3/30 [00:07<00:53,  1.97s/it, acc=nan] 13%|█▎        | 4/30 [00:07<00:44,  1.70s/it, acc=nan] 13%|█▎        | 4/30 [00:09<00:44,  1.70s/it, acc=nan] 17%|█▋        | 5/30 [00:09<00:42,  1.68s/it, acc=nan] 17%|█▋        | 5/30 [00:10<00:42,  1.68s/it, acc=nan] 20%|██        | 6/30 [00:10<00:40,  1.67s/it, acc=nan] 20%|██        | 6/30 [00:12<00:40,  1.67s/it, acc=nan] 23%|██▎       | 7/30 [00:12<00:39,  1.70s/it, acc=nan] 23%|██▎       | 7/30 [00:14<00:39,  1.70s/it, acc=nan] 27%|██▋       | 8/30 [00:14<00:36,  1.64s/it, acc=nan] 27%|██▋       | 8/30 [00:15<00:36,  1.64s/it, acc=nan] 30%|███       | 9/30 [00:15<00:34,  1.62s/it, acc=nan] 30%|███       | 9/30 [00:17<00:34,  1.62s/it, acc=nan] 33%|███▎      | 10/30 [00:17<00:33,  1.66s/it, acc=nan] 33%|███▎      | 10/30 [00:18<00:33,  1.66s/it, acc=nan] 37%|███▋      | 11/30 [00:18<00:31,  1.66s/it, acc=nan] 37%|███▋      | 11/30 [00:20<00:31,  1.66s/it, acc=nan] 40%|████      | 12/30 [00:20<00:29,  1.64s/it, acc=nan] 40%|████      | 12/30 [00:22<00:29,  1.64s/it, acc=nan] 43%|████▎     | 13/30 [00:22<00:29,  1.72s/it, acc=nan] 43%|████▎     | 13/30 [00:24<00:29,  1.72s/it, acc=nan] 47%|████▋     | 14/30 [00:24<00:26,  1.68s/it, acc=nan] 47%|████▋     | 14/30 [00:25<00:26,  1.68s/it, acc=nan] 50%|█████     | 15/30 [00:25<00:25,  1.69s/it, acc=nan] 50%|█████     | 15/30 [00:27<00:25,  1.69s/it, acc=nan] 53%|█████▎    | 16/30 [00:27<00:24,  1.73s/it, acc=nan] 53%|█████▎    | 16/30 [00:29<00:24,  1.73s/it, acc=nan] 57%|█████▋    | 17/30 [00:29<00:22,  1.76s/it, acc=nan] 57%|█████▋    | 17/30 [00:31<00:22,  1.76s/it, acc=nan] 60%|██████    | 18/30 [00:31<00:21,  1.82s/it, acc=nan] 60%|██████    | 18/30 [00:33<00:21,  1.82s/it, acc=nan] 63%|██████▎   | 19/30 [00:33<00:19,  1.78s/it, acc=nan] 63%|██████▎   | 19/30 [00:34<00:19,  1.78s/it, acc=nan] 67%|██████▋   | 20/30 [00:34<00:16,  1.70s/it, acc=nan] 67%|██████▋   | 20/30 [00:36<00:16,  1.70s/it, acc=nan] 70%|███████   | 21/30 [00:36<00:15,  1.71s/it, acc=nan] 70%|███████   | 21/30 [00:37<00:15,  1.71s/it, acc=nan] 73%|███████▎  | 22/30 [00:37<00:13,  1.69s/it, acc=nan] 73%|███████▎  | 22/30 [00:40<00:13,  1.69s/it, acc=nan] 77%|███████▋  | 23/30 [00:40<00:12,  1.84s/it, acc=nan] 77%|███████▋  | 23/30 [00:41<00:12,  1.84s/it, acc=nan] 80%|████████  | 24/30 [00:41<00:10,  1.79s/it, acc=nan] 80%|████████  | 24/30 [00:43<00:10,  1.79s/it, acc=nan] 83%|████████▎ | 25/30 [00:43<00:08,  1.70s/it, acc=nan] 83%|████████▎ | 25/30 [00:44<00:08,  1.70s/it, acc=nan] 87%|████████▋ | 26/30 [00:44<00:06,  1.68s/it, acc=nan] 87%|████████▋ | 26/30 [00:46<00:06,  1.68s/it, acc=nan] 90%|█████████ | 27/30 [00:46<00:04,  1.55s/it, acc=nan] 90%|█████████ | 27/30 [00:47<00:04,  1.55s/it, acc=nan] 93%|█████████▎| 28/30 [00:47<00:02,  1.48s/it, acc=nan] 93%|█████████▎| 28/30 [00:49<00:02,  1.48s/it, acc=nan] 97%|█████████▋| 29/30 [00:49<00:01,  1.52s/it, acc=nan] 97%|█████████▋| 29/30 [00:50<00:01,  1.52s/it, acc=nan]100%|██████████| 30/30 [00:50<00:00,  1.42s/it, acc=nan]100%|██████████| 30/30 [00:50<00:00,  1.68s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 36.21it/s]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 93/2376 [00:01<00:32, 71.04 examples/s]Tokenizing data (num_proc=16):  10%|▉         | 235/2376 [00:01<00:12, 177.30 examples/s]Tokenizing data (num_proc=16):  16%|█▋        | 387/2376 [00:01<00:06, 285.81 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 533/2376 [00:02<00:04, 372.25 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 688/2376 [00:02<00:03, 446.40 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 842/2376 [00:02<00:03, 497.98 examples/s]Tokenizing data (num_proc=16):  41%|████      | 977/2376 [00:02<00:02, 531.50 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 1136/2376 [00:02<00:02, 584.36 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 1285/2376 [00:03<00:01, 604.16 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1430/2376 [00:03<00:01, 601.64 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 1576/2376 [00:03<00:01, 619.92 examples/s]Tokenizing data (num_proc=16):  72%|███████▏  | 1714/2376 [00:03<00:01, 527.04 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 2029/2376 [00:04<00:00, 675.54 examples/s]Tokenizing data (num_proc=16):  91%|█████████ | 2151/2376 [00:04<00:00, 654.34 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 2325/2376 [00:04<00:00, 777.09 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 498.48 examples/s]
  0%|          | 0/48 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/48 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 11.19 MiB is free. Process 2461394 has 66.02 GiB memory in use. Including non-PyTorch memory, this process has 13.21 GiB memory in use. Of the allocated memory 10.47 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.73it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 1204 examples [00:00, 142263.91 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 60/1204 [00:01<00:21, 53.42 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 139/1204 [00:01<00:08, 119.41 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 211/1204 [00:01<00:05, 171.85 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 292/1204 [00:01<00:04, 213.83 examples/s]Tokenizing data (num_proc=16):  30%|███       | 367/1204 [00:02<00:03, 246.42 examples/s]Tokenizing data (num_proc=16):  36%|███▋      | 438/1204 [00:02<00:02, 264.50 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 514/1204 [00:02<00:02, 285.67 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 595/1204 [00:02<00:01, 310.45 examples/s]Tokenizing data (num_proc=16):  55%|█████▌    | 668/1204 [00:02<00:01, 312.47 examples/s]Tokenizing data (num_proc=16):  62%|██████▏   | 745/1204 [00:03<00:01, 325.02 examples/s]Tokenizing data (num_proc=16):  68%|██████▊   | 817/1204 [00:03<00:01, 330.52 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 894/1204 [00:03<00:00, 336.87 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 964/1204 [00:03<00:00, 361.37 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1049/1204 [00:03<00:00, 344.93 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1122/1204 [00:04<00:00, 388.33 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1190/1204 [00:04<00:00, 322.74 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:04<00:00, 265.36 examples/s]
  0%|          | 0/49 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/49 [00:13<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 41.19 MiB is free. Process 2461394 has 66.02 GiB memory in use. Including non-PyTorch memory, this process has 13.18 GiB memory in use. Of the allocated memory 10.03 GiB is allocated by PyTorch, and 2.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.72it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 56/1204 [00:01<00:24, 47.61 examples/s]Tokenizing data (num_proc=16):  10%|█         | 124/1204 [00:01<00:10, 101.47 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 209/1204 [00:01<00:05, 166.56 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 285/1204 [00:01<00:04, 211.72 examples/s]Tokenizing data (num_proc=16):  30%|███       | 362/1204 [00:02<00:03, 249.63 examples/s]Tokenizing data (num_proc=16):  36%|███▌      | 435/1204 [00:02<00:02, 272.84 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 510/1204 [00:02<00:02, 290.16 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 588/1204 [00:02<00:02, 300.81 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 655/1204 [00:02<00:01, 302.37 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 738/1204 [00:03<00:01, 316.89 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 806/1204 [00:03<00:01, 314.58 examples/s]Tokenizing data (num_proc=16):  74%|███████▎  | 886/1204 [00:03<00:00, 327.31 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 955/1204 [00:03<00:00, 351.54 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1042/1204 [00:04<00:00, 343.12 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1116/1204 [00:04<00:00, 339.03 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1189/1204 [00:04<00:00, 346.39 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:04<00:00, 260.84 examples/s]
  0%|          | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/16 [00:03<?, ?it/s, acc=nan]  6%|▋         | 1/16 [00:03<00:51,  3.41s/it, acc=nan]  6%|▋         | 1/16 [00:05<00:51,  3.41s/it, acc=nan] 12%|█▎        | 2/16 [00:05<00:37,  2.69s/it, acc=nan] 12%|█▎        | 2/16 [00:07<00:37,  2.69s/it, acc=nan] 19%|█▉        | 3/16 [00:07<00:31,  2.39s/it, acc=nan] 19%|█▉        | 3/16 [00:10<00:31,  2.39s/it, acc=nan] 25%|██▌       | 4/16 [00:10<00:30,  2.52s/it, acc=nan] 25%|██▌       | 4/16 [00:12<00:30,  2.52s/it, acc=nan] 31%|███▏      | 5/16 [00:12<00:26,  2.40s/it, acc=nan] 31%|███▏      | 5/16 [00:14<00:26,  2.40s/it, acc=nan] 38%|███▊      | 6/16 [00:14<00:23,  2.34s/it, acc=nan] 38%|███▊      | 6/16 [00:17<00:23,  2.34s/it, acc=nan] 44%|████▍     | 7/16 [00:17<00:20,  2.32s/it, acc=nan] 44%|████▍     | 7/16 [00:19<00:20,  2.32s/it, acc=nan] 50%|█████     | 8/16 [00:19<00:18,  2.37s/it, acc=nan] 50%|█████     | 8/16 [00:21<00:18,  2.37s/it, acc=nan] 56%|█████▋    | 9/16 [00:21<00:15,  2.27s/it, acc=nan] 56%|█████▋    | 9/16 [00:23<00:15,  2.27s/it, acc=nan] 62%|██████▎   | 10/16 [00:23<00:13,  2.20s/it, acc=nan] 62%|██████▎   | 10/16 [00:25<00:13,  2.20s/it, acc=nan] 69%|██████▉   | 11/16 [00:25<00:10,  2.20s/it, acc=nan] 69%|██████▉   | 11/16 [00:28<00:10,  2.20s/it, acc=nan] 75%|███████▌  | 12/16 [00:28<00:08,  2.20s/it, acc=nan] 75%|███████▌  | 12/16 [00:30<00:08,  2.20s/it, acc=nan] 81%|████████▏ | 13/16 [00:30<00:06,  2.14s/it, acc=nan] 81%|████████▏ | 13/16 [00:31<00:06,  2.14s/it, acc=nan] 88%|████████▊ | 14/16 [00:31<00:03,  1.99s/it, acc=nan] 88%|████████▊ | 14/16 [00:34<00:03,  1.99s/it, acc=nan] 94%|█████████▍| 15/16 [00:34<00:02,  2.13s/it, acc=nan] 94%|█████████▍| 15/16 [00:34<00:02,  2.13s/it, acc=nan]100%|██████████| 16/16 [00:34<00:00,  1.59s/it, acc=nan]100%|██████████| 16/16 [00:34<00:00,  2.15s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 40.82it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 65/1204 [00:01<00:21, 53.73 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 140/1204 [00:01<00:09, 113.52 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 213/1204 [00:01<00:06, 164.41 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 292/1204 [00:01<00:04, 214.24 examples/s]Tokenizing data (num_proc=16):  30%|███       | 367/1204 [00:02<00:04, 197.57 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 518/1204 [00:02<00:02, 308.43 examples/s]Tokenizing data (num_proc=16):  50%|████▉     | 596/1204 [00:02<00:01, 321.75 examples/s]Tokenizing data (num_proc=16):  55%|█████▌    | 663/1204 [00:02<00:01, 316.47 examples/s]Tokenizing data (num_proc=16):  62%|██████▏   | 748/1204 [00:03<00:01, 337.32 examples/s]Tokenizing data (num_proc=16):  68%|██████▊   | 818/1204 [00:03<00:01, 333.38 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 895/1204 [00:03<00:00, 321.68 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 965/1204 [00:03<00:00, 344.85 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1051/1204 [00:04<00:00, 355.41 examples/s]Tokenizing data (num_proc=16):  94%|█████████▎| 1126/1204 [00:04<00:00, 401.44 examples/s]Tokenizing data (num_proc=16): 100%|█████████▉| 1201/1204 [00:04<00:00, 337.22 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:04<00:00, 263.04 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/25 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 91.19 MiB is free. Process 2461394 has 66.02 GiB memory in use. Including non-PyTorch memory, this process has 13.13 GiB memory in use. Of the allocated memory 10.40 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.55it/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 448 examples [00:00, 69871.27 examples/s]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:17, 23.54 examples/s]Tokenizing data (num_proc=16):  12%|█▎        | 56/448 [00:01<00:08, 45.62 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:05, 64.68 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 112/448 [00:02<00:05, 65.89 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:02<00:03, 80.54 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:03, 89.69 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 196/448 [00:02<00:02, 96.76 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 252/448 [00:02<00:01, 132.14 examples/s]Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:03<00:01, 129.02 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 126.97 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:00, 118.41 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:03<00:00, 132.01 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:04<00:00, 111.98 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 126.56 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 139.76 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 96.91 examples/s] 
  0%|          | 0/18 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/18 [01:30<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 11.49 GiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Process 2511718 has 36.09 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 9.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 39.44it/s]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:18, 22.74 examples/s]Tokenizing data (num_proc=16):  12%|█▎        | 56/448 [00:01<00:09, 42.75 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:06, 59.32 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 112/448 [00:01<00:04, 73.79 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:02<00:03, 85.03 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:03, 93.05 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 196/448 [00:02<00:02, 100.03 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 224/448 [00:02<00:02, 104.54 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 252/448 [00:03<00:01, 106.43 examples/s]Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:03<00:01, 108.05 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 108.98 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:01, 109.45 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:04<00:00, 132.47 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:04<00:00, 120.99 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 120.66 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 136.99 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 93.12 examples/s] 
  0%|          | 0/6 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/6 [00:04<?, ?it/s, acc=nan] 17%|█▋        | 1/6 [00:04<00:21,  4.28s/it, acc=nan] 17%|█▋        | 1/6 [00:07<00:21,  4.28s/it, acc=nan] 33%|███▎      | 2/6 [00:07<00:14,  3.57s/it, acc=nan] 33%|███▎      | 2/6 [00:10<00:14,  3.57s/it, acc=nan] 50%|█████     | 3/6 [00:10<00:09,  3.27s/it, acc=nan] 50%|█████     | 3/6 [00:12<00:09,  3.27s/it, acc=nan] 67%|██████▋   | 4/6 [00:12<00:05,  2.76s/it, acc=nan] 67%|██████▋   | 4/6 [00:24<00:05,  2.76s/it, acc=nan] 83%|████████▎ | 5/6 [00:24<00:06,  6.18s/it, acc=nan] 83%|████████▎ | 5/6 [00:25<00:06,  6.18s/it, acc=nan]100%|██████████| 6/6 [00:25<00:00,  4.50s/it, acc=nan]100%|██████████| 6/6 [00:25<00:00,  4.29s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.99it/s]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:16, 26.16 examples/s]Tokenizing data (num_proc=16):  12%|█▎        | 56/448 [00:01<00:09, 39.83 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:06, 58.62 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:01<00:02, 105.02 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:02, 110.93 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 196/448 [00:02<00:02, 117.21 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 224/448 [00:02<00:01, 121.03 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 252/448 [00:02<00:01, 124.97 examples/s]Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:02<00:01, 127.09 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 127.47 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:00, 122.10 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:03<00:00, 124.03 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:03<00:00, 145.72 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 127.69 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 105.10 examples/s]
  0%|          | 0/9 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/9 [01:22<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 50.12 GiB memory in use. Process 2536393 has 25.78 GiB memory in use. Of the allocated memory 38.41 GiB is allocated by PyTorch, and 11.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 44.30it/s]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:24,  8.84 examples/s]Tokenizing data (num_proc=16):   6%|▋         | 48/756 [00:01<00:16, 41.78 examples/s]Tokenizing data (num_proc=16):  14%|█▍        | 106/756 [00:01<00:06, 92.91 examples/s]Tokenizing data (num_proc=16):  21%|██        | 155/756 [00:01<00:04, 126.58 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 192/756 [00:02<00:04, 137.79 examples/s]Tokenizing data (num_proc=16):  31%|███       | 233/756 [00:02<00:03, 151.05 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 286/756 [00:02<00:02, 178.55 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 325/756 [00:02<00:02, 180.47 examples/s]Tokenizing data (num_proc=16):  50%|████▉     | 377/756 [00:02<00:01, 199.02 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 460/756 [00:03<00:01, 256.89 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 499/756 [00:03<00:01, 238.13 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 582/756 [00:03<00:00, 293.55 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 615/756 [00:03<00:00, 297.14 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 647/756 [00:03<00:00, 243.83 examples/s]Tokenizing data (num_proc=16):  90%|████████▉ | 680/756 [00:04<00:00, 218.05 examples/s]Tokenizing data (num_proc=16):  96%|█████████▌| 727/756 [00:04<00:00, 248.30 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:04<00:00, 172.98 examples/s]
  0%|          | 0/31 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/31 [00:55<?, ?it/s, acc=nan]  3%|▎         | 1/31 [00:55<27:46, 55.56s/it, acc=nan]  3%|▎         | 1/31 [01:28<27:46, 55.56s/it, acc=nan]  6%|▋         | 2/31 [01:28<20:21, 42.14s/it, acc=nan]  6%|▋         | 2/31 [02:09<20:21, 42.14s/it, acc=nan] 10%|▉         | 3/31 [02:09<19:22, 41.52s/it, acc=nan] 10%|▉         | 3/31 [02:57<19:22, 41.52s/it, acc=nan] 13%|█▎        | 4/31 [02:57<19:56, 44.32s/it, acc=nan] 13%|█▎        | 4/31 [03:42<19:56, 44.32s/it, acc=nan] 16%|█▌        | 5/31 [03:42<19:13, 44.35s/it, acc=nan] 16%|█▌        | 5/31 [04:59<19:13, 44.35s/it, acc=nan] 19%|█▉        | 6/31 [04:59<23:10, 55.61s/it, acc=nan] 19%|█▉        | 6/31 [05:34<23:10, 55.61s/it, acc=nan] 23%|██▎       | 7/31 [05:34<19:28, 48.70s/it, acc=nan] 23%|██▎       | 7/31 [06:14<19:28, 48.70s/it, acc=nan] 26%|██▌       | 8/31 [06:14<17:37, 46.00s/it, acc=nan] 26%|██▌       | 8/31 [06:45<17:37, 46.00s/it, acc=nan] 29%|██▉       | 9/31 [06:45<15:09, 41.32s/it, acc=nan] 29%|██▉       | 9/31 [07:35<15:09, 41.32s/it, acc=nan] 32%|███▏      | 10/31 [07:35<15:23, 43.96s/it, acc=nan] 32%|███▏      | 10/31 [08:11<15:23, 43.96s/it, acc=nan] 35%|███▌      | 11/31 [08:11<13:49, 41.48s/it, acc=nan] 35%|███▌      | 11/31 [08:40<13:49, 41.48s/it, acc=nan] 39%|███▊      | 12/31 [08:40<11:59, 37.89s/it, acc=nan] 39%|███▊      | 12/31 [09:14<11:59, 37.89s/it, acc=nan] 42%|████▏     | 13/31 [09:14<10:59, 36.65s/it, acc=nan] 42%|████▏     | 13/31 [09:50<10:59, 36.65s/it, acc=nan] 45%|████▌     | 14/31 [09:50<10:21, 36.53s/it, acc=nan] 45%|████▌     | 14/31 [10:25<10:21, 36.53s/it, acc=nan] 48%|████▊     | 15/31 [10:25<09:37, 36.07s/it, acc=nan] 48%|████▊     | 15/31 [11:00<09:37, 36.07s/it, acc=nan] 52%|█████▏    | 16/31 [11:00<08:54, 35.63s/it, acc=nan] 52%|█████▏    | 16/31 [11:37<08:54, 35.63s/it, acc=nan] 55%|█████▍    | 17/31 [11:37<08:27, 36.22s/it, acc=nan] 55%|█████▍    | 17/31 [12:12<08:27, 36.22s/it, acc=nan] 58%|█████▊    | 18/31 [12:12<07:45, 35.83s/it, acc=nan] 58%|█████▊    | 18/31 [12:49<07:45, 35.83s/it, acc=nan] 61%|██████▏   | 19/31 [12:49<07:13, 36.11s/it, acc=nan] 61%|██████▏   | 19/31 [13:45<07:13, 36.11s/it, acc=nan] 65%|██████▍   | 20/31 [13:45<07:43, 42.15s/it, acc=nan] 65%|██████▍   | 20/31 [14:52<07:43, 42.15s/it, acc=nan] 68%|██████▊   | 21/31 [14:52<08:13, 49.40s/it, acc=nan] 68%|██████▊   | 21/31 [15:39<08:13, 49.40s/it, acc=nan] 71%|███████   | 22/31 [15:39<07:18, 48.72s/it, acc=nan] 71%|███████   | 22/31 [16:27<07:18, 48.72s/it, acc=nan] 74%|███████▍  | 23/31 [16:27<06:27, 48.48s/it, acc=nan] 74%|███████▍  | 23/31 [17:09<06:27, 48.48s/it, acc=nan] 77%|███████▋  | 24/31 [17:09<05:26, 46.63s/it, acc=nan] 77%|███████▋  | 24/31 [17:55<05:26, 46.63s/it, acc=nan] 81%|████████  | 25/31 [17:55<04:38, 46.43s/it, acc=nan] 81%|████████  | 25/31 [18:37<04:38, 46.43s/it, acc=nan] 84%|████████▍ | 26/31 [18:37<03:44, 44.96s/it, acc=nan] 84%|████████▍ | 26/31 [19:16<03:44, 44.96s/it, acc=nan] 87%|████████▋ | 27/31 [19:16<02:52, 43.19s/it, acc=nan] 87%|████████▋ | 27/31 [19:56<02:52, 43.19s/it, acc=nan] 90%|█████████ | 28/31 [19:56<02:07, 42.40s/it, acc=nan] 90%|█████████ | 28/31 [21:55<02:20, 46.98s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 42.01 GiB memory in use. Process 2628332 has 32.13 GiB memory in use. Of the allocated memory 35.42 GiB is allocated by PyTorch, and 6.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 42.29it/s]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:24,  8.86 examples/s]Tokenizing data (num_proc=16):   6%|▌         | 44/756 [00:01<00:19, 36.36 examples/s]Tokenizing data (num_proc=16):  14%|█▍        | 106/756 [00:01<00:07, 86.61 examples/s]Tokenizing data (num_proc=16):  21%|██        | 155/756 [00:02<00:05, 113.33 examples/s]Tokenizing data (num_proc=16):  25%|██▍       | 186/756 [00:02<00:04, 116.32 examples/s]Tokenizing data (num_proc=16):  33%|███▎      | 250/756 [00:02<00:03, 154.64 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 286/756 [00:02<00:03, 151.56 examples/s]Tokenizing data (num_proc=16):  46%|████▌     | 344/756 [00:03<00:02, 174.80 examples/s]Tokenizing data (num_proc=16):  52%|█████▏    | 393/756 [00:03<00:02, 181.05 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 427/756 [00:03<00:02, 162.19 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 464/756 [00:03<00:01, 156.56 examples/s]Tokenizing data (num_proc=16):  71%|███████   | 537/756 [00:04<00:01, 198.14 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 584/756 [00:04<00:00, 222.50 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 615/756 [00:04<00:00, 235.16 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 651/756 [00:04<00:00, 236.59 examples/s]Tokenizing data (num_proc=16):  90%|█████████ | 681/756 [00:04<00:00, 217.33 examples/s]Tokenizing data (num_proc=16):  96%|█████████▋| 728/756 [00:04<00:00, 216.93 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:05<00:00, 150.80 examples/s]
  0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/10 [00:12<?, ?it/s, acc=nan] 10%|█         | 1/10 [00:12<01:50, 12.27s/it, acc=nan] 10%|█         | 1/10 [00:30<01:50, 12.27s/it, acc=nan] 20%|██        | 2/10 [00:30<02:06, 15.79s/it, acc=nan] 20%|██        | 2/10 [00:42<02:06, 15.79s/it, acc=nan] 30%|███       | 3/10 [00:42<01:37, 13.88s/it, acc=nan] 30%|███       | 3/10 [00:53<01:37, 13.88s/it, acc=nan] 40%|████      | 4/10 [00:53<01:16, 12.71s/it, acc=nan] 40%|████      | 4/10 [01:02<01:16, 12.71s/it, acc=nan] 50%|█████     | 5/10 [01:02<00:57, 11.55s/it, acc=nan] 50%|█████     | 5/10 [01:11<00:57, 11.55s/it, acc=nan] 60%|██████    | 6/10 [01:11<00:42, 10.74s/it, acc=nan] 60%|██████    | 6/10 [01:22<00:42, 10.74s/it, acc=nan] 70%|███████   | 7/10 [01:22<00:32, 10.80s/it, acc=nan] 70%|███████   | 7/10 [01:28<00:32, 10.80s/it, acc=nan] 80%|████████  | 8/10 [01:28<00:18,  9.25s/it, acc=nan] 80%|████████  | 8/10 [01:40<00:18,  9.25s/it, acc=nan] 90%|█████████ | 9/10 [01:40<00:10, 10.05s/it, acc=nan] 90%|█████████ | 9/10 [01:43<00:10, 10.05s/it, acc=nan]100%|██████████| 10/10 [01:43<00:00,  7.83s/it, acc=nan]100%|██████████| 10/10 [01:43<00:00, 10.32s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 38.18it/s]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:21,  9.11 examples/s]Tokenizing data (num_proc=16):   6%|▋         | 48/756 [00:01<00:17, 41.12 examples/s]Tokenizing data (num_proc=16):  11%|█         | 83/756 [00:01<00:09, 67.80 examples/s]Tokenizing data (num_proc=16):  15%|█▌        | 116/756 [00:01<00:07, 87.17 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 164/756 [00:02<00:05, 113.07 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 223/756 [00:02<00:03, 153.99 examples/s]Tokenizing data (num_proc=16):  34%|███▍      | 259/756 [00:02<00:03, 158.14 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 325/756 [00:02<00:02, 198.19 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 362/756 [00:03<00:02, 192.87 examples/s]Tokenizing data (num_proc=16):  52%|█████▏    | 391/756 [00:03<00:02, 174.47 examples/s]Tokenizing data (num_proc=16):  60%|█████▉    | 450/756 [00:03<00:01, 190.25 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 508/756 [00:03<00:01, 195.30 examples/s]Tokenizing data (num_proc=16):  80%|███████▉  | 602/756 [00:03<00:00, 291.70 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 652/756 [00:04<00:00, 245.41 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 697/756 [00:04<00:00, 235.34 examples/s]Tokenizing data (num_proc=16):  96%|█████████▋| 729/756 [00:04<00:00, 226.30 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:04<00:00, 158.19 examples/s]
  0%|          | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/16 [01:24<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacity of 79.26 GiB of which 7.30 GiB is free. Process 2628332 has 32.13 GiB memory in use. Including non-PyTorch memory, this process has 39.82 GiB memory in use. Of the allocated memory 31.00 GiB is allocated by PyTorch, and 8.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 39.89it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 74/1838 [00:01<00:29, 60.37 examples/s]Tokenizing data (num_proc=16):  10%|█         | 191/1838 [00:01<00:10, 154.57 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 311/1838 [00:01<00:06, 237.71 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 542/1838 [00:02<00:03, 335.26 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 771/1838 [00:02<00:02, 472.98 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 876/1838 [00:02<00:02, 472.07 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 990/1838 [00:02<00:01, 482.22 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 1113/1838 [00:03<00:01, 408.05 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 1339/1838 [00:03<00:01, 490.55 examples/s]Tokenizing data (num_proc=16):  78%|███████▊  | 1442/1838 [00:03<00:00, 529.74 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 1573/1838 [00:04<00:00, 525.19 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1724/1838 [00:04<00:00, 635.02 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1817/1838 [00:04<00:00, 560.84 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 407.94 examples/s]
  0%|          | 0/74 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/74 [00:24<?, ?it/s, acc=nan]  1%|▏         | 1/74 [00:24<29:46, 24.47s/it, acc=nan]  1%|▏         | 1/74 [00:49<29:46, 24.47s/it, acc=nan]  3%|▎         | 2/74 [00:49<29:56, 24.95s/it, acc=nan]  3%|▎         | 2/74 [01:13<29:56, 24.95s/it, acc=nan]  4%|▍         | 3/74 [01:13<28:36, 24.17s/it, acc=nan]  4%|▍         | 3/74 [01:34<28:36, 24.17s/it, acc=nan]  5%|▌         | 4/74 [01:34<27:01, 23.17s/it, acc=nan]  5%|▌         | 4/74 [01:58<27:01, 23.17s/it, acc=nan]  7%|▋         | 5/74 [01:58<26:59, 23.48s/it, acc=nan]  7%|▋         | 5/74 [02:22<26:59, 23.48s/it, acc=nan]  8%|▊         | 6/74 [02:22<26:46, 23.63s/it, acc=nan]  8%|▊         | 6/74 [02:42<26:46, 23.63s/it, acc=nan]  9%|▉         | 7/74 [02:42<25:07, 22.50s/it, acc=nan]  9%|▉         | 7/74 [03:06<25:07, 22.50s/it, acc=nan] 11%|█         | 8/74 [03:06<25:19, 23.02s/it, acc=nan] 11%|█         | 8/74 [03:26<25:19, 23.02s/it, acc=nan] 12%|█▏        | 9/74 [03:26<23:53, 22.05s/it, acc=nan] 12%|█▏        | 9/74 [03:52<23:53, 22.05s/it, acc=nan] 14%|█▎        | 10/74 [03:52<24:43, 23.19s/it, acc=nan] 14%|█▎        | 10/74 [04:11<24:43, 23.19s/it, acc=nan] 15%|█▍        | 11/74 [04:11<22:56, 21.85s/it, acc=nan] 15%|█▍        | 11/74 [04:42<22:56, 21.85s/it, acc=nan] 16%|█▌        | 12/74 [04:42<25:31, 24.71s/it, acc=nan] 16%|█▌        | 12/74 [05:27<25:31, 24.71s/it, acc=nan] 18%|█▊        | 13/74 [05:27<31:11, 30.68s/it, acc=nan] 18%|█▊        | 13/74 [05:50<31:11, 30.68s/it, acc=nan] 19%|█▉        | 14/74 [05:50<28:38, 28.63s/it, acc=nan] 19%|█▉        | 14/74 [06:07<28:38, 28.63s/it, acc=nan] 20%|██        | 15/74 [06:07<24:39, 25.08s/it, acc=nan] 20%|██        | 15/74 [06:29<24:39, 25.08s/it, acc=nan] 22%|██▏       | 16/74 [06:29<23:13, 24.02s/it, acc=nan] 22%|██▏       | 16/74 [06:46<23:13, 24.02s/it, acc=nan] 23%|██▎       | 17/74 [06:46<20:53, 22.00s/it, acc=nan] 23%|██▎       | 17/74 [07:10<20:53, 22.00s/it, acc=nan] 24%|██▍       | 18/74 [07:10<21:08, 22.66s/it, acc=nan] 24%|██▍       | 18/74 [07:37<21:08, 22.66s/it, acc=nan] 26%|██▌       | 19/74 [07:37<22:00, 24.02s/it, acc=nan] 26%|██▌       | 19/74 [08:06<22:00, 24.02s/it, acc=nan] 27%|██▋       | 20/74 [08:06<22:56, 25.49s/it, acc=nan] 27%|██▋       | 20/74 [08:32<22:56, 25.49s/it, acc=nan] 28%|██▊       | 21/74 [08:32<22:29, 25.46s/it, acc=nan] 28%|██▊       | 21/74 [08:58<22:29, 25.46s/it, acc=nan] 30%|██▉       | 22/74 [08:58<22:14, 25.67s/it, acc=nan] 30%|██▉       | 22/74 [09:21<22:14, 25.67s/it, acc=nan] 31%|███       | 23/74 [09:21<21:13, 24.97s/it, acc=nan] 31%|███       | 23/74 [09:43<21:13, 24.97s/it, acc=nan] 32%|███▏      | 24/74 [09:43<19:58, 23.96s/it, acc=nan] 32%|███▏      | 24/74 [10:11<19:58, 23.96s/it, acc=nan] 34%|███▍      | 25/74 [10:11<20:41, 25.35s/it, acc=nan] 34%|███▍      | 25/74 [10:36<20:41, 25.35s/it, acc=nan] 35%|███▌      | 26/74 [10:36<20:03, 25.06s/it, acc=nan] 35%|███▌      | 26/74 [10:58<20:03, 25.06s/it, acc=nan] 36%|███▋      | 27/74 [10:58<18:51, 24.08s/it, acc=nan] 36%|███▋      | 27/74 [11:19<18:51, 24.08s/it, acc=nan] 38%|███▊      | 28/74 [11:19<17:46, 23.18s/it, acc=nan] 38%|███▊      | 28/74 [11:46<17:46, 23.18s/it, acc=nan] 39%|███▉      | 29/74 [11:46<18:16, 24.38s/it, acc=nan] 39%|███▉      | 29/74 [12:16<18:16, 24.38s/it, acc=nan] 41%|████      | 30/74 [12:16<19:02, 25.96s/it, acc=nan] 41%|████      | 30/74 [12:41<19:02, 25.96s/it, acc=nan] 42%|████▏     | 31/74 [12:41<18:30, 25.83s/it, acc=nan] 42%|████▏     | 31/74 [13:02<18:30, 25.83s/it, acc=nan] 43%|████▎     | 32/74 [13:02<16:59, 24.29s/it, acc=nan] 43%|████▎     | 32/74 [13:24<16:59, 24.29s/it, acc=nan] 45%|████▍     | 33/74 [13:24<16:11, 23.70s/it, acc=nan] 45%|████▍     | 33/74 [14:01<16:11, 23.70s/it, acc=nan] 46%|████▌     | 34/74 [14:01<18:25, 27.65s/it, acc=nan] 46%|████▌     | 34/74 [14:19<18:25, 27.65s/it, acc=nan] 47%|████▋     | 35/74 [14:19<16:07, 24.81s/it, acc=nan] 47%|████▋     | 35/74 [14:44<16:07, 24.81s/it, acc=nan] 49%|████▊     | 36/74 [14:44<15:47, 24.93s/it, acc=nan] 49%|████▊     | 36/74 [15:13<15:47, 24.93s/it, acc=nan] 50%|█████     | 37/74 [15:13<16:05, 26.08s/it, acc=nan] 50%|█████     | 37/74 [16:05<16:05, 26.08s/it, acc=nan] 51%|█████▏    | 38/74 [16:05<20:17, 33.83s/it, acc=nan] 51%|█████▏    | 38/74 [16:44<20:17, 33.83s/it, acc=nan] 53%|█████▎    | 39/74 [16:44<20:41, 35.46s/it, acc=nan] 53%|█████▎    | 39/74 [17:09<20:41, 35.46s/it, acc=nan] 54%|█████▍    | 40/74 [17:09<18:19, 32.33s/it, acc=nan] 54%|█████▍    | 40/74 [17:35<18:19, 32.33s/it, acc=nan] 55%|█████▌    | 41/74 [17:35<16:38, 30.24s/it, acc=nan] 55%|█████▌    | 41/74 [18:01<16:38, 30.24s/it, acc=nan] 57%|█████▋    | 42/74 [18:01<15:32, 29.13s/it, acc=nan] 57%|█████▋    | 42/74 [18:32<15:32, 29.13s/it, acc=nan] 58%|█████▊    | 43/74 [18:32<15:17, 29.60s/it, acc=nan] 58%|█████▊    | 43/74 [18:57<15:17, 29.60s/it, acc=nan] 59%|█████▉    | 44/74 [18:57<14:03, 28.12s/it, acc=nan] 59%|█████▉    | 44/74 [19:29<14:03, 28.12s/it, acc=nan] 61%|██████    | 45/74 [19:29<14:13, 29.44s/it, acc=nan] 61%|██████    | 45/74 [20:02<14:13, 29.44s/it, acc=nan] 62%|██████▏   | 46/74 [20:02<14:10, 30.37s/it, acc=nan] 62%|██████▏   | 46/74 [20:21<14:10, 30.37s/it, acc=nan] 64%|██████▎   | 47/74 [20:21<12:13, 27.16s/it, acc=nan] 64%|██████▎   | 47/74 [20:46<12:13, 27.16s/it, acc=nan] 65%|██████▍   | 48/74 [20:46<11:29, 26.52s/it, acc=nan] 65%|██████▍   | 48/74 [21:22<11:29, 26.52s/it, acc=nan] 66%|██████▌   | 49/74 [21:22<12:09, 29.16s/it, acc=nan] 66%|██████▌   | 49/74 [21:43<12:09, 29.16s/it, acc=nan] 68%|██████▊   | 50/74 [21:43<10:40, 26.68s/it, acc=nan] 68%|██████▊   | 50/74 [22:06<10:40, 26.68s/it, acc=nan] 69%|██████▉   | 51/74 [22:06<09:49, 25.63s/it, acc=nan] 69%|██████▉   | 51/74 [22:28<09:49, 25.63s/it, acc=nan] 70%|███████   | 52/74 [22:28<09:01, 24.59s/it, acc=nan] 70%|███████   | 52/74 [22:51<09:01, 24.59s/it, acc=nan] 72%|███████▏  | 53/74 [22:51<08:26, 24.10s/it, acc=nan] 72%|███████▏  | 53/74 [23:14<08:26, 24.10s/it, acc=nan] 73%|███████▎  | 54/74 [23:14<07:58, 23.93s/it, acc=nan] 73%|███████▎  | 54/74 [23:42<07:58, 23.93s/it, acc=nan] 74%|███████▍  | 55/74 [23:42<07:55, 25.05s/it, acc=nan] 74%|███████▍  | 55/74 [24:09<07:55, 25.05s/it, acc=nan] 76%|███████▌  | 56/74 [24:09<07:40, 25.56s/it, acc=nan] 76%|███████▌  | 56/74 [24:35<07:40, 25.56s/it, acc=nan] 77%|███████▋  | 57/74 [24:35<07:17, 25.74s/it, acc=nan] 77%|███████▋  | 57/74 [25:02<07:17, 25.74s/it, acc=nan] 78%|███████▊  | 58/74 [25:02<06:58, 26.14s/it, acc=nan] 78%|███████▊  | 58/74 [25:32<06:58, 26.14s/it, acc=nan] 80%|███████▉  | 59/74 [25:32<06:48, 27.25s/it, acc=nan] 80%|███████▉  | 59/74 [25:56<06:48, 27.25s/it, acc=nan] 81%|████████  | 60/74 [25:56<06:08, 26.33s/it, acc=nan] 81%|████████  | 60/74 [26:13<06:08, 26.33s/it, acc=nan] 82%|████████▏ | 61/74 [26:13<05:05, 23.54s/it, acc=nan] 82%|████████▏ | 61/74 [26:36<05:05, 23.54s/it, acc=nan] 84%|████████▍ | 62/74 [26:36<04:39, 23.27s/it, acc=nan] 84%|████████▍ | 62/74 [27:03<04:39, 23.27s/it, acc=nan] 85%|████████▌ | 63/74 [27:03<04:27, 24.31s/it, acc=nan] 85%|████████▌ | 63/74 [27:25<04:27, 24.31s/it, acc=nan] 86%|████████▋ | 64/74 [27:25<03:58, 23.81s/it, acc=nan] 86%|████████▋ | 64/74 [27:40<03:58, 23.81s/it, acc=nan] 88%|████████▊ | 65/74 [27:40<03:10, 21.20s/it, acc=nan] 88%|████████▊ | 65/74 [27:55<03:10, 21.20s/it, acc=nan] 89%|████████▉ | 66/74 [27:55<02:34, 19.30s/it, acc=nan] 89%|████████▉ | 66/74 [28:10<02:34, 19.30s/it, acc=nan] 91%|█████████ | 67/74 [28:10<02:04, 17.83s/it, acc=nan] 91%|█████████ | 67/74 [28:29<02:04, 17.83s/it, acc=nan] 92%|█████████▏| 68/74 [28:29<01:50, 18.39s/it, acc=nan] 92%|█████████▏| 68/74 [28:55<01:50, 18.39s/it, acc=nan] 93%|█████████▎| 69/74 [28:55<01:42, 20.55s/it, acc=nan] 93%|█████████▎| 69/74 [29:17<01:42, 20.55s/it, acc=nan] 95%|█████████▍| 70/74 [29:17<01:23, 20.94s/it, acc=nan] 95%|█████████▍| 70/74 [29:40<01:23, 20.94s/it, acc=nan] 96%|█████████▌| 71/74 [29:40<01:04, 21.63s/it, acc=nan] 96%|█████████▌| 71/74 [30:09<01:04, 21.63s/it, acc=nan] 97%|█████████▋| 72/74 [30:09<00:47, 23.78s/it, acc=nan] 97%|█████████▋| 72/74 [30:31<00:47, 23.78s/it, acc=nan] 99%|█████████▊| 73/74 [30:31<00:23, 23.31s/it, acc=nan] 99%|█████████▊| 73/74 [30:47<00:23, 23.31s/it, acc=nan]100%|██████████| 74/74 [30:47<00:00, 21.05s/it, acc=nan]100%|██████████| 74/74 [30:47<00:00, 24.96s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.27it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▎         | 67/1838 [00:01<00:32, 54.71 examples/s]Tokenizing data (num_proc=16):  10%|█         | 186/1838 [00:01<00:11, 147.93 examples/s]Tokenizing data (num_proc=16):  16%|█▋        | 300/1838 [00:01<00:06, 226.93 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 422/1838 [00:01<00:04, 303.97 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 536/1838 [00:02<00:03, 332.57 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 648/1838 [00:02<00:03, 376.41 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 768/1838 [00:02<00:02, 412.08 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 881/1838 [00:02<00:02, 437.12 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 988/1838 [00:03<00:01, 436.39 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 1112/1838 [00:03<00:01, 474.59 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 1220/1838 [00:03<00:01, 464.86 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 1340/1838 [00:03<00:01, 495.21 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1450/1838 [00:04<00:00, 488.57 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 1575/1838 [00:04<00:00, 563.87 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 1689/1838 [00:04<00:00, 557.95 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 1802/1838 [00:04<00:00, 485.23 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 378.47 examples/s]
  0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/23 [00:02<?, ?it/s, acc=nan]  4%|▍         | 1/23 [00:02<01:02,  2.83s/it, acc=nan]  4%|▍         | 1/23 [00:08<01:02,  2.83s/it, acc=nan]  9%|▊         | 2/23 [00:08<01:38,  4.67s/it, acc=nan]  9%|▊         | 2/23 [00:14<01:38,  4.67s/it, acc=nan] 13%|█▎        | 3/23 [00:14<01:39,  5.00s/it, acc=nan] 13%|█▎        | 3/23 [00:20<01:39,  5.00s/it, acc=nan] 17%|█▋        | 4/23 [00:20<01:41,  5.36s/it, acc=nan] 17%|█▋        | 4/23 [00:22<01:41,  5.36s/it, acc=nan] 22%|██▏       | 5/23 [00:22<01:15,  4.17s/it, acc=nan] 22%|██▏       | 5/23 [00:27<01:15,  4.17s/it, acc=nan] 26%|██▌       | 6/23 [00:27<01:20,  4.71s/it, acc=nan] 26%|██▌       | 6/23 [00:32<01:20,  4.71s/it, acc=nan] 30%|███       | 7/23 [00:32<01:15,  4.69s/it, acc=nan] 30%|███       | 7/23 [00:37<01:15,  4.69s/it, acc=nan] 35%|███▍      | 8/23 [00:37<01:13,  4.89s/it, acc=nan] 35%|███▍      | 8/23 [00:43<01:13,  4.89s/it, acc=nan] 39%|███▉      | 9/23 [00:43<01:12,  5.15s/it, acc=nan] 39%|███▉      | 9/23 [00:48<01:12,  5.15s/it, acc=nan] 43%|████▎     | 10/23 [00:48<01:04,  4.92s/it, acc=nan] 43%|████▎     | 10/23 [00:52<01:04,  4.92s/it, acc=nan] 48%|████▊     | 11/23 [00:52<00:58,  4.90s/it, acc=nan] 48%|████▊     | 11/23 [00:57<00:58,  4.90s/it, acc=nan] 52%|█████▏    | 12/23 [00:57<00:53,  4.82s/it, acc=nan] 52%|█████▏    | 12/23 [00:59<00:53,  4.82s/it, acc=nan] 57%|█████▋    | 13/23 [00:59<00:39,  3.99s/it, acc=nan] 57%|█████▋    | 13/23 [01:04<00:39,  3.99s/it, acc=nan] 61%|██████    | 14/23 [01:04<00:38,  4.24s/it, acc=nan] 61%|██████    | 14/23 [01:09<00:38,  4.24s/it, acc=nan] 65%|██████▌   | 15/23 [01:09<00:36,  4.50s/it, acc=nan] 65%|██████▌   | 15/23 [01:14<00:36,  4.50s/it, acc=nan] 70%|██████▉   | 16/23 [01:14<00:32,  4.59s/it, acc=nan] 70%|██████▉   | 16/23 [01:18<00:32,  4.59s/it, acc=nan] 74%|███████▍  | 17/23 [01:18<00:27,  4.56s/it, acc=nan] 74%|███████▍  | 17/23 [01:25<00:27,  4.56s/it, acc=nan] 78%|███████▊  | 18/23 [01:25<00:26,  5.27s/it, acc=nan] 78%|███████▊  | 18/23 [01:30<00:26,  5.27s/it, acc=nan] 83%|████████▎ | 19/23 [01:30<00:20,  5.00s/it, acc=nan] 83%|████████▎ | 19/23 [01:35<00:20,  5.00s/it, acc=nan] 87%|████████▋ | 20/23 [01:35<00:15,  5.08s/it, acc=nan] 87%|████████▋ | 20/23 [01:40<00:15,  5.08s/it, acc=nan] 91%|█████████▏| 21/23 [01:40<00:09,  4.98s/it, acc=nan] 91%|█████████▏| 21/23 [01:45<00:09,  4.98s/it, acc=nan] 96%|█████████▌| 22/23 [01:45<00:05,  5.10s/it, acc=nan] 96%|█████████▌| 22/23 [01:47<00:05,  5.10s/it, acc=nan]100%|██████████| 23/23 [01:47<00:00,  4.05s/it, acc=nan]100%|██████████| 23/23 [01:47<00:00,  4.66s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.10it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 88/1838 [00:01<00:22, 77.19 examples/s]Tokenizing data (num_proc=16):  11%|█         | 199/1838 [00:01<00:09, 167.13 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 315/1838 [00:01<00:07, 203.42 examples/s]Tokenizing data (num_proc=16):  30%|██▉       | 547/1838 [00:02<00:04, 308.09 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 690/1838 [00:02<00:03, 356.07 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 891/1838 [00:02<00:02, 470.34 examples/s]Tokenizing data (num_proc=16):  55%|█████▍    | 1003/1838 [00:03<00:01, 479.73 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 1127/1838 [00:03<00:01, 498.78 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 1239/1838 [00:03<00:01, 510.91 examples/s]Tokenizing data (num_proc=16):  74%|███████▎  | 1354/1838 [00:03<00:00, 485.91 examples/s]Tokenizing data (num_proc=16):  80%|███████▉  | 1470/1838 [00:03<00:00, 509.93 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1593/1838 [00:04<00:00, 526.35 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 1678/1838 [00:04<00:00, 464.08 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1820/1838 [00:04<00:00, 511.36 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 388.65 examples/s]
  0%|          | 0/37 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/37 [00:51<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 11.95 GiB is free. Process 3027654 has 33.98 GiB memory in use. Including non-PyTorch memory, this process has 33.31 GiB memory in use. Of the allocated memory 22.33 GiB is allocated by PyTorch, and 10.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/cot/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.75it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▎         | 71/1954 [00:01<00:34, 53.81 examples/s]Tokenizing data (num_proc=16):  11%|█▏        | 222/1954 [00:01<00:10, 169.99 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 342/1954 [00:01<00:06, 238.72 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 462/1954 [00:02<00:05, 292.97 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 612/1954 [00:02<00:03, 377.11 examples/s]Tokenizing data (num_proc=16):  36%|███▋      | 709/1954 [00:02<00:03, 391.09 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 819/1954 [00:03<00:03, 311.51 examples/s]Tokenizing data (num_proc=16):  55%|█████▍    | 1074/1954 [00:03<00:01, 492.22 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 1193/1954 [00:03<00:01, 498.43 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 1307/1954 [00:04<00:01, 392.67 examples/s]Tokenizing data (num_proc=16):  74%|███████▎  | 1438/1954 [00:04<00:01, 478.63 examples/s]Tokenizing data (num_proc=16):  85%|████████▍ | 1658/1954 [00:04<00:00, 480.39 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 1785/1954 [00:04<00:00, 504.91 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1935/1954 [00:05<00:00, 555.43 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:05<00:00, 380.60 examples/s]
  0%|          | 0/79 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/79 [00:24<?, ?it/s, acc=nan]  1%|▏         | 1/79 [00:24<31:13, 24.02s/it, acc=nan]  1%|▏         | 1/79 [00:43<31:13, 24.02s/it, acc=nan]  3%|▎         | 2/79 [00:43<27:16, 21.25s/it, acc=nan]  3%|▎         | 2/79 [01:00<27:16, 21.25s/it, acc=nan]  4%|▍         | 3/79 [01:00<24:34, 19.40s/it, acc=nan]  4%|▍         | 3/79 [02:11<55:32, 43.84s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.23 GiB. GPU 0 has a total capacity of 79.26 GiB of which 3.04 GiB is free. Process 3027654 has 36.87 GiB memory in use. Including non-PyTorch memory, this process has 39.34 GiB memory in use. Of the allocated memory 33.44 GiB is allocated by PyTorch, and 5.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/direct_answer/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 40.52it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 86/1954 [00:01<00:28, 66.57 examples/s]Tokenizing data (num_proc=16):  10%|█         | 205/1954 [00:01<00:11, 157.10 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 330/1954 [00:01<00:06, 237.53 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 450/1954 [00:02<00:04, 308.04 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 568/1954 [00:02<00:03, 362.10 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 686/1954 [00:02<00:03, 400.91 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 813/1954 [00:02<00:03, 347.88 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 935/1954 [00:03<00:02, 392.03 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 1100/1954 [00:03<00:01, 474.56 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1178/1954 [00:03<00:01, 425.90 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 1298/1954 [00:03<00:01, 452.20 examples/s]Tokenizing data (num_proc=16):  78%|███████▊  | 1526/1954 [00:03<00:00, 702.70 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 1672/1954 [00:04<00:00, 604.74 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 1793/1954 [00:04<00:00, 523.88 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:04<00:00, 407.74 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/25 [00:03<?, ?it/s, acc=nan]  4%|▍         | 1/25 [00:03<01:21,  3.41s/it, acc=nan]  4%|▍         | 1/25 [00:06<01:21,  3.41s/it, acc=nan]  8%|▊         | 2/25 [00:06<01:08,  2.98s/it, acc=nan]  8%|▊         | 2/25 [00:08<01:08,  2.98s/it, acc=nan] 12%|█▏        | 3/25 [00:08<00:58,  2.66s/it, acc=nan] 12%|█▏        | 3/25 [00:11<00:58,  2.66s/it, acc=nan] 16%|█▌        | 4/25 [00:11<00:59,  2.83s/it, acc=nan] 16%|█▌        | 4/25 [00:14<00:59,  2.83s/it, acc=nan] 20%|██        | 5/25 [00:14<00:57,  2.86s/it, acc=nan] 20%|██        | 5/25 [00:16<00:57,  2.86s/it, acc=nan] 24%|██▍       | 6/25 [00:16<00:52,  2.77s/it, acc=nan] 24%|██▍       | 6/25 [00:19<00:52,  2.77s/it, acc=nan] 28%|██▊       | 7/25 [00:19<00:50,  2.78s/it, acc=nan] 28%|██▊       | 7/25 [00:22<00:50,  2.78s/it, acc=nan] 32%|███▏      | 8/25 [00:22<00:47,  2.81s/it, acc=nan] 32%|███▏      | 8/25 [00:25<00:47,  2.81s/it, acc=nan] 36%|███▌      | 9/25 [00:25<00:44,  2.76s/it, acc=nan] 36%|███▌      | 9/25 [00:28<00:44,  2.76s/it, acc=nan] 40%|████      | 10/25 [00:28<00:43,  2.93s/it, acc=nan] 40%|████      | 10/25 [00:31<00:43,  2.93s/it, acc=nan] 44%|████▍     | 11/25 [00:31<00:41,  2.98s/it, acc=nan] 44%|████▍     | 11/25 [00:34<00:41,  2.98s/it, acc=nan] 48%|████▊     | 12/25 [00:34<00:38,  2.98s/it, acc=nan] 48%|████▊     | 12/25 [00:37<00:38,  2.98s/it, acc=nan] 52%|█████▏    | 13/25 [00:37<00:33,  2.80s/it, acc=nan] 52%|█████▏    | 13/25 [00:39<00:33,  2.80s/it, acc=nan] 56%|█████▌    | 14/25 [00:39<00:30,  2.78s/it, acc=nan] 56%|█████▌    | 14/25 [00:41<00:30,  2.78s/it, acc=nan] 60%|██████    | 15/25 [00:41<00:25,  2.59s/it, acc=nan] 60%|██████    | 15/25 [00:44<00:25,  2.59s/it, acc=nan] 64%|██████▍   | 16/25 [00:44<00:22,  2.55s/it, acc=nan] 64%|██████▍   | 16/25 [00:47<00:22,  2.55s/it, acc=nan] 68%|██████▊   | 17/25 [00:47<00:20,  2.58s/it, acc=nan] 68%|██████▊   | 17/25 [00:49<00:20,  2.58s/it, acc=nan] 72%|███████▏  | 18/25 [00:49<00:17,  2.53s/it, acc=nan] 72%|███████▏  | 18/25 [00:52<00:17,  2.53s/it, acc=nan] 76%|███████▌  | 19/25 [00:52<00:15,  2.56s/it, acc=nan] 76%|███████▌  | 19/25 [00:55<00:15,  2.56s/it, acc=nan] 80%|████████  | 20/25 [00:55<00:13,  2.70s/it, acc=nan] 80%|████████  | 20/25 [00:58<00:13,  2.70s/it, acc=nan] 84%|████████▍ | 21/25 [00:58<00:11,  2.76s/it, acc=nan] 84%|████████▍ | 21/25 [01:00<00:11,  2.76s/it, acc=nan] 88%|████████▊ | 22/25 [01:00<00:08,  2.80s/it, acc=nan] 88%|████████▊ | 22/25 [01:03<00:08,  2.80s/it, acc=nan] 92%|█████████▏| 23/25 [01:03<00:05,  2.66s/it, acc=nan] 92%|█████████▏| 23/25 [01:06<00:05,  2.66s/it, acc=nan] 96%|█████████▌| 24/25 [01:06<00:02,  2.71s/it, acc=nan] 96%|█████████▌| 24/25 [01:06<00:02,  2.71s/it, acc=nan]100%|██████████| 25/25 [01:06<00:00,  2.05s/it, acc=nan]100%|██████████| 25/25 [01:06<00:00,  2.66s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/standard/Meta-Llama-3.2-3B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 43.23it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 107/1954 [00:01<00:21, 87.70 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 225/1954 [00:01<00:09, 173.74 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 344/1954 [00:01<00:06, 249.60 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 470/1954 [00:01<00:04, 319.98 examples/s]Tokenizing data (num_proc=16):  30%|███       | 592/1954 [00:02<00:03, 366.00 examples/s]Tokenizing data (num_proc=16):  37%|███▋      | 715/1954 [00:02<00:03, 316.10 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 955/1954 [00:02<00:02, 471.68 examples/s]Tokenizing data (num_proc=16):  55%|█████▌    | 1077/1954 [00:03<00:01, 474.11 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 1197/1954 [00:03<00:01, 481.02 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 1317/1954 [00:03<00:01, 480.17 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 1443/1954 [00:03<00:01, 477.72 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 1568/1954 [00:04<00:00, 470.85 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1691/1954 [00:04<00:00, 555.04 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1813/1954 [00:04<00:00, 523.65 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1939/1954 [00:04<00:00, 534.79 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:04<00:00, 394.93 examples/s]
  0%|          | 0/40 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/40 [00:09<?, ?it/s, acc=nan]  2%|▎         | 1/40 [00:09<06:25,  9.88s/it, acc=nan]  2%|▎         | 1/40 [00:20<06:25,  9.88s/it, acc=nan]  5%|▌         | 2/40 [00:20<06:25, 10.14s/it, acc=nan]  5%|▌         | 2/40 [00:30<06:25, 10.14s/it, acc=nan]  8%|▊         | 3/40 [00:30<06:23, 10.38s/it, acc=nan]  8%|▊         | 3/40 [00:42<06:23, 10.38s/it, acc=nan] 10%|█         | 4/40 [00:42<06:35, 10.99s/it, acc=nan] 10%|█         | 4/40 [00:52<06:35, 10.99s/it, acc=nan] 12%|█▎        | 5/40 [00:52<06:10, 10.58s/it, acc=nan] 12%|█▎        | 5/40 [01:02<06:10, 10.58s/it, acc=nan] 15%|█▌        | 6/40 [01:02<05:50, 10.32s/it, acc=nan] 15%|█▌        | 6/40 [01:12<05:50, 10.32s/it, acc=nan] 18%|█▊        | 7/40 [01:12<05:38, 10.25s/it, acc=nan] 18%|█▊        | 7/40 [01:25<05:38, 10.25s/it, acc=nan] 20%|██        | 8/40 [01:25<05:57, 11.17s/it, acc=nan] 20%|██        | 8/40 [01:35<05:57, 11.17s/it, acc=nan] 22%|██▎       | 9/40 [01:35<05:36, 10.86s/it, acc=nan] 22%|██▎       | 9/40 [01:49<05:36, 10.86s/it, acc=nan] 25%|██▌       | 10/40 [01:49<05:52, 11.74s/it, acc=nan] 25%|██▌       | 10/40 [02:04<05:52, 11.74s/it, acc=nan] 28%|██▊       | 11/40 [02:04<06:06, 12.65s/it, acc=nan] 28%|██▊       | 11/40 [02:14<06:06, 12.65s/it, acc=nan] 30%|███       | 12/40 [02:14<05:35, 11.99s/it, acc=nan] 30%|███       | 12/40 [02:28<05:35, 11.99s/it, acc=nan] 32%|███▎      | 13/40 [02:28<05:34, 12.38s/it, acc=nan] 32%|███▎      | 13/40 [02:37<05:34, 12.38s/it, acc=nan] 35%|███▌      | 14/40 [02:37<05:00, 11.57s/it, acc=nan] 35%|███▌      | 14/40 [02:52<05:00, 11.57s/it, acc=nan] 38%|███▊      | 15/40 [02:52<05:09, 12.39s/it, acc=nan] 38%|███▊      | 15/40 [03:03<05:09, 12.39s/it, acc=nan] 40%|████      | 16/40 [03:03<04:52, 12.18s/it, acc=nan] 40%|████      | 16/40 [03:14<04:52, 12.18s/it, acc=nan] 42%|████▎     | 17/40 [03:14<04:31, 11.81s/it, acc=nan] 42%|████▎     | 17/40 [03:26<04:31, 11.81s/it, acc=nan] 45%|████▌     | 18/40 [03:26<04:21, 11.89s/it, acc=nan] 45%|████▌     | 18/40 [03:37<04:21, 11.89s/it, acc=nan] 48%|████▊     | 19/40 [03:37<04:05, 11.67s/it, acc=nan] 48%|████▊     | 19/40 [03:48<04:05, 11.67s/it, acc=nan] 50%|█████     | 20/40 [03:48<03:47, 11.37s/it, acc=nan] 50%|█████     | 20/40 [04:00<03:47, 11.37s/it, acc=nan] 52%|█████▎    | 21/40 [04:00<03:38, 11.49s/it, acc=nan] 52%|█████▎    | 21/40 [04:10<03:38, 11.49s/it, acc=nan] 55%|█████▌    | 22/40 [04:10<03:18, 11.00s/it, acc=nan] 55%|█████▌    | 22/40 [04:19<03:18, 11.00s/it, acc=nan] 57%|█████▊    | 23/40 [04:19<02:57, 10.47s/it, acc=nan] 57%|█████▊    | 23/40 [04:32<02:57, 10.47s/it, acc=nan] 60%|██████    | 24/40 [04:32<03:00, 11.26s/it, acc=nan] 60%|██████    | 24/40 [04:43<03:00, 11.26s/it, acc=nan] 62%|██████▎   | 25/40 [04:43<02:45, 11.03s/it, acc=nan] 62%|██████▎   | 25/40 [04:53<02:45, 11.03s/it, acc=nan] 65%|██████▌   | 26/40 [04:53<02:32, 10.92s/it, acc=nan] 65%|██████▌   | 26/40 [05:00<02:32, 10.92s/it, acc=nan] 68%|██████▊   | 27/40 [05:00<02:05,  9.65s/it, acc=nan] 68%|██████▊   | 27/40 [05:12<02:05,  9.65s/it, acc=nan] 70%|███████   | 28/40 [05:12<02:06, 10.51s/it, acc=nan] 70%|███████   | 28/40 [05:20<02:06, 10.51s/it, acc=nan] 72%|███████▎  | 29/40 [05:20<01:47,  9.76s/it, acc=nan] 72%|███████▎  | 29/40 [05:37<01:47,  9.76s/it, acc=nan] 75%|███████▌  | 30/40 [05:37<01:57, 11.70s/it, acc=nan] 75%|███████▌  | 30/40 [05:48<01:57, 11.70s/it, acc=nan] 78%|███████▊  | 31/40 [05:48<01:45, 11.72s/it, acc=nan] 78%|███████▊  | 31/40 [06:00<01:45, 11.72s/it, acc=nan] 80%|████████  | 32/40 [06:00<01:32, 11.54s/it, acc=nan] 80%|████████  | 32/40 [06:09<01:32, 11.54s/it, acc=nan] 82%|████████▎ | 33/40 [06:09<01:15, 10.82s/it, acc=nan] 82%|████████▎ | 33/40 [06:21<01:15, 10.82s/it, acc=nan] 85%|████████▌ | 34/40 [06:21<01:08, 11.37s/it, acc=nan] 85%|████████▌ | 34/40 [06:34<01:08, 11.37s/it, acc=nan] 88%|████████▊ | 35/40 [06:34<00:59, 11.83s/it, acc=nan] 88%|████████▊ | 35/40 [06:47<00:59, 11.83s/it, acc=nan] 90%|█████████ | 36/40 [06:47<00:47, 11.99s/it, acc=nan] 90%|█████████ | 36/40 [07:07<00:47, 11.99s/it, acc=nan] 92%|█████████▎| 37/40 [07:07<00:43, 14.53s/it, acc=nan] 92%|█████████▎| 37/40 [07:19<00:43, 14.53s/it, acc=nan] 95%|█████████▌| 38/40 [07:19<00:27, 13.91s/it, acc=nan] 95%|█████████▌| 38/40 [07:31<00:27, 13.91s/it, acc=nan] 98%|█████████▊| 39/40 [07:31<00:13, 13.19s/it, acc=nan] 98%|█████████▊| 39/40 [07:35<00:13, 13.19s/it, acc=nan]100%|██████████| 40/40 [07:35<00:00, 10.37s/it, acc=nan]100%|██████████| 40/40 [07:35<00:00, 11.38s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 78.30it/s]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:01<00:18, 29.80 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 76/600 [00:01<00:09, 55.72 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 114/600 [00:01<00:06, 78.39 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:02<00:04, 96.85 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 190/600 [00:02<00:03, 112.87 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 228/600 [00:02<00:02, 125.56 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:02<00:03, 108.03 examples/s]Tokenizing data (num_proc=16):  51%|█████     | 304/600 [00:03<00:02, 119.43 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 378/600 [00:03<00:01, 168.07 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 415/600 [00:03<00:01, 138.18 examples/s]Tokenizing data (num_proc=16):  82%|████████▏ | 489/600 [00:04<00:00, 175.49 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 526/600 [00:04<00:00, 171.75 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 563/600 [00:04<00:00, 172.21 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 179.21 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 123.96 examples/s]
  0%|          | 0/24 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/24 [00:10<?, ?it/s, acc=nan]  4%|▍         | 1/24 [00:10<04:11, 10.93s/it, acc=nan]  4%|▍         | 1/24 [00:20<04:11, 10.93s/it, acc=nan]  8%|▊         | 2/24 [00:20<03:47, 10.33s/it, acc=nan]  8%|▊         | 2/24 [00:30<03:47, 10.33s/it, acc=nan] 12%|█▎        | 3/24 [00:30<03:29,  9.95s/it, acc=nan] 12%|█▎        | 3/24 [00:39<03:29,  9.95s/it, acc=nan] 17%|█▋        | 4/24 [00:39<03:14,  9.71s/it, acc=nan] 17%|█▋        | 4/24 [00:53<03:14,  9.71s/it, acc=nan] 21%|██        | 5/24 [00:53<03:29, 11.02s/it, acc=nan] 21%|██        | 5/24 [01:02<03:29, 11.02s/it, acc=nan] 25%|██▌       | 6/24 [01:02<03:11, 10.64s/it, acc=nan] 25%|██▌       | 6/24 [01:13<03:11, 10.64s/it, acc=nan] 29%|██▉       | 7/24 [01:13<03:00, 10.61s/it, acc=nan] 29%|██▉       | 7/24 [01:24<03:00, 10.61s/it, acc=nan] 33%|███▎      | 8/24 [01:24<02:52, 10.78s/it, acc=nan] 33%|███▎      | 8/24 [01:41<02:52, 10.78s/it, acc=nan] 38%|███▊      | 9/24 [01:41<03:10, 12.69s/it, acc=nan] 38%|███▊      | 9/24 [01:55<03:10, 12.69s/it, acc=nan] 42%|████▏     | 10/24 [01:55<03:04, 13.15s/it, acc=nan] 42%|████▏     | 10/24 [02:05<03:04, 13.15s/it, acc=nan] 46%|████▌     | 11/24 [02:05<02:36, 12.05s/it, acc=nan] 46%|████▌     | 11/24 [02:15<02:36, 12.05s/it, acc=nan] 50%|█████     | 12/24 [02:15<02:19, 11.64s/it, acc=nan] 50%|█████     | 12/24 [02:29<02:19, 11.64s/it, acc=nan] 54%|█████▍    | 13/24 [02:29<02:15, 12.31s/it, acc=nan] 54%|█████▍    | 13/24 [02:44<02:15, 12.31s/it, acc=nan] 58%|█████▊    | 14/24 [02:44<02:11, 13.15s/it, acc=nan] 58%|█████▊    | 14/24 [02:55<02:11, 13.15s/it, acc=nan] 62%|██████▎   | 15/24 [02:55<01:50, 12.24s/it, acc=nan] 62%|██████▎   | 15/24 [03:05<01:50, 12.24s/it, acc=nan] 67%|██████▋   | 16/24 [03:05<01:34, 11.79s/it, acc=nan] 67%|██████▋   | 16/24 [03:15<01:34, 11.79s/it, acc=nan] 71%|███████   | 17/24 [03:15<01:18, 11.22s/it, acc=nan] 71%|███████   | 17/24 [03:26<01:18, 11.22s/it, acc=nan] 75%|███████▌  | 18/24 [03:26<01:05, 10.96s/it, acc=nan] 75%|███████▌  | 18/24 [03:37<01:05, 10.96s/it, acc=nan] 79%|███████▉  | 19/24 [03:37<00:54, 10.98s/it, acc=nan] 79%|███████▉  | 19/24 [04:06<00:54, 10.98s/it, acc=nan] 83%|████████▎ | 20/24 [04:06<01:05, 16.41s/it, acc=nan] 83%|████████▎ | 20/24 [04:21<01:05, 16.41s/it, acc=nan] 88%|████████▊ | 21/24 [04:21<00:48, 16.16s/it, acc=nan] 88%|████████▊ | 21/24 [04:32<00:48, 16.16s/it, acc=nan] 92%|█████████▏| 22/24 [04:32<00:28, 14.45s/it, acc=nan] 92%|█████████▏| 22/24 [04:42<00:28, 14.45s/it, acc=nan] 96%|█████████▌| 23/24 [04:42<00:13, 13.20s/it, acc=nan] 96%|█████████▌| 23/24 [04:56<00:13, 13.20s/it, acc=nan]100%|██████████| 24/24 [04:56<00:00, 13.50s/it, acc=nan]100%|██████████| 24/24 [04:56<00:00, 12.36s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 72.47it/s]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:01<00:18, 30.71 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 76/600 [00:01<00:09, 57.50 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 114/600 [00:01<00:06, 79.03 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:02<00:04, 96.26 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 190/600 [00:02<00:03, 107.00 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 228/600 [00:02<00:03, 122.73 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:02<00:02, 137.13 examples/s]Tokenizing data (num_proc=16):  51%|█████     | 304/600 [00:02<00:02, 147.05 examples/s]Tokenizing data (num_proc=16):  57%|█████▋    | 341/600 [00:03<00:02, 120.44 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 378/600 [00:03<00:01, 130.77 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 415/600 [00:03<00:01, 137.78 examples/s]Tokenizing data (num_proc=16):  82%|████████▏ | 489/600 [00:04<00:00, 182.12 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 526/600 [00:04<00:00, 178.25 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 563/600 [00:04<00:00, 180.50 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 177.54 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 124.02 examples/s]
  0%|          | 0/8 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/8 [00:03<?, ?it/s, acc=nan] 12%|█▎        | 1/8 [00:03<00:23,  3.30s/it, acc=nan] 12%|█▎        | 1/8 [00:05<00:23,  3.30s/it, acc=nan] 25%|██▌       | 2/8 [00:05<00:15,  2.56s/it, acc=nan] 25%|██▌       | 2/8 [00:07<00:15,  2.56s/it, acc=nan] 38%|███▊      | 3/8 [00:07<00:11,  2.31s/it, acc=nan] 38%|███▊      | 3/8 [00:09<00:11,  2.31s/it, acc=nan] 50%|█████     | 4/8 [00:09<00:08,  2.12s/it, acc=nan] 50%|█████     | 4/8 [00:11<00:08,  2.12s/it, acc=nan] 62%|██████▎   | 5/8 [00:11<00:06,  2.03s/it, acc=nan] 62%|██████▎   | 5/8 [00:13<00:06,  2.03s/it, acc=nan] 75%|███████▌  | 6/8 [00:13<00:04,  2.15s/it, acc=nan] 75%|███████▌  | 6/8 [00:15<00:04,  2.15s/it, acc=nan] 88%|████████▊ | 7/8 [00:15<00:02,  2.07s/it, acc=nan] 88%|████████▊ | 7/8 [00:16<00:02,  2.07s/it, acc=nan]100%|██████████| 8/8 [00:16<00:00,  1.78s/it, acc=nan]100%|██████████| 8/8 [00:16<00:00,  2.06s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 76.16it/s]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:01<00:17, 31.30 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 76/600 [00:01<00:08, 61.17 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 114/600 [00:01<00:05, 86.26 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:01<00:04, 107.53 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 190/600 [00:02<00:03, 127.07 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 228/600 [00:02<00:03, 104.74 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:02<00:02, 120.96 examples/s]Tokenizing data (num_proc=16):  57%|█████▋    | 341/600 [00:02<00:01, 175.00 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 378/600 [00:03<00:01, 136.41 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 415/600 [00:03<00:01, 143.70 examples/s]Tokenizing data (num_proc=16):  82%|████████▏ | 489/600 [00:03<00:00, 179.78 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 526/600 [00:04<00:00, 188.86 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 563/600 [00:04<00:00, 191.97 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 206.60 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:04<00:00, 133.03 examples/s]
  0%|          | 0/12 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/12 [00:12<?, ?it/s, acc=nan]  8%|▊         | 1/12 [00:12<02:18, 12.59s/it, acc=nan]  8%|▊         | 1/12 [00:26<02:18, 12.59s/it, acc=nan] 17%|█▋        | 2/12 [00:26<02:16, 13.62s/it, acc=nan] 17%|█▋        | 2/12 [00:36<02:16, 13.62s/it, acc=nan] 25%|██▌       | 3/12 [00:36<01:44, 11.65s/it, acc=nan] 25%|██▌       | 3/12 [00:48<01:44, 11.65s/it, acc=nan] 33%|███▎      | 4/12 [00:48<01:35, 11.89s/it, acc=nan] 33%|███▎      | 4/12 [01:06<01:35, 11.89s/it, acc=nan] 42%|████▏     | 5/12 [01:06<01:38, 14.11s/it, acc=nan] 42%|████▏     | 5/12 [01:16<01:38, 14.11s/it, acc=nan] 50%|█████     | 6/12 [01:16<01:15, 12.61s/it, acc=nan] 50%|█████     | 6/12 [01:33<01:15, 12.61s/it, acc=nan] 58%|█████▊    | 7/12 [01:33<01:11, 14.29s/it, acc=nan] 58%|█████▊    | 7/12 [01:45<01:11, 14.29s/it, acc=nan] 67%|██████▋   | 8/12 [01:45<00:53, 13.39s/it, acc=nan] 67%|██████▋   | 8/12 [01:55<00:53, 13.39s/it, acc=nan] 75%|███████▌  | 9/12 [01:55<00:36, 12.28s/it, acc=nan] 75%|███████▌  | 9/12 [02:05<00:36, 12.28s/it, acc=nan] 83%|████████▎ | 10/12 [02:05<00:23, 11.64s/it, acc=nan] 83%|████████▎ | 10/12 [02:25<00:29, 14.55s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.93 GiB. GPU 0 has a total capacity of 79.26 GiB of which 4.87 GiB is free. Process 3027654 has 41.96 GiB memory in use. Including non-PyTorch memory, this process has 32.41 GiB memory in use. Of the allocated memory 28.70 GiB is allocated by PyTorch, and 3.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 76.39it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1221 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 77/1221 [00:01<00:21, 54.13 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 231/1221 [00:01<00:06, 155.89 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 308/1221 [00:02<00:04, 188.10 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 385/1221 [00:02<00:04, 175.57 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 461/1221 [00:02<00:03, 202.87 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 537/1221 [00:03<00:03, 222.45 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 613/1221 [00:03<00:02, 238.86 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 689/1221 [00:03<00:02, 251.46 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 765/1221 [00:03<00:01, 261.32 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 841/1221 [00:04<00:01, 276.72 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 917/1221 [00:04<00:01, 280.96 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1069/1221 [00:04<00:00, 339.23 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1145/1221 [00:05<00:00, 298.20 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:05<00:00, 349.60 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:05<00:00, 233.62 examples/s]
  0%|          | 0/49 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/49 [00:19<?, ?it/s, acc=nan]  2%|▏         | 1/49 [00:19<15:35, 19.48s/it, acc=nan]  2%|▏         | 1/49 [00:31<15:35, 19.48s/it, acc=nan]  4%|▍         | 2/49 [00:31<11:50, 15.11s/it, acc=nan]  4%|▍         | 2/49 [00:49<11:50, 15.11s/it, acc=nan]  6%|▌         | 3/49 [00:49<12:32, 16.36s/it, acc=nan]  6%|▌         | 3/49 [01:07<12:32, 16.36s/it, acc=nan]  8%|▊         | 4/49 [01:07<12:45, 17.00s/it, acc=nan]  8%|▊         | 4/49 [01:28<12:45, 17.00s/it, acc=nan] 10%|█         | 5/49 [01:28<13:26, 18.34s/it, acc=nan] 10%|█         | 5/49 [01:42<13:26, 18.34s/it, acc=nan] 12%|█▏        | 6/49 [01:42<12:08, 16.93s/it, acc=nan] 12%|█▏        | 6/49 [01:59<12:08, 16.93s/it, acc=nan] 14%|█▍        | 7/49 [01:59<11:51, 16.94s/it, acc=nan] 14%|█▍        | 7/49 [02:15<11:51, 16.94s/it, acc=nan] 16%|█▋        | 8/49 [02:15<11:30, 16.84s/it, acc=nan] 16%|█▋        | 8/49 [02:34<11:30, 16.84s/it, acc=nan] 18%|█▊        | 9/49 [02:34<11:30, 17.26s/it, acc=nan] 18%|█▊        | 9/49 [02:49<11:30, 17.26s/it, acc=nan] 20%|██        | 10/49 [02:49<10:55, 16.80s/it, acc=nan] 20%|██        | 10/49 [03:07<10:55, 16.80s/it, acc=nan] 22%|██▏       | 11/49 [03:07<10:46, 17.02s/it, acc=nan] 22%|██▏       | 11/49 [03:23<10:46, 17.02s/it, acc=nan] 24%|██▍       | 12/49 [03:23<10:15, 16.65s/it, acc=nan] 24%|██▍       | 12/49 [03:42<10:15, 16.65s/it, acc=nan] 27%|██▋       | 13/49 [03:42<10:30, 17.52s/it, acc=nan] 27%|██▋       | 13/49 [04:02<10:30, 17.52s/it, acc=nan] 29%|██▊       | 14/49 [04:02<10:42, 18.37s/it, acc=nan] 29%|██▊       | 14/49 [04:18<10:42, 18.37s/it, acc=nan] 31%|███       | 15/49 [04:18<09:58, 17.60s/it, acc=nan] 31%|███       | 15/49 [04:38<09:58, 17.60s/it, acc=nan] 33%|███▎      | 16/49 [04:38<09:57, 18.12s/it, acc=nan] 33%|███▎      | 16/49 [05:00<09:57, 18.12s/it, acc=nan] 35%|███▍      | 17/49 [05:00<10:24, 19.53s/it, acc=nan] 35%|███▍      | 17/49 [05:21<10:24, 19.53s/it, acc=nan] 37%|███▋      | 18/49 [05:21<10:11, 19.72s/it, acc=nan] 37%|███▋      | 18/49 [05:40<10:11, 19.72s/it, acc=nan] 39%|███▉      | 19/49 [05:40<09:52, 19.76s/it, acc=nan] 39%|███▉      | 19/49 [06:02<09:52, 19.76s/it, acc=nan] 41%|████      | 20/49 [06:02<09:46, 20.21s/it, acc=nan] 41%|████      | 20/49 [06:24<09:46, 20.21s/it, acc=nan] 43%|████▎     | 21/49 [06:24<09:43, 20.86s/it, acc=nan] 43%|████▎     | 21/49 [06:41<09:43, 20.86s/it, acc=nan] 45%|████▍     | 22/49 [06:41<08:51, 19.68s/it, acc=nan] 45%|████▍     | 22/49 [07:00<08:51, 19.68s/it, acc=nan] 47%|████▋     | 23/49 [07:00<08:22, 19.33s/it, acc=nan] 47%|████▋     | 23/49 [07:25<08:22, 19.33s/it, acc=nan] 49%|████▉     | 24/49 [07:25<08:46, 21.07s/it, acc=nan] 49%|████▉     | 24/49 [07:45<08:46, 21.07s/it, acc=nan] 51%|█████     | 25/49 [07:45<08:21, 20.89s/it, acc=nan] 51%|█████     | 25/49 [08:05<08:21, 20.89s/it, acc=nan] 53%|█████▎    | 26/49 [08:05<07:51, 20.49s/it, acc=nan] 53%|█████▎    | 26/49 [08:20<07:51, 20.49s/it, acc=nan] 55%|█████▌    | 27/49 [08:20<06:56, 18.91s/it, acc=nan] 55%|█████▌    | 27/49 [08:38<06:56, 18.91s/it, acc=nan] 57%|█████▋    | 28/49 [08:38<06:32, 18.68s/it, acc=nan] 57%|█████▋    | 28/49 [09:03<06:32, 18.68s/it, acc=nan] 59%|█████▉    | 29/49 [09:03<06:51, 20.58s/it, acc=nan] 59%|█████▉    | 29/49 [09:22<06:51, 20.58s/it, acc=nan] 61%|██████    | 30/49 [09:22<06:19, 19.95s/it, acc=nan] 61%|██████    | 30/49 [09:42<06:19, 19.95s/it, acc=nan] 63%|██████▎   | 31/49 [09:42<06:00, 20.04s/it, acc=nan] 63%|██████▎   | 31/49 [09:58<06:00, 20.04s/it, acc=nan] 65%|██████▌   | 32/49 [09:58<05:19, 18.79s/it, acc=nan] 65%|██████▌   | 32/49 [10:16<05:19, 18.79s/it, acc=nan] 67%|██████▋   | 33/49 [10:16<04:56, 18.55s/it, acc=nan] 67%|██████▋   | 33/49 [10:31<04:56, 18.55s/it, acc=nan] 69%|██████▉   | 34/49 [10:31<04:25, 17.70s/it, acc=nan] 69%|██████▉   | 34/49 [10:56<04:25, 17.70s/it, acc=nan] 71%|███████▏  | 35/49 [10:56<04:35, 19.64s/it, acc=nan] 71%|███████▏  | 35/49 [11:19<04:35, 19.64s/it, acc=nan] 73%|███████▎  | 36/49 [11:19<04:29, 20.71s/it, acc=nan] 73%|███████▎  | 36/49 [11:43<04:29, 20.71s/it, acc=nan] 76%|███████▌  | 37/49 [11:43<04:21, 21.75s/it, acc=nan] 76%|███████▌  | 37/49 [12:02<04:21, 21.75s/it, acc=nan] 78%|███████▊  | 38/49 [12:02<03:49, 20.86s/it, acc=nan] 78%|███████▊  | 38/49 [12:22<03:49, 20.86s/it, acc=nan] 80%|███████▉  | 39/49 [12:22<03:27, 20.72s/it, acc=nan] 80%|███████▉  | 39/49 [12:45<03:27, 20.72s/it, acc=nan] 82%|████████▏ | 40/49 [12:45<03:11, 21.33s/it, acc=nan] 82%|████████▏ | 40/49 [13:11<03:11, 21.33s/it, acc=nan] 84%|████████▎ | 41/49 [13:11<03:02, 22.84s/it, acc=nan] 84%|████████▎ | 41/49 [13:34<03:02, 22.84s/it, acc=nan] 86%|████████▌ | 42/49 [13:34<02:39, 22.73s/it, acc=nan] 86%|████████▌ | 42/49 [13:52<02:39, 22.73s/it, acc=nan] 88%|████████▊ | 43/49 [13:52<02:09, 21.52s/it, acc=nan] 88%|████████▊ | 43/49 [14:13<02:09, 21.52s/it, acc=nan] 90%|████████▉ | 44/49 [14:13<01:46, 21.34s/it, acc=nan] 90%|████████▉ | 44/49 [14:28<01:46, 21.34s/it, acc=nan] 92%|█████████▏| 45/49 [14:28<01:16, 19.24s/it, acc=nan] 92%|█████████▏| 45/49 [14:47<01:16, 19.24s/it, acc=nan] 94%|█████████▍| 46/49 [14:47<00:57, 19.32s/it, acc=nan] 94%|█████████▍| 46/49 [15:06<00:57, 19.32s/it, acc=nan] 96%|█████████▌| 47/49 [15:06<00:38, 19.29s/it, acc=nan] 96%|█████████▌| 47/49 [15:27<00:38, 19.29s/it, acc=nan] 98%|█████████▊| 48/49 [15:27<00:19, 19.60s/it, acc=nan] 98%|█████████▊| 48/49 [15:43<00:19, 19.60s/it, acc=nan]100%|██████████| 49/49 [15:43<00:00, 18.49s/it, acc=nan]100%|██████████| 49/49 [15:43<00:00, 19.25s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 81.01it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1221 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▌         | 73/1221 [00:01<00:19, 58.99 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 154/1221 [00:01<00:08, 119.57 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 231/1221 [00:01<00:05, 173.36 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 308/1221 [00:01<00:04, 216.93 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 383/1221 [00:02<00:04, 189.08 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 461/1221 [00:02<00:03, 222.57 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 537/1221 [00:02<00:02, 253.14 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 613/1221 [00:03<00:02, 277.49 examples/s]Tokenizing data (num_proc=16):  62%|██████▏   | 760/1221 [00:03<00:01, 284.45 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 841/1221 [00:03<00:01, 286.53 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 993/1221 [00:03<00:00, 426.34 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1069/1221 [00:04<00:00, 368.01 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1145/1221 [00:04<00:00, 399.24 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:04<00:00, 328.32 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:04<00:00, 252.36 examples/s]
  0%|          | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/16 [00:02<?, ?it/s, acc=nan]  6%|▋         | 1/16 [00:02<00:37,  2.48s/it, acc=nan]  6%|▋         | 1/16 [00:04<00:37,  2.48s/it, acc=nan] 12%|█▎        | 2/16 [00:04<00:30,  2.17s/it, acc=nan] 12%|█▎        | 2/16 [00:06<00:30,  2.17s/it, acc=nan] 19%|█▉        | 3/16 [00:06<00:26,  2.03s/it, acc=nan] 19%|█▉        | 3/16 [00:08<00:26,  2.03s/it, acc=nan] 25%|██▌       | 4/16 [00:08<00:24,  2.05s/it, acc=nan] 25%|██▌       | 4/16 [00:10<00:24,  2.05s/it, acc=nan] 31%|███▏      | 5/16 [00:10<00:22,  2.04s/it, acc=nan] 31%|███▏      | 5/16 [00:12<00:22,  2.04s/it, acc=nan] 38%|███▊      | 6/16 [00:12<00:20,  2.01s/it, acc=nan] 38%|███▊      | 6/16 [00:16<00:20,  2.01s/it, acc=nan] 44%|████▍     | 7/16 [00:16<00:24,  2.75s/it, acc=nan] 44%|████▍     | 7/16 [00:18<00:24,  2.75s/it, acc=nan] 50%|█████     | 8/16 [00:18<00:19,  2.49s/it, acc=nan] 50%|█████     | 8/16 [00:20<00:19,  2.49s/it, acc=nan] 56%|█████▋    | 9/16 [00:20<00:15,  2.28s/it, acc=nan] 56%|█████▋    | 9/16 [00:23<00:15,  2.28s/it, acc=nan] 62%|██████▎   | 10/16 [00:23<00:15,  2.61s/it, acc=nan] 62%|██████▎   | 10/16 [00:25<00:15,  2.61s/it, acc=nan] 69%|██████▉   | 11/16 [00:25<00:11,  2.35s/it, acc=nan] 69%|██████▉   | 11/16 [00:27<00:11,  2.35s/it, acc=nan] 75%|███████▌  | 12/16 [00:27<00:08,  2.21s/it, acc=nan] 75%|███████▌  | 12/16 [00:31<00:08,  2.21s/it, acc=nan] 81%|████████▏ | 13/16 [00:31<00:08,  2.83s/it, acc=nan] 81%|████████▏ | 13/16 [00:35<00:08,  2.83s/it, acc=nan] 88%|████████▊ | 14/16 [00:35<00:06,  3.04s/it, acc=nan] 88%|████████▊ | 14/16 [00:39<00:06,  3.04s/it, acc=nan] 94%|█████████▍| 15/16 [00:39<00:03,  3.48s/it, acc=nan] 94%|█████████▍| 15/16 [00:40<00:03,  3.48s/it, acc=nan]100%|██████████| 16/16 [00:40<00:00,  2.63s/it, acc=nan]100%|██████████| 16/16 [00:40<00:00,  2.52s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 75.09it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1221 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 77/1221 [00:01<00:17, 64.34 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 154/1221 [00:01<00:08, 122.42 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 231/1221 [00:01<00:05, 175.03 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 308/1221 [00:01<00:04, 217.96 examples/s]Tokenizing data (num_proc=16):  32%|███▏      | 385/1221 [00:02<00:03, 240.49 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 461/1221 [00:02<00:03, 213.40 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 613/1221 [00:03<00:02, 253.59 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 765/1221 [00:03<00:01, 339.03 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 841/1221 [00:03<00:01, 336.71 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 917/1221 [00:03<00:00, 336.05 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 993/1221 [00:03<00:00, 328.44 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1069/1221 [00:04<00:00, 352.71 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1145/1221 [00:04<00:00, 348.69 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:04<00:00, 342.15 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:04<00:00, 259.82 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/25 [00:22<?, ?it/s, acc=nan]  4%|▍         | 1/25 [00:22<08:58, 22.44s/it, acc=nan]  4%|▍         | 1/25 [00:43<08:58, 22.44s/it, acc=nan]  8%|▊         | 2/25 [00:43<08:12, 21.43s/it, acc=nan]  8%|▊         | 2/25 [01:03<08:12, 21.43s/it, acc=nan] 12%|█▏        | 3/25 [01:03<07:41, 20.96s/it, acc=nan] 12%|█▏        | 3/25 [01:22<07:41, 20.96s/it, acc=nan] 16%|█▌        | 4/25 [01:22<07:03, 20.19s/it, acc=nan] 16%|█▌        | 4/25 [01:43<09:05, 25.98s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 79.26 GiB of which 3.60 GiB is free. Process 3329281 has 37.06 GiB memory in use. Including non-PyTorch memory, this process has 38.58 GiB memory in use. Of the allocated memory 29.17 GiB is allocated by PyTorch, and 8.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 74.00it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1009 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 48/1009 [00:01<00:25, 37.71 examples/s]Tokenizing data (num_proc=16):  11%|█         | 108/1009 [00:01<00:11, 81.39 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 172/1009 [00:01<00:06, 122.03 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 235/1009 [00:02<00:05, 153.12 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 296/1009 [00:02<00:04, 168.44 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 357/1009 [00:02<00:03, 185.07 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 422/1009 [00:02<00:02, 204.27 examples/s]Tokenizing data (num_proc=16):  47%|████▋     | 479/1009 [00:03<00:02, 203.31 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 568/1009 [00:03<00:01, 244.25 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 609/1009 [00:03<00:01, 215.33 examples/s]Tokenizing data (num_proc=16):  65%|██████▌   | 659/1009 [00:03<00:01, 208.42 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 736/1009 [00:04<00:01, 230.10 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 774/1009 [00:04<00:00, 236.15 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 813/1009 [00:04<00:00, 259.04 examples/s]Tokenizing data (num_proc=16):  84%|████████▍ | 852/1009 [00:04<00:00, 261.80 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 883/1009 [00:04<00:00, 268.99 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 921/1009 [00:04<00:00, 239.10 examples/s]Tokenizing data (num_proc=16):  96%|█████████▌| 965/1009 [00:05<00:00, 264.12 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:05<00:00, 286.45 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:05<00:00, 191.64 examples/s]
  0%|          | 0/41 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/41 [01:49<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 248.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 167.19 MiB is free. Process 3329281 has 40.47 GiB memory in use. Including non-PyTorch memory, this process has 38.60 GiB memory in use. Of the allocated memory 30.13 GiB is allocated by PyTorch, and 7.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 67.15it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1009 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 45/1009 [00:01<00:28, 34.38 examples/s]Tokenizing data (num_proc=16):   9%|▉         | 94/1009 [00:01<00:13, 69.22 examples/s]Tokenizing data (num_proc=16):  16%|█▋        | 165/1009 [00:01<00:07, 116.94 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 231/1009 [00:02<00:05, 148.81 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 292/1009 [00:02<00:04, 168.10 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 355/1009 [00:02<00:03, 190.83 examples/s]Tokenizing data (num_proc=16):  41%|████      | 416/1009 [00:02<00:02, 206.62 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 482/1009 [00:03<00:02, 218.27 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 544/1009 [00:03<00:02, 222.75 examples/s]Tokenizing data (num_proc=16):  60%|█████▉    | 605/1009 [00:03<00:01, 231.12 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 671/1009 [00:03<00:01, 241.45 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 736/1009 [00:04<00:01, 242.98 examples/s]Tokenizing data (num_proc=16):  76%|███████▌  | 766/1009 [00:04<00:00, 245.77 examples/s]Tokenizing data (num_proc=16):  79%|███████▊  | 794/1009 [00:04<00:00, 250.59 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 820/1009 [00:04<00:00, 251.11 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 875/1009 [00:04<00:00, 301.72 examples/s]Tokenizing data (num_proc=16):  91%|█████████ | 920/1009 [00:04<00:00, 257.97 examples/s]Tokenizing data (num_proc=16):  95%|█████████▌| 963/1009 [00:05<00:00, 244.67 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 998/1009 [00:05<00:00, 264.11 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:05<00:00, 190.30 examples/s]
  0%|          | 0/13 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/13 [00:06<?, ?it/s, acc=nan]  8%|▊         | 1/13 [00:06<01:18,  6.57s/it, acc=nan]  8%|▊         | 1/13 [00:13<01:18,  6.57s/it, acc=nan] 15%|█▌        | 2/13 [00:13<01:12,  6.63s/it, acc=nan] 15%|█▌        | 2/13 [00:19<01:12,  6.63s/it, acc=nan] 23%|██▎       | 3/13 [00:19<01:04,  6.50s/it, acc=nan] 23%|██▎       | 3/13 [00:25<01:04,  6.50s/it, acc=nan] 31%|███       | 4/13 [00:25<00:55,  6.12s/it, acc=nan] 31%|███       | 4/13 [00:31<00:55,  6.12s/it, acc=nan] 38%|███▊      | 5/13 [00:31<00:49,  6.16s/it, acc=nan] 38%|███▊      | 5/13 [00:37<00:49,  6.16s/it, acc=nan] 46%|████▌     | 6/13 [00:37<00:42,  6.05s/it, acc=nan] 46%|████▌     | 6/13 [00:43<00:42,  6.05s/it, acc=nan] 54%|█████▍    | 7/13 [00:43<00:36,  6.06s/it, acc=nan] 54%|█████▍    | 7/13 [00:49<00:36,  6.06s/it, acc=nan] 62%|██████▏   | 8/13 [00:49<00:30,  6.09s/it, acc=nan] 62%|██████▏   | 8/13 [00:54<00:30,  6.09s/it, acc=nan] 69%|██████▉   | 9/13 [00:54<00:23,  5.77s/it, acc=nan] 69%|██████▉   | 9/13 [01:05<00:23,  5.77s/it, acc=nan] 77%|███████▋  | 10/13 [01:05<00:22,  7.51s/it, acc=nan] 77%|███████▋  | 10/13 [01:17<00:22,  7.51s/it, acc=nan] 85%|████████▍ | 11/13 [01:17<00:17,  8.80s/it, acc=nan] 85%|████████▍ | 11/13 [01:30<00:17,  8.80s/it, acc=nan] 92%|█████████▏| 12/13 [01:30<00:10, 10.15s/it, acc=nan] 92%|█████████▏| 12/13 [01:38<00:10, 10.15s/it, acc=nan]100%|██████████| 13/13 [01:38<00:00,  9.33s/it, acc=nan]100%|██████████| 13/13 [01:38<00:00,  7.56s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/lsat/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 76.66it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1009 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 46/1009 [00:01<00:26, 36.68 examples/s]Tokenizing data (num_proc=16):  11%|█         | 113/1009 [00:01<00:10, 88.18 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 170/1009 [00:01<00:06, 120.10 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 231/1009 [00:02<00:05, 145.42 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 253/1009 [00:02<00:05, 127.15 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 358/1009 [00:02<00:03, 196.15 examples/s]Tokenizing data (num_proc=16):  41%|████      | 412/1009 [00:02<00:02, 205.90 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 484/1009 [00:03<00:02, 233.86 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 546/1009 [00:03<00:02, 179.59 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 670/1009 [00:04<00:01, 213.10 examples/s]Tokenizing data (num_proc=16):  78%|███████▊  | 791/1009 [00:04<00:00, 306.90 examples/s]Tokenizing data (num_proc=16):  83%|████████▎ | 839/1009 [00:04<00:00, 267.09 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 879/1009 [00:04<00:00, 283.31 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 922/1009 [00:04<00:00, 252.65 examples/s]Tokenizing data (num_proc=16):  96%|█████████▌| 964/1009 [00:04<00:00, 255.02 examples/s]Tokenizing data (num_proc=16): 100%|█████████▉| 1004/1009 [00:05<00:00, 278.17 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1009/1009 [00:05<00:00, 193.58 examples/s]
  0%|          | 0/41 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/41 [01:55<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 79.19 MiB is free. Process 3329281 has 40.47 GiB memory in use. Including non-PyTorch memory, this process has 38.69 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 7.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 74.97it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 74.22it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/crows/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 67.94it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/crows/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 78.54it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 76.45it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/sst-2/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 79.43it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 87, in main
    raw_dataset = load_dataset("json", data_files={'test': main_args.data_file})['test']
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 2074, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1795, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 1551, in dataset_module_factory
    ).get_module()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/load.py", line 935, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 721, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 624, in from_patterns
    resolve_pattern(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/datasets/data_files.py", line 411, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/benchmark/sst-2/data.jsonl'
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 79.95it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1172 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 74/1172 [00:01<00:19, 54.92 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 146/1172 [00:01<00:09, 108.24 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 222/1172 [00:01<00:05, 160.63 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 296/1172 [00:01<00:04, 201.59 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 369/1172 [00:02<00:03, 231.73 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 442/1172 [00:02<00:02, 258.65 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 515/1172 [00:02<00:02, 265.20 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 588/1172 [00:02<00:02, 271.71 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 734/1172 [00:03<00:01, 371.10 examples/s]Tokenizing data (num_proc=16):  69%|██████▊   | 803/1172 [00:03<00:01, 260.01 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 950/1172 [00:03<00:00, 353.45 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 998/1172 [00:04<00:00, 252.08 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1099/1172 [00:04<00:00, 300.14 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1160/1172 [00:04<00:00, 293.57 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 238.86 examples/s]
  0%|          | 0/47 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/47 [00:36<?, ?it/s, acc=nan]  2%|▏         | 1/47 [00:36<28:11, 36.78s/it, acc=nan]  2%|▏         | 1/47 [01:03<28:11, 36.78s/it, acc=nan]  4%|▍         | 2/47 [01:03<23:03, 30.74s/it, acc=nan]  4%|▍         | 2/47 [01:45<39:23, 52.52s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.91 GiB. GPU 0 has a total capacity of 79.26 GiB of which 4.16 GiB is free. Process 3329281 has 37.38 GiB memory in use. Including non-PyTorch memory, this process has 37.70 GiB memory in use. Of the allocated memory 28.80 GiB is allocated by PyTorch, and 8.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 72.17it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1172 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▌         | 67/1172 [00:01<00:20, 53.51 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 145/1172 [00:01<00:08, 114.18 examples/s]Tokenizing data (num_proc=16):  19%|█▊        | 217/1172 [00:01<00:05, 162.16 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 286/1172 [00:02<00:05, 157.97 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 369/1172 [00:02<00:04, 196.86 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 513/1172 [00:02<00:02, 285.58 examples/s]Tokenizing data (num_proc=16):  49%|████▊     | 570/1172 [00:02<00:02, 255.83 examples/s]Tokenizing data (num_proc=16):  56%|█████▌    | 654/1172 [00:03<00:01, 275.33 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 784/1172 [00:03<00:01, 279.13 examples/s]Tokenizing data (num_proc=16):  75%|███████▍  | 874/1172 [00:03<00:00, 308.67 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 950/1172 [00:04<00:00, 285.82 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1023/1172 [00:04<00:00, 311.73 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1095/1172 [00:04<00:00, 289.97 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 240.38 examples/s]
  0%|          | 0/15 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/15 [00:03<?, ?it/s, acc=nan]  7%|▋         | 1/15 [00:03<00:51,  3.70s/it, acc=nan]  7%|▋         | 1/15 [00:06<00:51,  3.70s/it, acc=nan] 13%|█▎        | 2/15 [00:06<00:41,  3.21s/it, acc=nan] 13%|█▎        | 2/15 [00:10<00:41,  3.21s/it, acc=nan] 20%|██        | 3/15 [00:10<00:40,  3.34s/it, acc=nan] 20%|██        | 3/15 [00:13<00:40,  3.34s/it, acc=nan] 27%|██▋       | 4/15 [00:13<00:37,  3.44s/it, acc=nan] 27%|██▋       | 4/15 [00:17<00:37,  3.44s/it, acc=nan] 33%|███▎      | 5/15 [00:17<00:34,  3.41s/it, acc=nan] 33%|███▎      | 5/15 [00:20<00:34,  3.41s/it, acc=nan] 40%|████      | 6/15 [00:20<00:30,  3.38s/it, acc=nan] 40%|████      | 6/15 [00:23<00:30,  3.38s/it, acc=nan] 47%|████▋     | 7/15 [00:23<00:27,  3.38s/it, acc=nan] 47%|████▋     | 7/15 [00:26<00:27,  3.38s/it, acc=nan] 53%|█████▎    | 8/15 [00:26<00:22,  3.26s/it, acc=nan] 53%|█████▎    | 8/15 [00:29<00:22,  3.26s/it, acc=nan] 60%|██████    | 9/15 [00:29<00:18,  3.05s/it, acc=nan] 60%|██████    | 9/15 [00:31<00:18,  3.05s/it, acc=nan] 67%|██████▋   | 10/15 [00:31<00:14,  2.93s/it, acc=nan] 67%|██████▋   | 10/15 [00:35<00:14,  2.93s/it, acc=nan] 73%|███████▎  | 11/15 [00:35<00:12,  3.09s/it, acc=nan] 73%|███████▎  | 11/15 [00:38<00:12,  3.09s/it, acc=nan] 80%|████████  | 12/15 [00:38<00:09,  3.19s/it, acc=nan] 80%|████████  | 12/15 [00:41<00:09,  3.19s/it, acc=nan] 87%|████████▋ | 13/15 [00:41<00:06,  3.08s/it, acc=nan] 87%|████████▋ | 13/15 [00:44<00:06,  3.08s/it, acc=nan] 93%|█████████▎| 14/15 [00:44<00:03,  3.02s/it, acc=nan] 93%|█████████▎| 14/15 [00:46<00:03,  3.02s/it, acc=nan]100%|██████████| 15/15 [00:46<00:00,  2.66s/it, acc=nan]100%|██████████| 15/15 [00:46<00:00,  3.09s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_challenge/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 76.58it/s]
Tokenizing data (num_proc=16):   0%|          | 0/1172 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 74/1172 [00:01<00:19, 57.31 examples/s]Tokenizing data (num_proc=16):  11%|█▏        | 134/1172 [00:01<00:10, 97.11 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 222/1172 [00:01<00:06, 149.20 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 274/1172 [00:02<00:05, 168.45 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 369/1172 [00:02<00:03, 221.41 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 440/1172 [00:02<00:03, 236.54 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 515/1172 [00:02<00:02, 248.78 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 579/1172 [00:03<00:02, 204.21 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 734/1172 [00:03<00:01, 300.15 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 807/1172 [00:04<00:01, 252.70 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 880/1172 [00:04<00:01, 286.50 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 953/1172 [00:04<00:00, 312.95 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1026/1172 [00:04<00:00, 332.31 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1099/1172 [00:04<00:00, 379.52 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:04<00:00, 330.89 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1172/1172 [00:05<00:00, 232.20 examples/s]
  0%|          | 0/24 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/24 [00:45<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.58 GiB. GPU 0 has a total capacity of 79.26 GiB of which 6.24 GiB is free. Process 3329281 has 37.39 GiB memory in use. Including non-PyTorch memory, this process has 35.62 GiB memory in use. Of the allocated memory 27.88 GiB is allocated by PyTorch, and 7.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 67.78it/s]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 73/2376 [00:01<00:37, 61.57 examples/s]Tokenizing data (num_proc=16):   6%|▋         | 149/2376 [00:01<00:18, 119.55 examples/s]Tokenizing data (num_proc=16):  10%|▉         | 230/2376 [00:01<00:12, 173.86 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 447/2376 [00:01<00:05, 360.82 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 596/2376 [00:02<00:04, 424.16 examples/s]Tokenizing data (num_proc=16):  34%|███▍      | 810/2376 [00:02<00:02, 549.12 examples/s]Tokenizing data (num_proc=16):  37%|███▋      | 881/2376 [00:02<00:03, 478.88 examples/s]Tokenizing data (num_proc=16):  47%|████▋     | 1109/2376 [00:02<00:02, 622.71 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 1192/2376 [00:03<00:02, 550.59 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 1340/2376 [00:03<00:01, 566.19 examples/s]Tokenizing data (num_proc=16):  59%|█████▉    | 1405/2376 [00:03<00:02, 444.02 examples/s]Tokenizing data (num_proc=16):  72%|███████▏  | 1699/2376 [00:03<00:01, 633.21 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 1932/2376 [00:04<00:00, 844.29 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 2080/2376 [00:04<00:00, 725.21 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 2170/2376 [00:04<00:00, 612.33 examples/s]Tokenizing data (num_proc=16):  97%|█████████▋| 2307/2376 [00:04<00:00, 674.52 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 487.62 examples/s]
  0%|          | 0/96 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/96 [00:32<?, ?it/s, acc=nan]  1%|          | 1/96 [00:32<51:56, 32.81s/it, acc=nan]  1%|          | 1/96 [01:05<51:56, 32.81s/it, acc=nan]  2%|▏         | 2/96 [01:05<51:25, 32.82s/it, acc=nan]  2%|▏         | 2/96 [01:28<51:25, 32.82s/it, acc=nan]  3%|▎         | 3/96 [01:28<43:56, 28.35s/it, acc=nan]  3%|▎         | 3/96 [01:57<43:56, 28.35s/it, acc=nan]  4%|▍         | 4/96 [01:57<44:01, 28.71s/it, acc=nan]  4%|▍         | 4/96 [02:21<44:01, 28.71s/it, acc=nan]  5%|▌         | 5/96 [02:21<40:43, 26.85s/it, acc=nan]  5%|▌         | 5/96 [02:45<40:43, 26.85s/it, acc=nan]  6%|▋         | 6/96 [02:45<38:52, 25.92s/it, acc=nan]  6%|▋         | 6/96 [03:21<38:52, 25.92s/it, acc=nan]  7%|▋         | 7/96 [03:21<43:08, 29.08s/it, acc=nan]  7%|▋         | 7/96 [03:45<43:08, 29.08s/it, acc=nan]  8%|▊         | 8/96 [03:45<40:31, 27.63s/it, acc=nan]  8%|▊         | 8/96 [04:18<40:31, 27.63s/it, acc=nan]  9%|▉         | 9/96 [04:18<42:22, 29.22s/it, acc=nan]  9%|▉         | 9/96 [04:38<42:22, 29.22s/it, acc=nan] 10%|█         | 10/96 [04:38<37:39, 26.28s/it, acc=nan] 10%|█         | 10/96 [04:59<37:39, 26.28s/it, acc=nan] 11%|█▏        | 11/96 [04:59<35:13, 24.87s/it, acc=nan] 11%|█▏        | 11/96 [05:26<35:13, 24.87s/it, acc=nan] 12%|█▎        | 12/96 [05:26<35:38, 25.46s/it, acc=nan] 12%|█▎        | 12/96 [05:52<35:38, 25.46s/it, acc=nan] 14%|█▎        | 13/96 [05:52<35:21, 25.56s/it, acc=nan] 14%|█▎        | 13/96 [06:18<35:21, 25.56s/it, acc=nan] 15%|█▍        | 14/96 [06:18<35:15, 25.80s/it, acc=nan] 15%|█▍        | 14/96 [06:49<35:15, 25.80s/it, acc=nan] 16%|█▌        | 15/96 [06:49<36:42, 27.19s/it, acc=nan] 16%|█▌        | 15/96 [07:16<36:42, 27.19s/it, acc=nan] 17%|█▋        | 16/96 [07:16<36:25, 27.32s/it, acc=nan] 17%|█▋        | 16/96 [07:41<36:25, 27.32s/it, acc=nan] 18%|█▊        | 17/96 [07:41<35:03, 26.63s/it, acc=nan] 18%|█▊        | 17/96 [08:15<35:03, 26.63s/it, acc=nan] 19%|█▉        | 18/96 [08:15<37:14, 28.65s/it, acc=nan] 19%|█▉        | 18/96 [08:35<37:14, 28.65s/it, acc=nan] 20%|█▉        | 19/96 [08:35<33:34, 26.17s/it, acc=nan] 20%|█▉        | 19/96 [09:01<33:34, 26.17s/it, acc=nan] 21%|██        | 20/96 [09:01<32:58, 26.03s/it, acc=nan] 21%|██        | 20/96 [09:23<32:58, 26.03s/it, acc=nan] 22%|██▏       | 21/96 [09:23<31:11, 24.95s/it, acc=nan] 22%|██▏       | 21/96 [09:47<31:11, 24.95s/it, acc=nan] 23%|██▎       | 22/96 [09:47<30:26, 24.68s/it, acc=nan] 23%|██▎       | 22/96 [10:10<30:26, 24.68s/it, acc=nan] 24%|██▍       | 23/96 [10:10<29:26, 24.21s/it, acc=nan] 24%|██▍       | 23/96 [10:36<29:26, 24.21s/it, acc=nan] 25%|██▌       | 24/96 [10:36<29:27, 24.55s/it, acc=nan] 25%|██▌       | 24/96 [11:03<29:27, 24.55s/it, acc=nan] 26%|██▌       | 25/96 [11:03<30:09, 25.48s/it, acc=nan] 26%|██▌       | 25/96 [11:26<30:09, 25.48s/it, acc=nan] 27%|██▋       | 26/96 [11:26<28:50, 24.72s/it, acc=nan] 27%|██▋       | 26/96 [11:59<28:50, 24.72s/it, acc=nan] 28%|██▊       | 27/96 [11:59<31:10, 27.10s/it, acc=nan] 28%|██▊       | 27/96 [12:23<31:10, 27.10s/it, acc=nan] 29%|██▉       | 28/96 [12:23<29:42, 26.22s/it, acc=nan] 29%|██▉       | 28/96 [12:48<29:42, 26.22s/it, acc=nan] 30%|███       | 29/96 [12:48<28:51, 25.84s/it, acc=nan] 30%|███       | 29/96 [13:10<28:51, 25.84s/it, acc=nan] 31%|███▏      | 30/96 [13:10<27:15, 24.78s/it, acc=nan] 31%|███▏      | 30/96 [13:37<27:15, 24.78s/it, acc=nan] 32%|███▏      | 31/96 [13:37<27:26, 25.33s/it, acc=nan] 32%|███▏      | 31/96 [14:01<27:26, 25.33s/it, acc=nan] 33%|███▎      | 32/96 [14:01<26:39, 25.00s/it, acc=nan] 33%|███▎      | 32/96 [14:29<26:39, 25.00s/it, acc=nan] 34%|███▍      | 33/96 [14:29<27:04, 25.78s/it, acc=nan] 34%|███▍      | 33/96 [15:04<27:04, 25.78s/it, acc=nan] 35%|███▌      | 34/96 [15:04<29:29, 28.54s/it, acc=nan] 35%|███▌      | 34/96 [15:31<29:29, 28.54s/it, acc=nan] 36%|███▋      | 35/96 [15:31<28:38, 28.17s/it, acc=nan] 36%|███▋      | 35/96 [15:55<28:38, 28.17s/it, acc=nan] 38%|███▊      | 36/96 [15:55<26:52, 26.88s/it, acc=nan] 38%|███▊      | 36/96 [16:23<26:52, 26.88s/it, acc=nan] 39%|███▊      | 37/96 [16:23<26:52, 27.34s/it, acc=nan] 39%|███▊      | 37/96 [16:52<26:52, 27.34s/it, acc=nan] 40%|███▉      | 38/96 [16:52<26:41, 27.61s/it, acc=nan] 40%|███▉      | 38/96 [17:17<26:41, 27.61s/it, acc=nan] 41%|████      | 39/96 [17:17<25:30, 26.85s/it, acc=nan] 41%|████      | 39/96 [17:43<25:30, 26.85s/it, acc=nan] 42%|████▏     | 40/96 [17:43<24:50, 26.61s/it, acc=nan] 42%|████▏     | 40/96 [18:20<24:50, 26.61s/it, acc=nan] 43%|████▎     | 41/96 [18:20<27:17, 29.78s/it, acc=nan] 43%|████▎     | 41/96 [18:46<27:17, 29.78s/it, acc=nan] 44%|████▍     | 42/96 [18:46<25:51, 28.74s/it, acc=nan] 44%|████▍     | 42/96 [19:14<25:51, 28.74s/it, acc=nan] 45%|████▍     | 43/96 [19:14<25:03, 28.37s/it, acc=nan] 45%|████▍     | 43/96 [19:39<25:03, 28.37s/it, acc=nan] 46%|████▌     | 44/96 [19:39<23:40, 27.32s/it, acc=nan] 46%|████▌     | 44/96 [20:03<23:40, 27.32s/it, acc=nan] 47%|████▋     | 45/96 [20:03<22:24, 26.36s/it, acc=nan] 47%|████▋     | 45/96 [20:24<22:24, 26.36s/it, acc=nan] 48%|████▊     | 46/96 [20:24<20:46, 24.92s/it, acc=nan] 48%|████▊     | 46/96 [20:50<20:46, 24.92s/it, acc=nan] 49%|████▉     | 47/96 [20:50<20:30, 25.12s/it, acc=nan] 49%|████▉     | 47/96 [21:22<20:30, 25.12s/it, acc=nan] 50%|█████     | 48/96 [21:22<21:44, 27.17s/it, acc=nan] 50%|█████     | 48/96 [21:40<21:44, 27.17s/it, acc=nan] 51%|█████     | 49/96 [21:40<19:14, 24.56s/it, acc=nan] 51%|█████     | 49/96 [22:02<19:14, 24.56s/it, acc=nan] 52%|█████▏    | 50/96 [22:02<18:12, 23.74s/it, acc=nan] 52%|█████▏    | 50/96 [22:28<18:12, 23.74s/it, acc=nan] 53%|█████▎    | 51/96 [22:28<18:20, 24.46s/it, acc=nan] 53%|█████▎    | 51/96 [22:56<18:20, 24.46s/it, acc=nan] 54%|█████▍    | 52/96 [22:56<18:34, 25.33s/it, acc=nan] 54%|█████▍    | 52/96 [23:21<18:34, 25.33s/it, acc=nan] 55%|█████▌    | 53/96 [23:21<18:15, 25.49s/it, acc=nan] 55%|█████▌    | 53/96 [23:39<18:15, 25.49s/it, acc=nan] 56%|█████▋    | 54/96 [23:39<16:07, 23.04s/it, acc=nan] 56%|█████▋    | 54/96 [24:01<16:07, 23.04s/it, acc=nan] 57%|█████▋    | 55/96 [24:01<15:36, 22.83s/it, acc=nan] 57%|█████▋    | 55/96 [24:23<15:36, 22.83s/it, acc=nan] 58%|█████▊    | 56/96 [24:23<14:58, 22.47s/it, acc=nan] 58%|█████▊    | 56/96 [24:49<14:58, 22.47s/it, acc=nan] 59%|█████▉    | 57/96 [24:49<15:15, 23.47s/it, acc=nan] 59%|█████▉    | 57/96 [25:12<15:15, 23.47s/it, acc=nan] 60%|██████    | 58/96 [25:12<14:47, 23.35s/it, acc=nan] 60%|██████    | 58/96 [25:34<14:47, 23.35s/it, acc=nan] 61%|██████▏   | 59/96 [25:34<14:15, 23.13s/it, acc=nan] 61%|██████▏   | 59/96 [25:54<14:15, 23.13s/it, acc=nan] 62%|██████▎   | 60/96 [25:54<13:11, 22.00s/it, acc=nan] 62%|██████▎   | 60/96 [26:09<13:11, 22.00s/it, acc=nan] 64%|██████▎   | 61/96 [26:09<11:37, 19.94s/it, acc=nan] 64%|██████▎   | 61/96 [26:33<11:37, 19.94s/it, acc=nan] 65%|██████▍   | 62/96 [26:33<12:00, 21.19s/it, acc=nan] 65%|██████▍   | 62/96 [26:59<12:00, 21.19s/it, acc=nan] 66%|██████▌   | 63/96 [26:59<12:27, 22.66s/it, acc=nan] 66%|██████▌   | 63/96 [27:27<12:27, 22.66s/it, acc=nan] 67%|██████▋   | 64/96 [27:27<12:57, 24.29s/it, acc=nan] 67%|██████▋   | 64/96 [27:53<12:57, 24.29s/it, acc=nan] 68%|██████▊   | 65/96 [27:53<12:44, 24.66s/it, acc=nan] 68%|██████▊   | 65/96 [28:18<12:44, 24.66s/it, acc=nan] 69%|██████▉   | 66/96 [28:18<12:26, 24.89s/it, acc=nan] 69%|██████▉   | 66/96 [28:43<12:26, 24.89s/it, acc=nan] 70%|██████▉   | 67/96 [28:43<12:03, 24.96s/it, acc=nan] 70%|██████▉   | 67/96 [29:13<12:03, 24.96s/it, acc=nan] 71%|███████   | 68/96 [29:13<12:21, 26.48s/it, acc=nan] 71%|███████   | 68/96 [29:39<12:21, 26.48s/it, acc=nan] 72%|███████▏  | 69/96 [29:39<11:50, 26.33s/it, acc=nan] 72%|███████▏  | 69/96 [30:09<11:50, 26.33s/it, acc=nan] 73%|███████▎  | 70/96 [30:09<11:56, 27.54s/it, acc=nan] 73%|███████▎  | 70/96 [30:43<11:56, 27.54s/it, acc=nan] 74%|███████▍  | 71/96 [30:43<12:15, 29.44s/it, acc=nan] 74%|███████▍  | 71/96 [31:07<12:15, 29.44s/it, acc=nan] 75%|███████▌  | 72/96 [31:07<11:06, 27.77s/it, acc=nan] 75%|███████▌  | 72/96 [31:43<11:06, 27.77s/it, acc=nan] 76%|███████▌  | 73/96 [31:43<11:32, 30.11s/it, acc=nan] 76%|███████▌  | 73/96 [32:11<11:32, 30.11s/it, acc=nan] 77%|███████▋  | 74/96 [32:11<10:51, 29.60s/it, acc=nan] 77%|███████▋  | 74/96 [32:34<10:51, 29.60s/it, acc=nan] 78%|███████▊  | 75/96 [32:34<09:41, 27.68s/it, acc=nan] 78%|███████▊  | 75/96 [33:05<09:41, 27.68s/it, acc=nan] 79%|███████▉  | 76/96 [33:05<09:29, 28.48s/it, acc=nan] 79%|███████▉  | 76/96 [33:29<09:29, 28.48s/it, acc=nan] 80%|████████  | 77/96 [33:29<08:36, 27.18s/it, acc=nan] 80%|████████  | 77/96 [33:55<08:36, 27.18s/it, acc=nan] 81%|████████▏ | 78/96 [33:55<08:01, 26.76s/it, acc=nan] 81%|████████▏ | 78/96 [34:19<08:01, 26.76s/it, acc=nan] 82%|████████▏ | 79/96 [34:19<07:22, 26.03s/it, acc=nan] 82%|████████▏ | 79/96 [34:41<07:22, 26.03s/it, acc=nan] 83%|████████▎ | 80/96 [34:41<06:35, 24.75s/it, acc=nan] 83%|████████▎ | 80/96 [35:42<06:35, 24.75s/it, acc=nan] 84%|████████▍ | 81/96 [35:42<08:55, 35.69s/it, acc=nan] 84%|████████▍ | 81/96 [36:05<08:55, 35.69s/it, acc=nan] 85%|████████▌ | 82/96 [36:05<07:24, 31.77s/it, acc=nan] 85%|████████▌ | 82/96 [36:18<07:24, 31.77s/it, acc=nan] 86%|████████▋ | 83/96 [36:18<05:42, 26.37s/it, acc=nan] 86%|████████▋ | 83/96 [36:37<05:42, 26.37s/it, acc=nan] 88%|████████▊ | 84/96 [36:37<04:48, 24.03s/it, acc=nan] 88%|████████▊ | 84/96 [36:53<04:48, 24.03s/it, acc=nan] 89%|████████▊ | 85/96 [36:53<03:58, 21.70s/it, acc=nan] 89%|████████▊ | 85/96 [37:13<03:58, 21.70s/it, acc=nan] 90%|████████▉ | 86/96 [37:13<03:30, 21.05s/it, acc=nan] 90%|████████▉ | 86/96 [37:30<03:30, 21.05s/it, acc=nan] 91%|█████████ | 87/96 [37:30<03:00, 20.00s/it, acc=nan] 91%|█████████ | 87/96 [37:49<03:00, 20.00s/it, acc=nan] 92%|█████████▏| 88/96 [37:49<02:37, 19.71s/it, acc=nan] 92%|█████████▏| 88/96 [38:08<02:37, 19.71s/it, acc=nan] 93%|█████████▎| 89/96 [38:08<02:15, 19.41s/it, acc=nan] 93%|█████████▎| 89/96 [38:40<02:15, 19.41s/it, acc=nan] 94%|█████████▍| 90/96 [38:40<02:19, 23.24s/it, acc=nan] 94%|█████████▍| 90/96 [38:58<02:19, 23.24s/it, acc=nan] 95%|█████████▍| 91/96 [38:58<01:47, 21.58s/it, acc=nan] 95%|█████████▍| 91/96 [39:17<01:47, 21.58s/it, acc=nan] 96%|█████████▌| 92/96 [39:17<01:23, 20.76s/it, acc=nan] 96%|█████████▌| 92/96 [39:35<01:23, 20.76s/it, acc=nan] 97%|█████████▋| 93/96 [39:35<01:00, 20.04s/it, acc=nan] 97%|█████████▋| 93/96 [39:55<01:00, 20.04s/it, acc=nan] 98%|█████████▊| 94/96 [39:55<00:39, 19.85s/it, acc=nan] 98%|█████████▊| 94/96 [40:12<00:39, 19.85s/it, acc=nan] 99%|█████████▉| 95/96 [40:12<00:19, 19.01s/it, acc=nan] 99%|█████████▉| 95/96 [40:17<00:19, 19.01s/it, acc=nan]100%|██████████| 96/96 [40:17<00:00, 15.04s/it, acc=nan]100%|██████████| 96/96 [40:17<00:00, 25.19s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:09,  4.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 73/2376 [00:01<00:39, 57.60 examples/s]Tokenizing data (num_proc=16):   9%|▉         | 223/2376 [00:01<00:11, 183.36 examples/s]Tokenizing data (num_proc=16):  19%|█▊        | 441/2376 [00:01<00:05, 364.80 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 519/2376 [00:01<00:05, 357.39 examples/s]Tokenizing data (num_proc=16):  30%|███       | 722/2376 [00:02<00:03, 500.81 examples/s]Tokenizing data (num_proc=16):  37%|███▋      | 885/2376 [00:02<00:02, 563.73 examples/s]Tokenizing data (num_proc=16):  41%|████      | 963/2376 [00:02<00:02, 506.18 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 1192/2376 [00:02<00:01, 644.83 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 1340/2376 [00:03<00:01, 644.48 examples/s]Tokenizing data (num_proc=16):  59%|█████▉    | 1410/2376 [00:03<00:01, 555.66 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 1557/2376 [00:03<00:01, 582.04 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 1839/2376 [00:03<00:00, 836.32 examples/s]Tokenizing data (num_proc=16):  84%|████████▍ | 1998/2376 [00:03<00:00, 710.17 examples/s]Tokenizing data (num_proc=16):  90%|█████████ | 2147/2376 [00:04<00:00, 804.31 examples/s]Tokenizing data (num_proc=16):  97%|█████████▋| 2308/2376 [00:04<00:00, 739.76 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 525.18 examples/s]
  0%|          | 0/30 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/30 [00:03<?, ?it/s, acc=nan]  3%|▎         | 1/30 [00:03<01:46,  3.67s/it, acc=nan]  3%|▎         | 1/30 [00:07<01:46,  3.67s/it, acc=nan]  7%|▋         | 2/30 [00:07<01:38,  3.53s/it, acc=nan]  7%|▋         | 2/30 [00:09<01:38,  3.53s/it, acc=nan] 10%|█         | 3/30 [00:09<01:26,  3.21s/it, acc=nan] 10%|█         | 3/30 [00:13<01:26,  3.21s/it, acc=nan] 13%|█▎        | 4/30 [00:13<01:24,  3.25s/it, acc=nan] 13%|█▎        | 4/30 [00:15<01:24,  3.25s/it, acc=nan] 17%|█▋        | 5/30 [00:15<01:16,  3.06s/it, acc=nan] 17%|█▋        | 5/30 [00:18<01:16,  3.06s/it, acc=nan] 20%|██        | 6/30 [00:18<01:12,  3.04s/it, acc=nan] 20%|██        | 6/30 [00:22<01:12,  3.04s/it, acc=nan] 23%|██▎       | 7/30 [00:22<01:10,  3.06s/it, acc=nan] 23%|██▎       | 7/30 [00:24<01:10,  3.06s/it, acc=nan] 27%|██▋       | 8/30 [00:24<01:05,  2.96s/it, acc=nan] 27%|██▋       | 8/30 [00:27<01:05,  2.96s/it, acc=nan] 30%|███       | 9/30 [00:27<01:00,  2.90s/it, acc=nan] 30%|███       | 9/30 [00:30<01:00,  2.90s/it, acc=nan] 33%|███▎      | 10/30 [00:30<00:58,  2.92s/it, acc=nan] 33%|███▎      | 10/30 [00:34<00:58,  2.92s/it, acc=nan] 37%|███▋      | 11/30 [00:34<01:01,  3.24s/it, acc=nan] 37%|███▋      | 11/30 [00:37<01:01,  3.24s/it, acc=nan] 40%|████      | 12/30 [00:37<00:55,  3.06s/it, acc=nan] 40%|████      | 12/30 [00:40<00:55,  3.06s/it, acc=nan] 43%|████▎     | 13/30 [00:40<00:53,  3.17s/it, acc=nan] 43%|████▎     | 13/30 [00:43<00:53,  3.17s/it, acc=nan] 47%|████▋     | 14/30 [00:43<00:48,  3.03s/it, acc=nan] 47%|████▋     | 14/30 [00:46<00:48,  3.03s/it, acc=nan] 50%|█████     | 15/30 [00:46<00:44,  2.95s/it, acc=nan] 50%|█████     | 15/30 [00:49<00:44,  2.95s/it, acc=nan] 53%|█████▎    | 16/30 [00:49<00:41,  2.96s/it, acc=nan] 53%|█████▎    | 16/30 [00:52<00:41,  2.96s/it, acc=nan] 57%|█████▋    | 17/30 [00:52<00:38,  2.99s/it, acc=nan] 57%|█████▋    | 17/30 [00:54<00:38,  2.99s/it, acc=nan] 60%|██████    | 18/30 [00:54<00:35,  2.93s/it, acc=nan] 60%|██████    | 18/30 [00:58<00:35,  2.93s/it, acc=nan] 63%|██████▎   | 19/30 [00:58<00:33,  3.05s/it, acc=nan] 63%|██████▎   | 19/30 [01:01<00:33,  3.05s/it, acc=nan] 67%|██████▋   | 20/30 [01:01<00:31,  3.19s/it, acc=nan] 67%|██████▋   | 20/30 [01:04<00:31,  3.19s/it, acc=nan] 70%|███████   | 21/30 [01:04<00:27,  3.09s/it, acc=nan] 70%|███████   | 21/30 [01:07<00:27,  3.09s/it, acc=nan] 73%|███████▎  | 22/30 [01:07<00:23,  3.00s/it, acc=nan] 73%|███████▎  | 22/30 [01:11<00:23,  3.00s/it, acc=nan] 77%|███████▋  | 23/30 [01:11<00:22,  3.23s/it, acc=nan] 77%|███████▋  | 23/30 [01:13<00:22,  3.23s/it, acc=nan] 80%|████████  | 24/30 [01:13<00:18,  3.08s/it, acc=nan] 80%|████████  | 24/30 [01:16<00:18,  3.08s/it, acc=nan] 83%|████████▎ | 25/30 [01:16<00:14,  2.95s/it, acc=nan] 83%|████████▎ | 25/30 [01:19<00:14,  2.95s/it, acc=nan] 87%|████████▋ | 26/30 [01:19<00:11,  2.89s/it, acc=nan] 87%|████████▋ | 26/30 [01:21<00:11,  2.89s/it, acc=nan] 90%|█████████ | 27/30 [01:21<00:08,  2.76s/it, acc=nan] 90%|█████████ | 27/30 [01:24<00:08,  2.76s/it, acc=nan] 93%|█████████▎| 28/30 [01:24<00:05,  2.75s/it, acc=nan] 93%|█████████▎| 28/30 [01:27<00:05,  2.75s/it, acc=nan] 97%|█████████▋| 29/30 [01:27<00:02,  2.76s/it, acc=nan] 97%|█████████▋| 29/30 [01:29<00:02,  2.76s/it, acc=nan]100%|██████████| 30/30 [01:29<00:00,  2.59s/it, acc=nan]100%|██████████| 30/30 [01:29<00:00,  2.98s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/arc_easy/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]
Tokenizing data (num_proc=16):   0%|          | 0/2376 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 82/2376 [00:01<00:29, 76.52 examples/s]Tokenizing data (num_proc=16):   6%|▋         | 149/2376 [00:01<00:17, 129.85 examples/s]Tokenizing data (num_proc=16):  10%|█         | 240/2376 [00:01<00:10, 197.88 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 447/2376 [00:01<00:04, 393.27 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 685/2376 [00:01<00:02, 579.40 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 834/2376 [00:02<00:02, 588.84 examples/s]Tokenizing data (num_proc=16):  41%|████      | 976/2376 [00:02<00:02, 599.51 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 1132/2376 [00:02<00:02, 475.62 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1429/2376 [00:03<00:01, 682.45 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 1571/2376 [00:03<00:01, 678.91 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 1723/2376 [00:03<00:00, 697.35 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1874/2376 [00:03<00:00, 675.16 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 2027/2376 [00:03<00:00, 676.20 examples/s]Tokenizing data (num_proc=16):  91%|█████████▏| 2174/2376 [00:04<00:00, 677.72 examples/s]Tokenizing data (num_proc=16):  97%|█████████▋| 2313/2376 [00:04<00:00, 731.08 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 2376/2376 [00:04<00:00, 526.85 examples/s]
  0%|          | 0/48 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/48 [00:33<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacity of 79.26 GiB of which 2.71 GiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 11.73 GiB memory in use. Of the allocated memory 8.87 GiB is allocated by PyTorch, and 2.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 63/1204 [00:01<00:20, 54.99 examples/s]Tokenizing data (num_proc=16):  11%|█         | 131/1204 [00:01<00:09, 110.65 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 216/1204 [00:01<00:05, 177.82 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 290/1204 [00:01<00:04, 220.55 examples/s]Tokenizing data (num_proc=16):  30%|███       | 366/1204 [00:02<00:03, 252.84 examples/s]Tokenizing data (num_proc=16):  36%|███▋      | 438/1204 [00:02<00:02, 282.73 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 512/1204 [00:02<00:02, 235.78 examples/s]Tokenizing data (num_proc=16):  55%|█████▌    | 663/1204 [00:03<00:01, 280.74 examples/s]Tokenizing data (num_proc=16):  68%|██████▊   | 817/1204 [00:03<00:01, 308.73 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 965/1204 [00:03<00:00, 396.35 examples/s]Tokenizing data (num_proc=16):  86%|████████▋ | 1039/1204 [00:03<00:00, 397.83 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1120/1204 [00:04<00:00, 388.46 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1191/1204 [00:04<00:00, 389.34 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:04<00:00, 274.63 examples/s]
  0%|          | 0/49 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/49 [01:13<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.77 GiB. GPU 0 has a total capacity of 79.26 GiB of which 2.29 GiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 12.14 GiB memory in use. Of the allocated memory 9.16 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▎         | 43/1204 [00:01<00:33, 34.95 examples/s]Tokenizing data (num_proc=16):   6%|▋         | 76/1204 [00:01<00:19, 56.81 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 201/1204 [00:01<00:07, 138.85 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 283/1204 [00:02<00:04, 185.47 examples/s]Tokenizing data (num_proc=16):  28%|██▊       | 336/1204 [00:02<00:04, 190.33 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 454/1204 [00:02<00:02, 252.50 examples/s]Tokenizing data (num_proc=16):  46%|████▋     | 558/1204 [00:03<00:02, 217.13 examples/s]Tokenizing data (num_proc=16):  60%|█████▉    | 718/1204 [00:03<00:01, 299.31 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 796/1204 [00:04<00:01, 242.62 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 904/1204 [00:04<00:00, 321.40 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 979/1204 [00:04<00:00, 314.74 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 1027/1204 [00:04<00:00, 318.40 examples/s]Tokenizing data (num_proc=16):  90%|█████████ | 1087/1204 [00:05<00:00, 244.03 examples/s]Tokenizing data (num_proc=16):  97%|█████████▋| 1164/1204 [00:05<00:00, 299.55 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:05<00:00, 227.27 examples/s]
  0%|          | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/16 [00:04<?, ?it/s, acc=nan]  6%|▋         | 1/16 [00:04<01:14,  4.96s/it, acc=nan]  6%|▋         | 1/16 [00:08<01:14,  4.96s/it, acc=nan] 12%|█▎        | 2/16 [00:08<00:57,  4.10s/it, acc=nan] 12%|█▎        | 2/16 [00:11<00:57,  4.10s/it, acc=nan] 19%|█▉        | 3/16 [00:11<00:48,  3.73s/it, acc=nan] 19%|█▉        | 3/16 [00:16<00:48,  3.73s/it, acc=nan] 25%|██▌       | 4/16 [00:16<00:48,  4.05s/it, acc=nan] 25%|██▌       | 4/16 [00:20<00:48,  4.05s/it, acc=nan] 31%|███▏      | 5/16 [00:20<00:43,  3.95s/it, acc=nan] 31%|███▏      | 5/16 [00:23<00:43,  3.95s/it, acc=nan] 38%|███▊      | 6/16 [00:23<00:38,  3.84s/it, acc=nan] 38%|███▊      | 6/16 [00:27<00:38,  3.84s/it, acc=nan] 44%|████▍     | 7/16 [00:27<00:34,  3.81s/it, acc=nan] 44%|████▍     | 7/16 [00:31<00:34,  3.81s/it, acc=nan] 50%|█████     | 8/16 [00:31<00:31,  3.88s/it, acc=nan] 50%|█████     | 8/16 [00:34<00:31,  3.88s/it, acc=nan] 56%|█████▋    | 9/16 [00:34<00:26,  3.73s/it, acc=nan] 56%|█████▋    | 9/16 [00:38<00:26,  3.73s/it, acc=nan] 62%|██████▎   | 10/16 [00:38<00:21,  3.61s/it, acc=nan] 62%|██████▎   | 10/16 [00:42<00:21,  3.61s/it, acc=nan] 69%|██████▉   | 11/16 [00:42<00:18,  3.70s/it, acc=nan] 69%|██████▉   | 11/16 [00:45<00:18,  3.70s/it, acc=nan] 75%|███████▌  | 12/16 [00:45<00:14,  3.63s/it, acc=nan] 75%|███████▌  | 12/16 [00:48<00:14,  3.63s/it, acc=nan] 81%|████████▏ | 13/16 [00:48<00:10,  3.51s/it, acc=nan] 81%|████████▏ | 13/16 [00:52<00:10,  3.51s/it, acc=nan] 88%|████████▊ | 14/16 [00:52<00:06,  3.42s/it, acc=nan] 88%|████████▊ | 14/16 [00:56<00:06,  3.42s/it, acc=nan] 94%|█████████▍| 15/16 [00:56<00:03,  3.80s/it, acc=nan] 94%|█████████▍| 15/16 [00:57<00:03,  3.80s/it, acc=nan]100%|██████████| 16/16 [00:57<00:00,  2.88s/it, acc=nan]100%|██████████| 16/16 [00:57<00:00,  3.59s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/FOLIO/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1204 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 38/1204 [00:01<00:35, 32.75 examples/s]Tokenizing data (num_proc=16):  11%|█         | 132/1204 [00:01<00:09, 114.81 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 219/1204 [00:01<00:05, 176.57 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 290/1204 [00:01<00:04, 204.47 examples/s]Tokenizing data (num_proc=16):  30%|███       | 366/1204 [00:02<00:03, 237.75 examples/s]Tokenizing data (num_proc=16):  36%|███▌      | 436/1204 [00:02<00:03, 254.02 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 504/1204 [00:02<00:02, 258.95 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 593/1204 [00:02<00:02, 293.44 examples/s]Tokenizing data (num_proc=16):  55%|█████▍    | 662/1204 [00:03<00:01, 295.86 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 740/1204 [00:03<00:01, 308.06 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 812/1204 [00:03<00:01, 307.93 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 889/1204 [00:03<00:00, 319.06 examples/s]Tokenizing data (num_proc=16):  80%|███████▉  | 960/1204 [00:03<00:00, 366.38 examples/s]Tokenizing data (num_proc=16):  87%|████████▋ | 1048/1204 [00:04<00:00, 369.99 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1116/1204 [00:04<00:00, 355.33 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1190/1204 [00:04<00:00, 325.26 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1204/1204 [00:04<00:00, 254.82 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/25 [00:43<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 206.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 131.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 9.53 GiB is allocated by PyTorch, and 4.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:17, 24.09 examples/s]Tokenizing data (num_proc=16):  12%|█▎        | 56/448 [00:01<00:08, 45.89 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:05, 64.88 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 112/448 [00:01<00:04, 79.62 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:02<00:03, 91.03 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:02, 97.84 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 196/448 [00:02<00:02, 101.60 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 224/448 [00:03<00:02, 82.78 examples/s] Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:03<00:01, 121.63 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 121.67 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:00, 119.92 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:03<00:00, 139.67 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:04<00:00, 125.58 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 126.80 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 115.37 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 94.45 examples/s] 
  0%|          | 0/18 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/18 [01:46<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 21.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.41 GiB memory in use. Of the allocated memory 11.04 GiB is allocated by PyTorch, and 2.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:16, 25.54 examples/s]Tokenizing data (num_proc=16):  12%|█▎        | 56/448 [00:01<00:08, 48.92 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:05, 66.59 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 112/448 [00:01<00:04, 78.88 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:02<00:04, 74.44 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:03, 85.89 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 224/448 [00:02<00:01, 126.35 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 252/448 [00:02<00:01, 125.96 examples/s]Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:03<00:01, 121.98 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 120.65 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:00, 119.53 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:03<00:00, 133.92 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:04<00:00, 112.55 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 123.38 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 125.52 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 97.66 examples/s] 
  0%|          | 0/6 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/6 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 309, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 155, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.16 GiB. GPU 0 has a total capacity of 79.26 GiB of which 341.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.10 GiB memory in use. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/gpqa/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]
Tokenizing data (num_proc=16):   0%|          | 0/448 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 28/448 [00:01<00:20, 20.93 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 84/448 [00:01<00:07, 51.09 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 112/448 [00:02<00:05, 62.06 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 140/448 [00:02<00:04, 72.30 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 168/448 [00:02<00:03, 83.11 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 196/448 [00:02<00:02, 93.49 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 224/448 [00:03<00:02, 103.25 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 252/448 [00:03<00:01, 110.15 examples/s]Tokenizing data (num_proc=16):  62%|██████▎   | 280/448 [00:03<00:01, 117.27 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 308/448 [00:03<00:01, 122.72 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 336/448 [00:03<00:00, 124.74 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 364/448 [00:04<00:00, 124.31 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 392/448 [00:04<00:00, 139.02 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 420/448 [00:04<00:00, 139.34 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 137.77 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 448/448 [00:04<00:00, 93.77 examples/s] 
  0%|          | 0/9 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/9 [01:20<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2787, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 252, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 68, in sdpa_attention_forward
    value = repeat_kv(value, module.num_key_value_groups)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 27, in repeat_kv
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 484.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 263.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.18 GiB memory in use. Of the allocated memory 10.89 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:23,  8.89 examples/s]Tokenizing data (num_proc=16):   5%|▌         | 38/756 [00:01<00:22, 31.91 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 88/756 [00:01<00:08, 75.78 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 136/756 [00:01<00:05, 110.82 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 178/756 [00:02<00:04, 131.79 examples/s]Tokenizing data (num_proc=16):  28%|██▊       | 210/756 [00:02<00:04, 133.67 examples/s]Tokenizing data (num_proc=16):  34%|███▍      | 257/756 [00:02<00:03, 151.63 examples/s]Tokenizing data (num_proc=16):  39%|███▉      | 297/756 [00:02<00:03, 148.41 examples/s]Tokenizing data (num_proc=16):  46%|████▋     | 350/756 [00:03<00:02, 165.96 examples/s]Tokenizing data (num_proc=16):  53%|█████▎    | 401/756 [00:03<00:02, 166.94 examples/s]Tokenizing data (num_proc=16):  62%|██████▏   | 468/756 [00:03<00:01, 194.14 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 509/756 [00:03<00:01, 180.43 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 580/756 [00:04<00:00, 216.14 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 610/756 [00:04<00:00, 209.58 examples/s]Tokenizing data (num_proc=16):  85%|████████▍ | 642/756 [00:04<00:00, 193.83 examples/s]Tokenizing data (num_proc=16):  89%|████████▉ | 675/756 [00:04<00:00, 176.76 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 696/756 [00:04<00:00, 181.21 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 748/756 [00:05<00:00, 224.90 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:05<00:00, 146.01 examples/s]
  0%|          | 0/31 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/31 [01:50<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.81 GiB. GPU 0 has a total capacity of 79.26 GiB of which 4.14 GiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 10.30 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:14,  9.94 examples/s]Tokenizing data (num_proc=16):   3%|▎         | 19/756 [00:01<00:45, 16.20 examples/s]Tokenizing data (num_proc=16):   8%|▊         | 57/756 [00:01<00:12, 54.63 examples/s]Tokenizing data (num_proc=16):  16%|█▌        | 118/756 [00:01<00:05, 113.54 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 166/756 [00:01<00:04, 143.72 examples/s]Tokenizing data (num_proc=16):  28%|██▊       | 212/756 [00:02<00:03, 151.88 examples/s]Tokenizing data (num_proc=16):  34%|███▍      | 257/756 [00:02<00:03, 156.61 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 320/756 [00:02<00:02, 177.09 examples/s]Tokenizing data (num_proc=16):  50%|█████     | 380/756 [00:03<00:01, 188.72 examples/s]Tokenizing data (num_proc=16):  59%|█████▉    | 448/756 [00:03<00:01, 207.92 examples/s]Tokenizing data (num_proc=16):  64%|██████▍   | 487/756 [00:03<00:01, 193.03 examples/s]Tokenizing data (num_proc=16):  71%|███████   | 537/756 [00:03<00:01, 195.12 examples/s]Tokenizing data (num_proc=16):  76%|███████▋  | 578/756 [00:03<00:00, 203.63 examples/s]Tokenizing data (num_proc=16):  81%|████████  | 611/756 [00:04<00:00, 222.75 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 650/756 [00:04<00:00, 230.46 examples/s]Tokenizing data (num_proc=16):  90%|████████▉ | 679/756 [00:04<00:00, 219.34 examples/s]Tokenizing data (num_proc=16):  96%|█████████▌| 725/756 [00:04<00:00, 188.95 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 749/756 [00:04<00:00, 196.98 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:04<00:00, 152.42 examples/s]
  0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/10 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 309, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 155, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/activations.py", line 99, in forward
    return nn.functional.silu(input)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
    return torch._C._nn.silu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.31 GiB. GPU 0 has a total capacity of 79.26 GiB of which 2.61 GiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 11.82 GiB memory in use. Of the allocated memory 9.89 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MuSR/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
Tokenizing data (num_proc=16):   0%|          | 0/756 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   1%|▏         | 11/756 [00:01<01:24,  8.77 examples/s]Tokenizing data (num_proc=16):   7%|▋         | 56/756 [00:01<00:15, 45.47 examples/s]Tokenizing data (num_proc=16):  14%|█▍        | 107/756 [00:01<00:08, 79.47 examples/s]Tokenizing data (num_proc=16):  22%|██▏       | 164/756 [00:02<00:05, 116.76 examples/s]Tokenizing data (num_proc=16):  28%|██▊       | 212/756 [00:02<00:03, 136.44 examples/s]Tokenizing data (num_proc=16):  33%|███▎      | 249/756 [00:02<00:03, 142.28 examples/s]Tokenizing data (num_proc=16):  40%|███▉      | 299/756 [00:02<00:02, 154.72 examples/s]Tokenizing data (num_proc=16):  46%|████▌     | 344/756 [00:03<00:02, 166.04 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 365/756 [00:03<00:02, 146.97 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 427/756 [00:03<00:01, 182.27 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 460/756 [00:03<00:01, 167.93 examples/s]Tokenizing data (num_proc=16):  66%|██████▌   | 496/756 [00:03<00:01, 164.02 examples/s]Tokenizing data (num_proc=16):  77%|███████▋  | 585/756 [00:04<00:00, 244.62 examples/s]Tokenizing data (num_proc=16):  84%|████████▎ | 633/756 [00:04<00:00, 259.37 examples/s]Tokenizing data (num_proc=16):  90%|████████▉ | 679/756 [00:04<00:00, 233.06 examples/s]Tokenizing data (num_proc=16):  96%|█████████▋| 729/756 [00:04<00:00, 250.68 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 756/756 [00:04<00:00, 153.73 examples/s]
  0%|          | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/16 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 22, in greedy_decoding_solve
    gen_ids = model.generate(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/generation/utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 309, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 155, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.26 GiB of which 49.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.39 GiB memory in use. Of the allocated memory 10.12 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 80/1838 [00:01<00:24, 70.43 examples/s]Tokenizing data (num_proc=16):  11%|█         | 202/1838 [00:01<00:09, 176.60 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 313/1838 [00:01<00:05, 258.90 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 432/1838 [00:01<00:05, 270.16 examples/s]Tokenizing data (num_proc=16):  31%|███▏      | 575/1838 [00:02<00:03, 361.19 examples/s]Tokenizing data (num_proc=16):  36%|███▌      | 657/1838 [00:02<00:03, 359.63 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 772/1838 [00:02<00:02, 402.68 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 886/1838 [00:02<00:02, 438.79 examples/s]Tokenizing data (num_proc=16):  56%|█████▋    | 1035/1838 [00:03<00:01, 500.26 examples/s]Tokenizing data (num_proc=16):  67%|██████▋   | 1227/1838 [00:03<00:01, 473.09 examples/s]Tokenizing data (num_proc=16):  75%|███████▌  | 1380/1838 [00:03<00:00, 602.99 examples/s]Tokenizing data (num_proc=16):  81%|████████▏ | 1495/1838 [00:03<00:00, 623.07 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1610/1838 [00:03<00:00, 705.97 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 1700/1838 [00:04<00:00, 621.54 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1818/1838 [00:04<00:00, 608.70 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 417.78 examples/s]
  0%|          | 0/74 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/74 [00:47<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.11 GiB. GPU 0 has a total capacity of 79.26 GiB of which 125.19 MiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 11.49 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   3%|▎         | 63/1838 [00:01<00:37, 47.80 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 306/1838 [00:01<00:06, 254.42 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 419/1838 [00:01<00:04, 305.71 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 531/1838 [00:01<00:03, 365.09 examples/s]Tokenizing data (num_proc=16):  35%|███▌      | 649/1838 [00:02<00:02, 411.56 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 767/1838 [00:02<00:02, 433.71 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 876/1838 [00:02<00:02, 457.85 examples/s]Tokenizing data (num_proc=16):  54%|█████▎    | 985/1838 [00:02<00:01, 464.90 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1106/1838 [00:03<00:01, 490.48 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 1218/1838 [00:03<00:01, 494.50 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 1338/1838 [00:03<00:00, 507.07 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1454/1838 [00:03<00:00, 584.87 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 1568/1838 [00:03<00:00, 547.70 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 1682/1838 [00:04<00:00, 561.96 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 1801/1838 [00:04<00:00, 560.66 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 417.31 examples/s]
  0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/23 [00:04<?, ?it/s, acc=nan]  4%|▍         | 1/23 [00:04<01:48,  4.93s/it, acc=nan]  4%|▍         | 1/23 [00:09<01:48,  4.93s/it, acc=nan]  9%|▊         | 2/23 [00:09<01:42,  4.88s/it, acc=nan]  9%|▊         | 2/23 [00:13<01:42,  4.88s/it, acc=nan] 13%|█▎        | 3/23 [00:13<01:26,  4.33s/it, acc=nan] 13%|█▎        | 3/23 [00:20<01:26,  4.33s/it, acc=nan] 17%|█▋        | 4/23 [00:20<01:39,  5.26s/it, acc=nan] 17%|█▋        | 4/23 [00:24<01:39,  5.26s/it, acc=nan] 22%|██▏       | 5/23 [00:24<01:26,  4.79s/it, acc=nan] 22%|██▏       | 5/23 [00:29<01:26,  4.79s/it, acc=nan] 26%|██▌       | 6/23 [00:29<01:23,  4.89s/it, acc=nan] 26%|██▌       | 6/23 [00:33<01:23,  4.89s/it, acc=nan] 30%|███       | 7/23 [00:33<01:13,  4.61s/it, acc=nan] 30%|███       | 7/23 [00:40<01:13,  4.61s/it, acc=nan] 35%|███▍      | 8/23 [00:40<01:23,  5.56s/it, acc=nan] 35%|███▍      | 8/23 [00:45<01:23,  5.56s/it, acc=nan] 39%|███▉      | 9/23 [00:45<01:13,  5.25s/it, acc=nan] 39%|███▉      | 9/23 [00:48<01:13,  5.25s/it, acc=nan] 43%|████▎     | 10/23 [00:48<00:59,  4.60s/it, acc=nan] 43%|████▎     | 10/23 [00:52<00:59,  4.60s/it, acc=nan] 48%|████▊     | 11/23 [00:52<00:51,  4.32s/it, acc=nan] 48%|████▊     | 11/23 [00:55<00:51,  4.32s/it, acc=nan] 52%|█████▏    | 12/23 [00:55<00:45,  4.14s/it, acc=nan] 52%|█████▏    | 12/23 [00:59<00:45,  4.14s/it, acc=nan] 57%|█████▋    | 13/23 [00:59<00:40,  4.05s/it, acc=nan] 57%|█████▋    | 13/23 [01:03<00:40,  4.05s/it, acc=nan] 61%|██████    | 14/23 [01:03<00:35,  3.96s/it, acc=nan] 61%|██████    | 14/23 [01:07<00:35,  3.96s/it, acc=nan] 65%|██████▌   | 15/23 [01:07<00:31,  3.91s/it, acc=nan] 65%|██████▌   | 15/23 [01:11<00:31,  3.91s/it, acc=nan] 70%|██████▉   | 16/23 [01:11<00:27,  3.93s/it, acc=nan] 70%|██████▉   | 16/23 [01:14<00:27,  3.93s/it, acc=nan] 74%|███████▍  | 17/23 [01:14<00:23,  3.84s/it, acc=nan] 74%|███████▍  | 17/23 [01:20<00:23,  3.84s/it, acc=nan] 78%|███████▊  | 18/23 [01:20<00:21,  4.40s/it, acc=nan] 78%|███████▊  | 18/23 [01:23<00:21,  4.40s/it, acc=nan] 83%|████████▎ | 19/23 [01:23<00:16,  4.06s/it, acc=nan] 83%|████████▎ | 19/23 [01:29<00:16,  4.06s/it, acc=nan] 87%|████████▋ | 20/23 [01:29<00:13,  4.62s/it, acc=nan] 87%|████████▋ | 20/23 [01:33<00:13,  4.62s/it, acc=nan] 91%|█████████▏| 21/23 [01:33<00:08,  4.37s/it, acc=nan] 91%|█████████▏| 21/23 [01:38<00:08,  4.37s/it, acc=nan] 96%|█████████▌| 22/23 [01:38<00:04,  4.42s/it, acc=nan] 96%|█████████▌| 22/23 [01:41<00:04,  4.42s/it, acc=nan]100%|██████████| 23/23 [01:41<00:00,  3.98s/it, acc=nan]100%|██████████| 23/23 [01:41<00:00,  4.39s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/piqa/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1838 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 92/1838 [00:01<00:22, 76.86 examples/s]Tokenizing data (num_proc=16):  11%|█         | 201/1838 [00:01<00:10, 162.88 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 307/1838 [00:01<00:06, 235.84 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 433/1838 [00:01<00:04, 318.29 examples/s]Tokenizing data (num_proc=16):  30%|██▉       | 549/1838 [00:02<00:03, 374.91 examples/s]Tokenizing data (num_proc=16):  36%|███▋      | 667/1838 [00:02<00:02, 422.44 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 771/1838 [00:02<00:02, 456.63 examples/s]Tokenizing data (num_proc=16):  48%|████▊     | 887/1838 [00:02<00:01, 490.85 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 1001/1838 [00:02<00:01, 495.96 examples/s]Tokenizing data (num_proc=16):  61%|██████▏   | 1126/1838 [00:03<00:01, 402.29 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 1357/1838 [00:03<00:00, 559.46 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1459/1838 [00:03<00:00, 574.88 examples/s]Tokenizing data (num_proc=16):  85%|████████▌ | 1568/1838 [00:04<00:00, 480.23 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 1724/1838 [00:04<00:00, 626.16 examples/s]Tokenizing data (num_proc=16):  99%|█████████▊| 1815/1838 [00:04<00:00, 618.65 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1838/1838 [00:04<00:00, 413.20 examples/s]
  0%|          | 0/37 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|          | 0/37 [00:38<?, ?it/s]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.16 GiB. GPU 0 has a total capacity of 79.26 GiB of which 2.57 GiB is free. Process 4063728 has 64.81 GiB memory in use. Including non-PyTorch memory, this process has 11.86 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/cot/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▍         | 96/1954 [00:01<00:23, 78.43 examples/s]Tokenizing data (num_proc=16):  12%|█▏        | 226/1954 [00:01<00:09, 184.96 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 333/1954 [00:01<00:06, 256.53 examples/s]Tokenizing data (num_proc=16):  24%|██▎       | 461/1954 [00:01<00:04, 336.96 examples/s]Tokenizing data (num_proc=16):  30%|██▉       | 584/1954 [00:02<00:03, 394.06 examples/s]Tokenizing data (num_proc=16):  36%|███▋      | 709/1954 [00:02<00:02, 441.54 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 961/1954 [00:02<00:01, 659.00 examples/s]Tokenizing data (num_proc=16):  55%|█████▌    | 1076/1954 [00:02<00:01, 615.97 examples/s]Tokenizing data (num_proc=16):  61%|██████    | 1195/1954 [00:03<00:01, 477.55 examples/s]Tokenizing data (num_proc=16):  68%|██████▊   | 1322/1954 [00:03<00:01, 510.01 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1543/1954 [00:03<00:00, 619.64 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 1684/1954 [00:03<00:00, 658.07 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1811/1954 [00:03<00:00, 714.17 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1932/1954 [00:04<00:00, 598.46 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:04<00:00, 451.96 examples/s]
  0%|          | 0/79 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/79 [00:34<?, ?it/s, acc=nan]  1%|▏         | 1/79 [00:34<44:43, 34.40s/it, acc=nan]  1%|▏         | 1/79 [01:10<44:43, 34.40s/it, acc=nan]  3%|▎         | 2/79 [01:10<45:43, 35.63s/it, acc=nan]  3%|▎         | 2/79 [01:44<45:43, 35.63s/it, acc=nan]  4%|▍         | 3/79 [01:44<43:59, 34.73s/it, acc=nan]  4%|▍         | 3/79 [02:17<57:51, 45.68s/it, acc=nan]
Traceback (most recent call last):
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 137, in <module>
    main()
  File "/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/main.py", line 121, in main
    outputs = solve(model, tokenizer, task, batch, args=decoding_args,device=device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 73, in solve
    return greedy_decoding_solve(model, tokenizer, task, batch, args, device)
  File "/home/aswini/Fifth_project_on_dialogue/random/Token_Signature-master/solve.py", line 29, in greedy_decoding_solve
    gen_probs = torch.stack(gen_ids['scores'], dim=1).softmax(-1)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.13 GiB. GPU 0 has a total capacity of 79.26 GiB of which 505.19 MiB is free. Process 4063728 has 64.94 GiB memory in use. Including non-PyTorch memory, this process has 13.80 GiB memory in use. Of the allocated memory 9.45 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/direct_answer/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   4%|▍         | 85/1954 [00:01<00:28, 65.73 examples/s]Tokenizing data (num_proc=16):  13%|█▎        | 246/1954 [00:01<00:08, 190.30 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 327/1954 [00:01<00:07, 220.08 examples/s]Tokenizing data (num_proc=16):  23%|██▎       | 443/1954 [00:02<00:05, 275.04 examples/s]Tokenizing data (num_proc=16):  29%|██▉       | 574/1954 [00:02<00:04, 321.22 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 734/1954 [00:02<00:03, 402.61 examples/s]Tokenizing data (num_proc=16):  42%|████▏     | 817/1954 [00:02<00:02, 394.81 examples/s]Tokenizing data (num_proc=16):  47%|████▋     | 925/1954 [00:03<00:02, 419.47 examples/s]Tokenizing data (num_proc=16):  54%|█████▍    | 1059/1954 [00:03<00:02, 435.20 examples/s]Tokenizing data (num_proc=16):  60%|██████    | 1180/1954 [00:03<00:01, 460.89 examples/s]Tokenizing data (num_proc=16):  66%|██████▋   | 1296/1954 [00:03<00:01, 466.38 examples/s]Tokenizing data (num_proc=16):  73%|███████▎  | 1425/1954 [00:04<00:01, 477.21 examples/s]Tokenizing data (num_proc=16):  79%|███████▉  | 1548/1954 [00:04<00:00, 526.62 examples/s]Tokenizing data (num_proc=16):  86%|████████▌ | 1673/1954 [00:04<00:00, 560.29 examples/s]Tokenizing data (num_proc=16):  92%|█████████▏| 1795/1954 [00:04<00:00, 608.02 examples/s]Tokenizing data (num_proc=16):  98%|█████████▊| 1919/1954 [00:04<00:00, 553.69 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:05<00:00, 387.90 examples/s]
  0%|          | 0/25 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/25 [00:02<?, ?it/s, acc=nan]  4%|▍         | 1/25 [00:02<00:58,  2.45s/it, acc=nan]  4%|▍         | 1/25 [00:04<00:58,  2.45s/it, acc=nan]  8%|▊         | 2/25 [00:04<00:44,  1.93s/it, acc=nan]  8%|▊         | 2/25 [00:05<00:44,  1.93s/it, acc=nan] 12%|█▏        | 3/25 [00:05<00:40,  1.83s/it, acc=nan] 12%|█▏        | 3/25 [00:07<00:40,  1.83s/it, acc=nan] 16%|█▌        | 4/25 [00:07<00:37,  1.79s/it, acc=nan] 16%|█▌        | 4/25 [00:09<00:37,  1.79s/it, acc=nan] 20%|██        | 5/25 [00:09<00:37,  1.90s/it, acc=nan] 20%|██        | 5/25 [00:11<00:37,  1.90s/it, acc=nan] 24%|██▍       | 6/25 [00:11<00:36,  1.90s/it, acc=nan] 24%|██▍       | 6/25 [00:13<00:36,  1.90s/it, acc=nan] 28%|██▊       | 7/25 [00:13<00:32,  1.80s/it, acc=nan] 28%|██▊       | 7/25 [00:14<00:32,  1.80s/it, acc=nan] 32%|███▏      | 8/25 [00:14<00:31,  1.82s/it, acc=nan] 32%|███▏      | 8/25 [00:16<00:31,  1.82s/it, acc=nan] 36%|███▌      | 9/25 [00:16<00:28,  1.80s/it, acc=nan] 36%|███▌      | 9/25 [00:18<00:28,  1.80s/it, acc=nan] 40%|████      | 10/25 [00:18<00:28,  1.89s/it, acc=nan] 40%|████      | 10/25 [00:20<00:28,  1.89s/it, acc=nan] 44%|████▍     | 11/25 [00:20<00:26,  1.90s/it, acc=nan] 44%|████▍     | 11/25 [00:22<00:26,  1.90s/it, acc=nan] 48%|████▊     | 12/25 [00:22<00:24,  1.90s/it, acc=nan] 48%|████▊     | 12/25 [00:24<00:24,  1.90s/it, acc=nan] 52%|█████▏    | 13/25 [00:24<00:22,  1.88s/it, acc=nan] 52%|█████▏    | 13/25 [00:26<00:22,  1.88s/it, acc=nan] 56%|█████▌    | 14/25 [00:26<00:20,  1.90s/it, acc=nan] 56%|█████▌    | 14/25 [00:28<00:20,  1.90s/it, acc=nan] 60%|██████    | 15/25 [00:28<00:18,  1.89s/it, acc=nan] 60%|██████    | 15/25 [00:30<00:18,  1.89s/it, acc=nan] 64%|██████▍   | 16/25 [00:30<00:17,  1.89s/it, acc=nan] 64%|██████▍   | 16/25 [00:31<00:17,  1.89s/it, acc=nan] 68%|██████▊   | 17/25 [00:31<00:15,  1.88s/it, acc=nan] 68%|██████▊   | 17/25 [00:33<00:15,  1.88s/it, acc=nan] 72%|███████▏  | 18/25 [00:33<00:13,  1.87s/it, acc=nan] 72%|███████▏  | 18/25 [00:39<00:13,  1.87s/it, acc=nan] 76%|███████▌  | 19/25 [00:39<00:17,  2.92s/it, acc=nan] 76%|███████▌  | 19/25 [00:44<00:17,  2.92s/it, acc=nan] 80%|████████  | 20/25 [00:44<00:17,  3.58s/it, acc=nan] 80%|████████  | 20/25 [00:46<00:17,  3.58s/it, acc=nan] 84%|████████▍ | 21/25 [00:46<00:12,  3.06s/it, acc=nan] 84%|████████▍ | 21/25 [00:47<00:12,  3.06s/it, acc=nan] 88%|████████▊ | 22/25 [00:47<00:08,  2.70s/it, acc=nan] 88%|████████▊ | 22/25 [00:49<00:08,  2.70s/it, acc=nan] 92%|█████████▏| 23/25 [00:49<00:04,  2.48s/it, acc=nan] 92%|█████████▏| 23/25 [00:51<00:04,  2.48s/it, acc=nan] 96%|█████████▌| 24/25 [00:51<00:02,  2.27s/it, acc=nan] 96%|█████████▌| 24/25 [00:52<00:02,  2.27s/it, acc=nan]100%|██████████| 25/25 [00:52<00:00,  1.91s/it, acc=nan]100%|██████████| 25/25 [00:52<00:00,  2.11s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/siqa/llama/standard/Meta-Llama-3-8B-Instruct.jsonl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1954 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   5%|▌         | 106/1954 [00:01<00:18, 98.78 examples/s]Tokenizing data (num_proc=16):  11%|█         | 218/1954 [00:01<00:09, 192.42 examples/s]Tokenizing data (num_proc=16):  18%|█▊        | 346/1954 [00:01<00:05, 287.68 examples/s]Tokenizing data (num_proc=16):  24%|██▍       | 465/1954 [00:01<00:04, 355.10 examples/s]Tokenizing data (num_proc=16):  30%|██▉       | 586/1954 [00:02<00:04, 311.84 examples/s]Tokenizing data (num_proc=16):  43%|████▎     | 840/1954 [00:02<00:02, 497.93 examples/s]Tokenizing data (num_proc=16):  49%|████▉     | 961/1954 [00:02<00:01, 513.66 examples/s]Tokenizing data (num_proc=16):  56%|█████▌    | 1086/1954 [00:02<00:01, 529.11 examples/s]Tokenizing data (num_proc=16):  62%|██████▏   | 1204/1954 [00:03<00:01, 505.72 examples/s]Tokenizing data (num_proc=16):  68%|██████▊   | 1323/1954 [00:03<00:01, 496.37 examples/s]Tokenizing data (num_proc=16):  74%|███████▍  | 1446/1954 [00:03<00:01, 488.36 examples/s]Tokenizing data (num_proc=16):  80%|████████  | 1569/1954 [00:03<00:00, 593.87 examples/s]Tokenizing data (num_proc=16):  86%|████████▋ | 1686/1954 [00:03<00:00, 585.22 examples/s]Tokenizing data (num_proc=16):  93%|█████████▎| 1814/1954 [00:04<00:00, 550.05 examples/s]Tokenizing data (num_proc=16):  99%|█████████▉| 1942/1954 [00:04<00:00, 498.28 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1954/1954 [00:04<00:00, 420.42 examples/s]
  0%|          | 0/40 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/40 [00:14<?, ?it/s, acc=nan]  2%|▎         | 1/40 [00:14<09:16, 14.27s/it, acc=nan]  2%|▎         | 1/40 [00:28<09:16, 14.27s/it, acc=nan]  5%|▌         | 2/40 [00:28<09:03, 14.30s/it, acc=nan]  5%|▌         | 2/40 [00:42<09:03, 14.30s/it, acc=nan]  8%|▊         | 3/40 [00:42<08:43, 14.15s/it, acc=nan]  8%|▊         | 3/40 [00:59<08:43, 14.15s/it, acc=nan] 10%|█         | 4/40 [00:59<09:10, 15.29s/it, acc=nan] 10%|█         | 4/40 [01:24<09:10, 15.29s/it, acc=nan] 12%|█▎        | 5/40 [01:24<10:53, 18.68s/it, acc=nan] 12%|█▎        | 5/40 [01:38<10:53, 18.68s/it, acc=nan] 15%|█▌        | 6/40 [01:38<09:48, 17.31s/it, acc=nan] 15%|█▌        | 6/40 [02:02<09:48, 17.31s/it, acc=nan] 18%|█▊        | 7/40 [02:02<10:34, 19.22s/it, acc=nan] 18%|█▊        | 7/40 [02:22<10:34, 19.22s/it, acc=nan] 20%|██        | 8/40 [02:22<10:23, 19.50s/it, acc=nan] 20%|██        | 8/40 [02:37<10:23, 19.50s/it, acc=nan] 22%|██▎       | 9/40 [02:37<09:19, 18.05s/it, acc=nan] 22%|██▎       | 9/40 [02:58<09:19, 18.05s/it, acc=nan] 25%|██▌       | 10/40 [02:58<09:30, 19.03s/it, acc=nan] 25%|██▌       | 10/40 [03:22<09:30, 19.03s/it, acc=nan] 28%|██▊       | 11/40 [03:22<09:55, 20.53s/it, acc=nan] 28%|██▊       | 11/40 [03:40<09:55, 20.53s/it, acc=nan] 30%|███       | 12/40 [03:40<09:19, 19.97s/it, acc=nan] 30%|███       | 12/40 [04:01<09:19, 19.97s/it, acc=nan] 32%|███▎      | 13/40 [04:01<09:02, 20.08s/it, acc=nan] 32%|███▎      | 13/40 [04:28<09:02, 20.08s/it, acc=nan] 35%|███▌      | 14/40 [04:28<09:40, 22.33s/it, acc=nan] 35%|███▌      | 14/40 [04:44<09:40, 22.33s/it, acc=nan] 38%|███▊      | 15/40 [04:44<08:31, 20.45s/it, acc=nan] 38%|███▊      | 15/40 [04:58<08:31, 20.45s/it, acc=nan] 40%|████      | 16/40 [04:58<07:18, 18.29s/it, acc=nan] 40%|████      | 16/40 [05:14<07:18, 18.29s/it, acc=nan] 42%|████▎     | 17/40 [05:14<06:43, 17.56s/it, acc=nan] 42%|████▎     | 17/40 [05:27<06:43, 17.56s/it, acc=nan] 45%|████▌     | 18/40 [05:27<06:00, 16.37s/it, acc=nan] 45%|████▌     | 18/40 [05:43<06:00, 16.37s/it, acc=nan] 48%|████▊     | 19/40 [05:43<05:40, 16.21s/it, acc=nan] 48%|████▊     | 19/40 [05:56<05:40, 16.21s/it, acc=nan] 50%|█████     | 20/40 [05:56<05:06, 15.34s/it, acc=nan] 50%|█████     | 20/40 [06:20<05:06, 15.34s/it, acc=nan] 52%|█████▎    | 21/40 [06:20<05:38, 17.79s/it, acc=nan] 52%|█████▎    | 21/40 [06:32<05:38, 17.79s/it, acc=nan] 55%|█████▌    | 22/40 [06:32<04:52, 16.26s/it, acc=nan] 55%|█████▌    | 22/40 [06:47<04:52, 16.26s/it, acc=nan] 57%|█████▊    | 23/40 [06:47<04:26, 15.69s/it, acc=nan] 57%|█████▊    | 23/40 [07:02<04:26, 15.69s/it, acc=nan] 60%|██████    | 24/40 [07:02<04:06, 15.42s/it, acc=nan] 60%|██████    | 24/40 [07:20<04:06, 15.42s/it, acc=nan] 62%|██████▎   | 25/40 [07:20<04:06, 16.44s/it, acc=nan] 62%|██████▎   | 25/40 [07:36<04:06, 16.44s/it, acc=nan] 65%|██████▌   | 26/40 [07:36<03:45, 16.13s/it, acc=nan] 65%|██████▌   | 26/40 [07:52<03:45, 16.13s/it, acc=nan] 68%|██████▊   | 27/40 [07:52<03:28, 16.07s/it, acc=nan] 68%|██████▊   | 27/40 [08:08<03:28, 16.07s/it, acc=nan] 70%|███████   | 28/40 [08:08<03:14, 16.19s/it, acc=nan] 70%|███████   | 28/40 [08:24<03:14, 16.19s/it, acc=nan] 72%|███████▎  | 29/40 [08:24<02:55, 15.97s/it, acc=nan] 72%|███████▎  | 29/40 [08:39<02:55, 15.97s/it, acc=nan] 75%|███████▌  | 30/40 [08:39<02:38, 15.90s/it, acc=nan] 75%|███████▌  | 30/40 [08:51<02:38, 15.90s/it, acc=nan] 78%|███████▊  | 31/40 [08:51<02:10, 14.51s/it, acc=nan] 78%|███████▊  | 31/40 [09:06<02:10, 14.51s/it, acc=nan] 80%|████████  | 32/40 [09:06<01:58, 14.85s/it, acc=nan] 80%|████████  | 32/40 [09:20<01:58, 14.85s/it, acc=nan] 82%|████████▎ | 33/40 [09:20<01:40, 14.42s/it, acc=nan] 82%|████████▎ | 33/40 [09:35<01:40, 14.42s/it, acc=nan] 85%|████████▌ | 34/40 [09:35<01:27, 14.56s/it, acc=nan] 85%|████████▌ | 34/40 [09:52<01:27, 14.56s/it, acc=nan] 88%|████████▊ | 35/40 [09:52<01:16, 15.34s/it, acc=nan] 88%|████████▊ | 35/40 [10:09<01:16, 15.34s/it, acc=nan] 90%|█████████ | 36/40 [10:09<01:04, 16.05s/it, acc=nan] 90%|█████████ | 36/40 [10:30<01:04, 16.05s/it, acc=nan] 92%|█████████▎| 37/40 [10:30<00:51, 17.29s/it, acc=nan] 92%|█████████▎| 37/40 [10:48<00:51, 17.29s/it, acc=nan] 95%|█████████▌| 38/40 [10:48<00:35, 17.66s/it, acc=nan] 95%|█████████▌| 38/40 [11:02<00:35, 17.66s/it, acc=nan] 98%|█████████▊| 39/40 [11:02<00:16, 16.65s/it, acc=nan] 98%|█████████▊| 39/40 [11:14<00:16, 16.65s/it, acc=nan]100%|██████████| 40/40 [11:14<00:00, 15.05s/it, acc=nan]100%|██████████| 40/40 [11:14<00:00, 16.86s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/mistralai/cot/Mistral-7B-Instruct-v0.1.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:40<00:40, 40.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 27.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 29.47s/it]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:00<00:04, 113.62 examples/s]Tokenizing data (num_proc=16):  19%|█▉        | 114/600 [00:00<00:01, 291.97 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 228/600 [00:00<00:00, 528.88 examples/s]Tokenizing data (num_proc=16):  57%|█████▋    | 341/600 [00:00<00:00, 683.36 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 526/600 [00:00<00:00, 974.08 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:00<00:00, 610.07 examples/s]
  0%|          | 0/24 [00:00<?, ?it/s]/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/24 [00:30<?, ?it/s, acc=nan]  4%|▍         | 1/24 [00:30<11:34, 30.18s/it, acc=nan]  4%|▍         | 1/24 [01:01<11:34, 30.18s/it, acc=nan]  8%|▊         | 2/24 [01:01<11:16, 30.73s/it, acc=nan]  8%|▊         | 2/24 [01:24<11:16, 30.73s/it, acc=nan] 12%|█▎        | 3/24 [01:24<09:33, 27.30s/it, acc=nan] 12%|█▎        | 3/24 [01:57<09:33, 27.30s/it, acc=nan] 17%|█▋        | 4/24 [01:57<09:50, 29.50s/it, acc=nan] 17%|█▋        | 4/24 [02:29<09:50, 29.50s/it, acc=nan] 21%|██        | 5/24 [02:29<09:36, 30.34s/it, acc=nan] 21%|██        | 5/24 [02:55<09:36, 30.34s/it, acc=nan] 25%|██▌       | 6/24 [02:55<08:42, 29.01s/it, acc=nan] 25%|██▌       | 6/24 [03:24<08:42, 29.01s/it, acc=nan] 29%|██▉       | 7/24 [03:24<08:12, 28.99s/it, acc=nan] 29%|██▉       | 7/24 [03:52<08:12, 28.99s/it, acc=nan] 33%|███▎      | 8/24 [03:52<07:36, 28.56s/it, acc=nan] 33%|███▎      | 8/24 [04:34<07:36, 28.56s/it, acc=nan] 38%|███▊      | 9/24 [04:34<08:14, 32.95s/it, acc=nan] 38%|███▊      | 9/24 [05:10<08:14, 32.95s/it, acc=nan] 42%|████▏     | 10/24 [05:10<07:51, 33.67s/it, acc=nan] 42%|████▏     | 10/24 [05:44<07:51, 33.67s/it, acc=nan] 46%|████▌     | 11/24 [05:44<07:21, 33.93s/it, acc=nan] 46%|████▌     | 11/24 [06:41<07:21, 33.93s/it, acc=nan] 50%|█████     | 12/24 [06:41<08:09, 40.76s/it, acc=nan] 50%|█████     | 12/24 [07:13<08:09, 40.76s/it, acc=nan] 54%|█████▍    | 13/24 [07:13<07:00, 38.25s/it, acc=nan] 54%|█████▍    | 13/24 [07:48<07:00, 38.25s/it, acc=nan] 58%|█████▊    | 14/24 [07:48<06:12, 37.28s/it, acc=nan] 58%|█████▊    | 14/24 [08:14<06:12, 37.28s/it, acc=nan] 62%|██████▎   | 15/24 [08:14<05:04, 33.88s/it, acc=nan] 62%|██████▎   | 15/24 [08:46<05:04, 33.88s/it, acc=nan] 67%|██████▋   | 16/24 [08:46<04:25, 33.16s/it, acc=nan] 67%|██████▋   | 16/24 [09:23<04:25, 33.16s/it, acc=nan] 71%|███████   | 17/24 [09:23<04:00, 34.37s/it, acc=nan] 71%|███████   | 17/24 [10:11<04:00, 34.37s/it, acc=nan] 75%|███████▌  | 18/24 [10:11<03:52, 38.67s/it, acc=nan] 75%|███████▌  | 18/24 [10:46<03:52, 38.67s/it, acc=nan] 79%|███████▉  | 19/24 [10:46<03:06, 37.34s/it, acc=nan] 79%|███████▉  | 19/24 [11:29<03:06, 37.34s/it, acc=nan] 83%|████████▎ | 20/24 [11:29<02:36, 39.11s/it, acc=nan] 83%|████████▎ | 20/24 [12:22<02:36, 39.11s/it, acc=nan] 88%|████████▊ | 21/24 [12:22<02:09, 43.18s/it, acc=nan] 88%|████████▊ | 21/24 [13:09<02:09, 43.18s/it, acc=nan] 92%|█████████▏| 22/24 [13:09<01:29, 44.56s/it, acc=nan] 92%|█████████▏| 22/24 [17:05<01:29, 44.56s/it, acc=nan] 96%|█████████▌| 23/24 [17:05<01:41, 101.94s/it, acc=nan] 96%|█████████▌| 23/24 [18:03<01:41, 101.94s/it, acc=nan]100%|██████████| 24/24 [18:03<00:00, 88.78s/it, acc=nan] 100%|██████████| 24/24 [18:03<00:00, 45.15s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/mistralai/direct_answer/Mistral-7B-Instruct-v0.1.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:00<00:03, 143.92 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:00<00:01, 428.53 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:00<00:00, 643.95 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 378/600 [00:00<00:00, 752.49 examples/s]Tokenizing data (num_proc=16):  82%|████████▏ | 489/600 [00:00<00:00, 842.71 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:00<00:00, 865.27 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:00<00:00, 635.96 examples/s]
  0%|          | 0/8 [00:00<?, ?it/s]/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/8 [00:05<?, ?it/s, acc=nan] 12%|█▎        | 1/8 [00:05<00:37,  5.38s/it, acc=nan] 12%|█▎        | 1/8 [00:08<00:37,  5.38s/it, acc=nan] 25%|██▌       | 2/8 [00:08<00:25,  4.27s/it, acc=nan] 25%|██▌       | 2/8 [00:12<00:25,  4.27s/it, acc=nan] 38%|███▊      | 3/8 [00:12<00:19,  3.98s/it, acc=nan] 38%|███▊      | 3/8 [00:20<00:19,  3.98s/it, acc=nan] 50%|█████     | 4/8 [00:20<00:21,  5.38s/it, acc=nan] 50%|█████     | 4/8 [00:26<00:21,  5.38s/it, acc=nan] 62%|██████▎   | 5/8 [00:26<00:17,  5.67s/it, acc=nan] 62%|██████▎   | 5/8 [00:29<00:17,  5.67s/it, acc=nan] 75%|███████▌  | 6/8 [00:29<00:09,  4.83s/it, acc=nan] 75%|███████▌  | 6/8 [00:33<00:09,  4.83s/it, acc=nan] 88%|████████▊ | 7/8 [00:33<00:04,  4.61s/it, acc=nan] 88%|████████▊ | 7/8 [00:36<00:04,  4.61s/it, acc=nan]100%|██████████| 8/8 [00:36<00:00,  3.96s/it, acc=nan]100%|██████████| 8/8 [00:36<00:00,  4.52s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/MultiArith/mistralai/standard/Mistral-7B-Instruct-v0.1.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.25s/it]
Tokenizing data (num_proc=16):   0%|          | 0/600 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 38/600 [00:00<00:04, 124.97 examples/s]Tokenizing data (num_proc=16):  25%|██▌       | 152/600 [00:00<00:01, 429.46 examples/s]Tokenizing data (num_proc=16):  44%|████▍     | 266/600 [00:00<00:00, 580.20 examples/s]Tokenizing data (num_proc=16):  69%|██████▉   | 415/600 [00:00<00:00, 812.77 examples/s]Tokenizing data (num_proc=16):  94%|█████████▍| 563/600 [00:00<00:00, 877.92 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 600/600 [00:00<00:00, 655.02 examples/s]
  0%|          | 0/12 [00:00<?, ?it/s]/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/12 [01:14<?, ?it/s, acc=nan]  8%|▊         | 1/12 [01:14<13:36, 74.20s/it, acc=nan]  8%|▊         | 1/12 [01:58<13:36, 74.20s/it, acc=nan] 17%|█▋        | 2/12 [01:58<09:24, 56.44s/it, acc=nan] 17%|█▋        | 2/12 [03:45<09:24, 56.44s/it, acc=nan] 25%|██▌       | 3/12 [03:45<11:55, 79.47s/it, acc=nan] 25%|██▌       | 3/12 [05:26<11:55, 79.47s/it, acc=nan] 33%|███▎      | 4/12 [05:26<11:46, 88.32s/it, acc=nan] 33%|███▎      | 4/12 [06:25<11:46, 88.32s/it, acc=nan] 42%|████▏     | 5/12 [06:25<09:03, 77.59s/it, acc=nan] 42%|████▏     | 5/12 [07:25<09:03, 77.59s/it, acc=nan] 50%|█████     | 6/12 [07:25<07:10, 71.67s/it, acc=nan] 50%|█████     | 6/12 [08:21<07:10, 71.67s/it, acc=nan] 58%|█████▊    | 7/12 [08:21<05:32, 66.59s/it, acc=nan] 58%|█████▊    | 7/12 [09:25<05:32, 66.59s/it, acc=nan] 67%|██████▋   | 8/12 [09:25<04:22, 65.68s/it, acc=nan] 67%|██████▋   | 8/12 [10:22<04:22, 65.68s/it, acc=nan] 75%|███████▌  | 9/12 [10:22<03:09, 63.08s/it, acc=nan] 75%|███████▌  | 9/12 [11:26<03:09, 63.08s/it, acc=nan] 83%|████████▎ | 10/12 [11:26<02:06, 63.25s/it, acc=nan] 83%|████████▎ | 10/12 [12:43<02:06, 63.25s/it, acc=nan] 92%|█████████▏| 11/12 [12:43<01:07, 67.34s/it, acc=nan] 92%|█████████▏| 11/12 [14:30<01:07, 67.34s/it, acc=nan]100%|██████████| 12/12 [14:30<00:00, 79.62s/it, acc=nan]100%|██████████| 12/12 [14:30<00:00, 72.58s/it, acc=nan]
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(split_batches=False)
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
outputs/commensenseqa/mistralai/cot/Mistral-7B-Instruct-v0.1.jsonl
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]
Tokenizing data (num_proc=16):   0%|          | 0/1221 [00:00<?, ? examples/s]Tokenizing data (num_proc=16):   6%|▋         | 77/1221 [00:00<00:04, 259.81 examples/s]Tokenizing data (num_proc=16):  17%|█▋        | 205/1221 [00:00<00:01, 588.80 examples/s]Tokenizing data (num_proc=16):  38%|███▊      | 459/1221 [00:00<00:00, 1071.94 examples/s]Tokenizing data (num_proc=16):  63%|██████▎   | 765/1221 [00:00<00:00, 1605.98 examples/s]Tokenizing data (num_proc=16):  88%|████████▊ | 1069/1221 [00:00<00:00, 1969.70 examples/s]Tokenizing data (num_proc=16): 100%|██████████| 1221/1221 [00:00<00:00, 1300.46 examples/s]
  0%|          | 0/49 [00:00<?, ?it/s]/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/aswini/Fifth_project_on_dialogue/Custom_loss_experiment/riddhankur_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
  0%|          | 0/49 [00:56<?, ?it/s, acc=nan]  2%|▏         | 1/49 [00:56<45:18, 56.64s/it, acc=nan]  2%|▏         | 1/49 [01:41<45:18, 56.64s/it, acc=nan]  4%|▍         | 2/49 [01:41<38:51, 49.61s/it, acc=nan]  4%|▍         | 2/49 [02:38<38:51, 49.61s/it, acc=nan]  6%|▌         | 3/49 [02:38<40:34, 52.92s/it, acc=nan]  6%|▌         | 3/49 [03:58<40:34, 52.92s/it, acc=nan]  8%|▊         | 4/49 [03:58<47:54, 63.88s/it, acc=nan]  8%|▊         | 4/49 [05:15<47:54, 63.88s/it, acc=nan] 10%|█         | 5/49 [05:15<50:19, 68.63s/it, acc=nan] 10%|█         | 5/49 [06:14<50:19, 68.63s/it, acc=nan] 12%|█▏        | 6/49 [06:14<46:45, 65.25s/it, acc=nan] 12%|█▏        | 6/49 [07:17<46:45, 65.25s/it, acc=nan] 14%|█▍        | 7/49 [07:17<45:12, 64.59s/it, acc=nan] 14%|█▍        | 7/49 [08:16<45:12, 64.59s/it, acc=nan] 16%|█▋        | 8/49 [08:16<42:51, 62.73s/it, acc=nan] 16%|█▋        | 8/49 [09:24<42:51, 62.73s/it, acc=nan] 18%|█▊        | 9/49 [09:24<42:54, 64.36s/it, acc=nan] 18%|█▊        | 9/49 [10:30<42:54, 64.36s/it, acc=nan] 20%|██        | 10/49 [10:30<42:04, 64.74s/it, acc=nan] 20%|██        | 10/49 [11:35<42:04, 64.74s/it, acc=nan] 22%|██▏       | 11/49 [11:35<41:02, 64.81s/it, acc=nan] 22%|██▏       | 11/49 [12:44<41:02, 64.81s/it, acc=nan] 24%|██▍       | 12/49 [12:44<40:50, 66.22s/it, acc=nan] 24%|██▍       | 12/49 [13:37<40:50, 66.22s/it, acc=nan] 27%|██▋       | 13/49 [13:37<37:23, 62.31s/it, acc=nan] 27%|██▋       | 13/49 [14:47<37:23, 62.31s/it, acc=nan] 29%|██▊       | 14/49 [14:47<37:42, 64.64s/it, acc=nan] 29%|██▊       | 14/49 [15:29<37:42, 64.64s/it, acc=nan] 31%|███       | 15/49 [15:29<32:44, 57.78s/it, acc=nan] 31%|███       | 15/49 [16:30<32:44, 57.78s/it, acc=nan] 33%|███▎      | 16/49 [16:30<32:12, 58.56s/it, acc=nan] 33%|███▎      | 16/49 [17:23<32:12, 58.56s/it, acc=nan] 35%|███▍      | 17/49 [17:23<30:20, 56.88s/it, acc=nan] 35%|███▍      | 17/49 [18:38<30:20, 56.88s/it, acc=nan] 37%|███▋      | 18/49 [18:38<32:13, 62.36s/it, acc=nan] 37%|███▋      | 18/49 [19:32<32:13, 62.36s/it, acc=nan] 39%|███▉      | 19/49 [19:32<30:02, 60.07s/it, acc=nan] 39%|███▉      | 19/49 [20:26<30:02, 60.07s/it, acc=nan] 41%|████      | 20/49 [20:26<28:04, 58.10s/it, acc=nan] 41%|████      | 20/49 [21:57<28:04, 58.10s/it, acc=nan] 43%|████▎     | 21/49 [21:57<31:41, 67.93s/it, acc=nan] 43%|████▎     | 21/49 [22:49<31:41, 67.93s/it, acc=nan] 45%|████▍     | 22/49 [22:49<28:24, 63.12s/it, acc=nan] 45%|████▍     | 22/49 [23:41<28:24, 63.12s/it, acc=nan] 47%|████▋     | 23/49 [23:41<25:55, 59.82s/it, acc=nan] 47%|████▋     | 23/49 [25:01<25:55, 59.82s/it, acc=nan] 49%|████▉     | 24/49 [25:01<27:29, 65.96s/it, acc=nan] 49%|████▉     | 24/49 [26:08<27:29, 65.96s/it, acc=nan] 51%|█████     | 25/49 [26:08<26:32, 66.36s/it, acc=nan] 51%|█████     | 25/49 [27:24<26:32, 66.36s/it, acc=nan] 53%|█████▎    | 26/49 [27:24<26:31, 69.21s/it, acc=nan] 53%|█████▎    | 26/49 [28:24<26:31, 69.21s/it, acc=nan] 55%|█████▌    | 27/49 [28:24<24:21, 66.41s/it, acc=nan] 55%|█████▌    | 27/49 [29:42<24:21, 66.41s/it, acc=nan] 57%|█████▋    | 28/49 [29:42<24:27, 69.86s/it, acc=nan] 57%|█████▋    | 28/49 [30:50<24:27, 69.86s/it, acc=nan] 59%|█████▉    | 29/49 [30:50<23:04, 69.21s/it, acc=nan] 59%|█████▉    | 29/49 [31:54<23:04, 69.21s/it, acc=nan] 61%|██████    | 30/49 [31:54<21:26, 67.71s/it, acc=nan] 61%|██████    | 30/49 [33:04<21:26, 67.71s/it, acc=nan] 63%|██████▎   | 31/49 [33:04<20:30, 68.34s/it, acc=nan] 63%|██████▎   | 31/49 [34:41<20:30, 68.34s/it, acc=nan] 65%|██████▌   | 32/49 [34:41<21:51, 77.12s/it, acc=nan] 65%|██████▌   | 32/49 [36:12<21:51, 77.12s/it, acc=nan] 67%|██████▋   | 33/49 [36:12<21:36, 81.06s/it, acc=nan] 67%|██████▋   | 33/49 [38:27<21:36, 81.06s/it, acc=nan] 69%|██████▉   | 34/49 [38:27<24:21, 97.45s/it, acc=nan] 69%|██████▉   | 34/49 [39:43<24:21, 97.45s/it, acc=nan] 71%|███████▏  | 35/49 [39:43<21:13, 90.94s/it, acc=nan] 71%|███████▏  | 35/49 [41:04<21:13, 90.94s/it, acc=nan] 73%|███████▎  | 36/49 [41:04<19:04, 88.02s/it, acc=nan] 73%|███████▎  | 36/49 [42:10<19:04, 88.02s/it, acc=nan] 76%|███████▌  | 37/49 [42:10<16:14, 81.20s/it, acc=nan] 76%|███████▌  | 37/49 [43:07<16:14, 81.20s/it, acc=nan] 78%|███████▊  | 38/49 [43:07<13:34, 74.08s/it, acc=nan] 78%|███████▊  | 38/49 [44:34<13:34, 74.08s/it, acc=nan] 80%|███████▉  | 39/49 [44:34<13:00, 78.09s/it, acc=nan] 80%|███████▉  | 39/49 [45:36<13:00, 78.09s/it, acc=nan] 82%|████████▏ | 40/49 [45:36<10:58, 73.12s/it, acc=nan] 82%|████████▏ | 40/49 [47:08<10:58, 73.12s/it, acc=nan] 84%|████████▎ | 41/49 [47:08<10:29, 78.67s/it, acc=nan] 84%|████████▎ | 41/49 [48:02<10:29, 78.67s/it, acc=nan] 86%|████████▌ | 42/49 [48:02<08:20, 71.45s/it, acc=nan] 86%|████████▌ | 42/49 [49:02<08:20, 71.45s/it, acc=nan] 88%|████████▊ | 43/49 [49:02<06:48, 68.04s/it, acc=nan] 88%|████████▊ | 43/49 [49:56<06:48, 68.04s/it, acc=nan] 90%|████████▉ | 44/49 [49:56<05:18, 63.63s/it, acc=nan] 90%|████████▉ | 44/49 [51:16<05:18, 63.63s/it, acc=nan] 92%|█████████▏| 45/49 [51:16<04:34, 68.56s/it, acc=nan]